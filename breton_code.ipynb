{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
   "metadata": {
    "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
    "tags": []
   },
   "source": [
    "# Exporting the Breton example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f8b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwclient in /opt/homebrew/lib/python3.11/site-packages (0.10.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/lib/python3.11/site-packages (from mwclient) (1.3.1)\n",
      "Requirement already satisfied: six in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from mwclient) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib->mwclient) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib->mwclient) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: mwparserfromhell in /opt/homebrew/lib/python3.11/site-packages (0.6.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: mwcomposerfromhell in /opt/homebrew/lib/python3.11/site-packages (0.5)\n",
      "Requirement already satisfied: mwparserfromhell>=0.6.2 in /opt/homebrew/lib/python3.11/site-packages (from mwcomposerfromhell) (0.6.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: jupyter in /opt/homebrew/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: notebook in /opt/homebrew/lib/python3.11/site-packages (from jupyter) (7.0.0)\n",
      "Requirement already satisfied: qtconsole in /opt/homebrew/lib/python3.11/site-packages (from jupyter) (5.4.3)\n",
      "Requirement already satisfied: jupyter-console in /opt/homebrew/lib/python3.11/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/homebrew/lib/python3.11/site-packages (from jupyter) (7.7.3)\n",
      "Requirement already satisfied: ipykernel in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter) (6.22.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/lib/python3.11/site-packages (from jupyter) (8.0.7)\n",
      "Requirement already satisfied: appnope in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (1.6.6)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (8.12.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (8.1.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from ipykernel->jupyter) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/homebrew/lib/python3.11/site-packages (from ipywidgets->jupyter) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/homebrew/lib/python3.11/site-packages (from ipywidgets->jupyter) (3.0.8)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-console->jupyter) (3.0.38)\n",
      "Requirement already satisfied: pygments in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-console->jupyter) (2.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (2.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (5.9.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/homebrew/lib/python3.11/site-packages (from notebook->jupyter) (2.7.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/homebrew/lib/python3.11/site-packages (from notebook->jupyter) (2.24.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/homebrew/lib/python3.11/site-packages (from notebook->jupyter) (4.0.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/homebrew/lib/python3.11/site-packages (from notebook->jupyter) (0.2.3)\n",
      "Requirement already satisfied: ipython-genutils in /opt/homebrew/lib/python3.11/site-packages (from qtconsole->jupyter) (0.2.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /opt/homebrew/lib/python3.11/site-packages (from qtconsole->jupyter) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: backcall in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (3.6.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.4.4)\n",
      "Requirement already satisfied: overrides in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (7.3.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: send2trash in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.6.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter) (2.0.3)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.12.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.17.3 in /opt/homebrew/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (4.18.4)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/homebrew/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.31.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/homebrew/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.18.0)\n",
      "Requirement already satisfied: wcwidth in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.4.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.9.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (6.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2022.12.7)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/homebrew/lib/python3.11/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /opt/homebrew/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/homebrew/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.4)\n",
      "Requirement already satisfied: uri-template in /opt/homebrew/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/homebrew/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/lib/python3.11/site-packages (8.0.7)\n",
      "Requirement already satisfied: widgetsnbextension in /opt/homebrew/lib/python3.11/site-packages (4.0.8)\n",
      "Requirement already satisfied: pandas-profiling in /opt/homebrew/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/homebrew/lib/python3.11/site-packages (from ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: joblib~=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (1.10.1)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (2.0.3)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (3.7.1)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from pandas-profiling) (6.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (2.1.2)\n",
      "Requirement already satisfied: visions==0.7.4 in /opt/homebrew/lib/python3.11/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (1.24.3)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: missingno>=0.4.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (0.5.2)\n",
      "Requirement already satisfied: phik>=0.11.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (0.12.3)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from pandas-profiling) (4.64.1)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (0.12.2)\n",
      "Requirement already satisfied: multimethod>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from pandas-profiling) (1.9.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (23.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/homebrew/lib/python3.11/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (3.1)\n",
      "Requirement already satisfied: imagehash in /opt/homebrew/lib/python3.11/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (4.3.1)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (9.5.0)\n",
      "Requirement already satisfied: appnope in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: backcall in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=1.8.1->pandas-profiling) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (2022.12.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Requirement already satisfied: PyWavelets in /opt/homebrew/lib/python3.11/site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/liuyingzi/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mwclient\n",
    "!pip install mwparserfromhell\n",
    "!pip install mwcomposerfromhell\n",
    "!pip install jupyter\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6",
   "metadata": {
    "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6"
   },
   "outputs": [],
   "source": [
    "import datetime, tqdm, re, time, string\n",
    "\n",
    "from mwclient import Site\n",
    "#myparserfromhell : parser pour le code wiki\n",
    "import mwparserfromhell, mwcomposerfromhell\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f90d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "got all 9613 pages\n"
     ]
    }
   ],
   "source": [
    "print('connecting...')\n",
    "#site contenant tous les exemples\n",
    "site = Site('arbres.iker.cnrs.fr/', path='/')\n",
    "outf = open('fichiers_extraction/problemPages.html','w')\n",
    "allpages = list(site.allpages())\n",
    "print('got all', len(allpages), 'pages')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
   "metadata": {
    "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
    "tags": []
   },
   "source": [
    "#### Charger toutes les pages du site: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a6e0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9613/9613 [19:16<00:00,  8.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction done. it took 1156.3399622440338 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "#dictionnaire contenant toutes les pages aspirées\n",
    "pages = {}\n",
    "#on peut ici modifier le nombre de pages à charger pour charger petit à petit ex : allpages[:3000] pour les 3000 premières pages\n",
    "# newlist=allpages\n",
    "for p in tqdm.tqdm(allpages):\n",
    "# for p in tqdm.tqdm(newlist):\n",
    "    #si les pages ne sont pas de la documentation\n",
    "    if not('/documentation' in p.name or '/docname' in p.name or 'Module:' in p.name):\n",
    "        #on ajoute le nom de la page comme clé, son contenu comme valeur\n",
    "        pages[p.name] = p.text()\n",
    "\n",
    "print('extraction done. it took', time.time()-t, 'seconds.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
   "metadata": {
    "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
    "tags": []
   },
   "source": [
    "### Compter tous les exemples trouvés dans les pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64d0685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour enregistrer les pages chargées dans un fichier Pickle: \n",
    "# une fois toutes les pages chargées, on le met dans un pickle\n",
    "with open(\"fichiers_extraction/Pages.pickle\", \"wb\") as myFile:\n",
    "   pickle.dump(pages, myFile)\n",
    "with open(\"fichiers_extraction/Pages.txt\", \"w\") as myFile:\n",
    "    for title,text in pages.items():\n",
    "        myFile.write(title+'\\n'+text+'\\n------------------------\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
    "outputId": "119a6b13-11d3-41cc-e22e-302399c2a06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9613 pages\n"
     ]
    }
   ],
   "source": [
    "# print(', '.join(pages.keys()))\n",
    "#on créé un fichier txt contenant tous les titres de pages\n",
    "open('fichiers_extraction/pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1778bdef",
   "metadata": {},
   "source": [
    "# Restart from local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7221879-cd08-49de-990c-73a09bf3550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9613 pages\n"
     ]
    }
   ],
   "source": [
    "import datetime, tqdm, re, time, string\n",
    "from mwclient import Site\n",
    "import mwparserfromhell, mwcomposerfromhell\n",
    "import pickle\n",
    "\n",
    "with open('fichiers_extraction/Pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)\n",
    "open('fichiers_extraction/pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9da780d9",
   "metadata": {},
   "source": [
    "# wiki2lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39d52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki2lines(wikitext, title):\n",
    "    \"\"\"\n",
    "    takes the wikitext of the example table and the title\n",
    "    returns obj:\n",
    "    obj = {'title':title,'phonetic':phonetic,'text':text, 'gloss':gloss, 'standardise':standardise,'lemgloss':lemgloss,'translation':translation, 'agrammaticale': agrammaticale, 'info':info}\n",
    "    \n",
    "    \"\"\"\n",
    "    contexte, phonetic, text, gloss, standardise, lemgloss, translation, agrammaticale, info = None, None, None, None, None, None, None, None, None\n",
    "    rest = ''\n",
    "    wikitext_split = wikitext.split('\\n')\n",
    "        \n",
    "    # special case if the text is separated in two lines(line 1 and line 5), and the gloss is separated in line 3 and line 7, we put them together\n",
    "    if \"(4) ...|| gouarnamant || Pariz || ne || glask || nemed || eun || dra : || […]\" in wikitext_split[1] or \"bgcolor=#F0FF80|  gwenn,\" in wikitext_split[1] or \"(4)|| Me || zo || bet || ouezan || ket || 'ta || marse … || marse || me || teir || gwezh\" in wikitext_split[1] \\\n",
    "    or \"(6) ... || gouarnamant || Pariz || ne || glask || nemed || eun dra: […]\" in wikitext_split[1] or \"(6)|| Ne || oa'''n''' || ket || gant || ma || c'hentañ || treizhadenn, || hogen || argadet || e voe || '''an douaradez''' || ac'han'''on'''.\" in wikitext_split[1] \\\n",
    "    or \"(1)|| Pa || ne || oar || ket || '''an nen''', || pa || ne || re'''er''' || tamm || kegin || ebet,\" in wikitext_split[1] or \"(2)|| Amañ || em eus || klasket || strollañ || ar pezh || a zo || bet || skrivet || gantañ || diwar-benn || ar politikerezh,\" in wikitext_split[1] \\\n",
    "    or \"(4)||- Ur wech oa kouezhet klañv. An dra-se oa er bloavezh, a gav din, 1949.\" in wikitext_split[1] or \"(1)|| Met || ne selle || ket-eñ || doc'h || ar re-se. \" in wikitext_split[1] or \"(1)|| Hag || an holl || o doa || pellaet || a-benn || doc'h || an oaled \" in wikitext_split[1]\\\n",
    "    or \"(7)|| Mar || deus || tud || hag a || zo || inouus, […], || ez eus || <u>lod all</u> \" in wikitext_split[1] or \"(3)|| <u>Ar re-mañ</u>, || evid || ar bloaz || kenta || e oant || e Huelgoad,\" in wikitext_split[1] or \"(3)|| Me || zo || '''bet''' || || ouezan || ket || 'ta || marse … || '''marse''' || || '''me''' || teir || gwezh\" in wikitext_split[1]\\\n",
    "    or \"(2)|| Ha || n'oe || selaouet || anezhañ || da || wel || pelec'h || vie || bet || aretet.\" in wikitext_split[1] or \"(1)|| Ret || eo || bout || graet || '''ar vicher-se''' || e-pad || pevar || blez || bennak || \" in wikitext_split[1] or \"(3)|| Lakaet || ez eus || bet || da || gefridi || dezhañ || ambroug || hag || aliañ || '''neb''' || den,\" in wikitext_split[1]\\\n",
    "    or \"(4)|| '''Neb''' || n'en deus || gwelet, || keta, || e vez || roet || peoc'h || dezhañ\" in wikitext_split[1] or \"(7)|| '''Ret''' || eo || bout || graet || ar vicher-se || e-pad || pevar || blez || bennak ||\" in wikitext_split[1]:\n",
    "        wikitext_split[1] = wikitext_split[1]+wikitext_split[5]\n",
    "        wikitext_split[3] = wikitext_split[3]+wikitext_split[7]\n",
    "        wikitext_split[5]= '|-'\n",
    "        wikitext_split[7]= '|-'\n",
    "    # special case if the text is separated in two lines，and also the gloss and translation are separated in two lines, we put them together\n",
    "    elif len(wikitext_split)> 12 and (\"(3)|| E-lec'h || mont, || avat, || '''setu'''-hi || '''hag''' || azezañ || war ar || skabell-bediñ,\" in wikitext_split[1] or \"(4)|| Ha koulskoude || he deus || ezhomm || a-walc'h || da gaout || un dra || bennak || da reiñ || d'he || bugale\" in wikitext_split[1]):\n",
    "        wikitext_split[1] = wikitext_split[1]+wikitext_split[5]\n",
    "        wikitext_split[3] = wikitext_split[3]+wikitext_split[7]\n",
    "        wikitext_split[9]= wikitext_split[9]+wikitext_split[11]\n",
    "        wikitext_split[5]= '|-'\n",
    "        wikitext_split[7]= '|-'\n",
    "        wikitext_split[11]= '|-'\n",
    "    # special case if the text is separated in three lines(line 1 and line 5 and line 9), and the gloss is separated in line 3 and line 7 and line 11, we put them together\n",
    "    elif \"chouk,\" in wikitext_split[1]:\n",
    "        wikitext_split[1] = wikitext_split[1]+wikitext_split[5]+wikitext[9]\n",
    "        wikitext_split[3] = wikitext_split[3]+wikitext_split[7]+wikitext[11]\n",
    "        wikitext_split[5]= '|-'\n",
    "        wikitext_split[7]= '|-'\n",
    "        wikitext_split[9]= '|-'\n",
    "        wikitext_split[11]= '|-'\n",
    "    # special case if the third line includes \"[[*]] || Kaset\", we ignore it\n",
    "    elif len(wikitext_split)> 4 and (\"[[*]] || Kaset \" in wikitext_split[3] or \"[[*]] || MA || zi |||| HOR || yezh\" in wikitext_split[3]):\n",
    "        wikitext_split[3]= '|-'\n",
    "    # special case if the gloss is seperated in two lines(line 3 and line 4), we put them together\n",
    "    elif len(wikitext_split)> 4 and (\" || [[POSS|leur]]<sup>[[2]]</sup> || [[lavar|langue]] || [[pemdeziek|quotidien]] || [[evel|comme]] || [[ma|que]] [[emañ|est]] || [[POSS|notre]] || [[hini|celui]]-nous\" in wikitext_split[4] or \"|| [[R]] [[zo|est]] || [[anavezout|conn]].[[-et (Adj.)|u]] || [[gant|par]].[[pronom incorporé|lui]]\" in wikitext_split[4] or \"|| [[forzh|force]] || \\\"costaud\\\"\" in wikitext_split[4]):\n",
    "        wikitext_split[3] = wikitext_split[3]+wikitext_split[4]\n",
    "        wikitext_split[4]= '|-'\n",
    "    # special case if the translation line not begain with \"colspan=15\", we add it in the beginning\n",
    "    elif len(wikitext_split)> 8 and \"||| 'Comme c'est stupide !' \" in wikitext_split[7]:\n",
    "        wikitext_split[7]= wikitext_split[7].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 6 and \"||| 'Je sais.'\" in wikitext_split[5]:\n",
    "        wikitext_split[5]= wikitext_split[5].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 6 and \"||| 'réincarner'\" in wikitext_split[5]:\n",
    "        wikitext_split[5]= wikitext_split[5].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 6 and \"'tel ou tel' || 'telle ou telle'\" in wikitext_split[5]:\n",
    "        wikitext_split[5]= wikitext_split[5].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 6 and \"||| Vous (ne) '''faites que''' prendre la mouche, aussi !'\" in wikitext_split[5]:\n",
    "        wikitext_split[5]= wikitext_split[5].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 8 and \"||| 'cent vingt, 120' || 'cent quarante, 140' || 'cent soixante, 160'\" in wikitext_split[7]:\n",
    "        wikitext_split[7]= wikitext_split[7].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "    elif len(wikitext_split)> 8 and \"||| 'autour de la table'\" in wikitext_split[7]:\n",
    "        wikitext_split[7]= wikitext_split[7].replace(\"|||\",\"||| colspan=\\\"15\\\" | \")\n",
    "\n",
    "    for i,line in enumerate(wikitext_split):\n",
    "        if line.strip().startswith('|-') or 'prettytable' in line: \n",
    "            continue\n",
    "        # if we find a line starting with \"|(\" and not a phonetic line, we have a contexte line\n",
    "        if 'CONTEXTE' in line and not contexte:\n",
    "            split_result = re.split(r'CONTEXTE', line)\n",
    "            if len(split_result) > 1:\n",
    "                contexte = split_result[1].strip()\n",
    "                continue\n",
    "        if line[:2]=='|(' and (\"[…]\" or \"traou-ze,\" or \"chouk,\"not in line) and bool(re.search(r'</?font.*?>',line))==True and not phonetic:\n",
    "            # if we find a <font> tag in the line, this is a phonetic line\n",
    "            phonetic = line[1:]\n",
    "        elif re.match(r'^\\|', line) and 'CONTEXTE' not in line and not text:\n",
    "            # otherwise first line will be text\n",
    "            text = line[1:]\n",
    "        \n",
    "        elif ('Équivalent standardisé' in line or 'équivalent écriture standard' in line or 'équivalent standard' in line or 'interprétation' in line or \"[[KLT]]\" in line) and not standardise:\n",
    "            # if the word \"équivalent standardisé\" is in the line, it's a \"standardise\" line\n",
    "            standardise = line[1:]\n",
    "        elif text and '[[' in line and not lemgloss and not 'colspan' in line and not \"agrammatical\" in line:\n",
    "            # if there is “[[” in the line and not a colspan, then it's a lemgloss \n",
    "            # most of the time lemgloss is a \"lemma-gloss\"\n",
    "            lemgloss = line[1:]\n",
    "            continue\n",
    "        elif text and re.match(r'^\\|', line) and not lemgloss and not gloss:\n",
    "            # if no [[ is found it's a gloss\n",
    "            gloss = line[1:]\n",
    "        elif 'colspan' in line and 'CONTEXTE' not in line and not translation:\n",
    "            # first colspan should be the translation\n",
    "            split_result = re.split(r'colspan=\"\\d+\".*?\\|\\s*', line)\n",
    "            if len(split_result) > 1:\n",
    "                translation = split_result[1].strip()\n",
    "        elif 'colspan' in line and '| (' in line  and 'CONTEXTE' not in line:\n",
    "            translation += '| ('+line.split('| (')[1]\n",
    "        elif ('colspan' in line and '[[*]] ' in line or 'agrammatical' in line ) and not agrammaticale:\n",
    "            # second colspan should be the agrammaticales\n",
    "            split_result = re.split(r'\\[\\*\\] ', line)\n",
    "            if len(split_result) > 1:\n",
    "                agrammaticale = split_result[1].strip()\n",
    "        elif 'colspan' in line and not info:\n",
    "            # second colspan should be the information about the source\n",
    "            split_result = re.split(r'colspan=\"\\d+\".*?\\|\\s*', line)\n",
    "            if len(split_result) > 1:\n",
    "                info = split_result[1].strip()\n",
    "        elif \"colspan\" in line and 'cité' in line:\n",
    "            info += ' cité dans '+line.split('cité')[1]\n",
    "        elif \"colspan\" in line and 'citée' in line:\n",
    "            info += ' citée '+line.split('citée ')[1]\n",
    "        elif \"colspan\" in line and '(noté' in line:\n",
    "            info += '(noté' +line.split('(noté')[1]\n",
    "        elif \"colspan\" in line and '[[' in line:\n",
    "            info += '[[' +line.split('[[')[1]\n",
    "        elif \"colspan\" in line and 'Anna' in line:\n",
    "            info+= 'Anna' +line.split('Anna')[1]\n",
    "        elif \"colspan\" in line and '[http:' in line:\n",
    "            info+= '[http:' +line.split('[http:')[1]\n",
    "        elif \"colspan\" in line and 'Finistère' in line:\n",
    "            info+= 'Finistère' +line.split('Finistère')[1]\n",
    "        elif \"colspan\" in line and 'comme' in line:\n",
    "            info+= 'comme' +line.split('comme')[1]\n",
    "        elif \"colspan\" in line and 'citant' in line:\n",
    "            info+= 'citant' +line.split('citant')[1]\n",
    "        elif \"colspan\" in line and 'interview par' in line:\n",
    "            info+= 'interview par' +line.split('interview par')[1]\n",
    "        #special case for \"||||||||||||||| [[Plourin (2000)|Plourin (2000]]:28)\"\n",
    "        elif \"|||||||||||||| \" in line:\n",
    "            info = '|||||||||||||| '+line.split('|||||||||||||| ')[1]\n",
    "        elif len(line) > 2:\n",
    "            rest+=line\n",
    "    \n",
    "    obj = {'title':title,'contexte': contexte, 'phonetic':phonetic,'text':text, 'gloss':gloss, 'standardise':standardise,'lemgloss':lemgloss,'translation':translation, 'agrammaticale': agrammaticale, 'info':info}\n",
    "    obj = info2catsource(obj)\n",
    "\n",
    "    \n",
    "    if not obj['text'] or (not obj['gloss'] and not obj['lemgloss']) :\n",
    "        rest += ' something missing!!! '\n",
    "    if rest:\n",
    "        return {**obj, 'rest':rest}\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prettyprint(item):\n",
    "    for key, value in item.items():\n",
    "            print('____',key)\n",
    "            print(str(value) )\n",
    "    print( '__________________\\n')\n",
    "\n",
    "\n",
    "\n",
    "def info2catsource(obj):\n",
    "    line_info = obj['info']\n",
    "    if not line_info:\n",
    "        return {**obj, 'category': None, 'source': None}\n",
    "        \n",
    "    category, source= None, None\n",
    "    if line_info.startswith(\"''\"):\n",
    "        line_split = line_info.split(\"'', \") # split at '',_\n",
    "        category = line_split[0][2:]\n",
    "        if len(line_split) > 1:\n",
    "            source = line_split[1]\n",
    "    elif line_info.startswith(\"[[\"):\n",
    "        # case xxxx\n",
    "        source = line_info\n",
    "    elif \"', \" in line_info:\n",
    "        line_split = line_info.split(\"', \") # split at ,_\n",
    "        category = line_split[0].replace(\"'\",\"\")\n",
    "        source = line_split[1].replace(\"'\",\"\")\n",
    "\n",
    "    else:\n",
    "        source = line_info\n",
    "        # print('Too many items: ', line_info)\n",
    "        # qsdf\n",
    "    if source:\n",
    "        source = source.split('|')[-1].replace('[[','').replace(']]','')\n",
    "        if re.search(\"(\\'\\'.*\\'\\' ?,?).*\", source):                               \n",
    "            other = re.search(\"(\\'\\'.*?\\'\\' ?,?).*\", source).group(1)                                 \n",
    "            source = source.replace(other, '').replace(\"''\",\"\") \n",
    "    # if \":\" in source: \n",
    "    #     l = re.search(\"(.*):.*\", source).group(1) + ')'\n",
    "    # else: \n",
    "    # l = source\n",
    "    # return object with categories and sources\n",
    "    return {**obj, 'category':category, 'source':source} # **obj = spread obj\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58ae1cb8",
   "metadata": {},
   "source": [
    "### Test pour la fonction avec une seule exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "9e4a0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obj = wiki2lines(\"\"\"{| class=\"prettytable\"\n",
    "# |(1)|| Al || labous'''ed'''-mor […] || n'en em || vagont || ken || nemet || diwar || pesk'''ed'''.\n",
    "# |-\n",
    "# ||| [[art|le]] || [[labous|oiseaux]]-[[mor|mer]] || [[ne]]<sup>[[1]]</sup> [[en em|se]]<sup>[[1]]</sup> || [[magañ|nourrissent]] || [[ken|pas]] || [[nemet|seulement]] || [[diwar|de]] || [[pesk|poisson]].s  \n",
    "# |-\n",
    "# ||| colspan=\"15\" | 'Les oiseaux de mer se nourrissent exclusivement de poisson.'\n",
    "# |-\n",
    "# ||||||||| colspan=\"15\" | ''Cornouaillais (Bigouden)'', [[Bijer (2007)|Bijer (2007]]:134)\n",
    "# |}\"\"\", 'test')\n",
    "# prettyprint(obj)\n",
    "# info2catsource(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a3c82",
   "metadata": {},
   "source": [
    "# Extraction globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f274024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7ab7de879445f08cd6b20b1b440e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons trouvés 14893 exemples(y compris doublons)\n",
      "0 errors in  fichiers_extraction/error_exemple.txt\n",
      "0 errors in  fichiers_extraction/missing_translations.txt\n",
      "346 exemples in ignored.txt\n",
      "3716 groupes d'exemples répétés dans all_line_objects.txt\n",
      "4515 exemples uniques dans all_line_objects.txt\n",
      "8231 exemples au total sans doublons\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "def write_errors(error_list, file_name):\n",
    "    with open(file_name, 'w') as file_error:\n",
    "        for item in error_list:\n",
    "            for key, value in item.items():\n",
    "                file_error.write(key + '\\n')\n",
    "                if key==\"title\":\n",
    "                    file_error.write('https://arbres.iker.cnrs.fr/index.php?title='+str(value) + '\\n') \n",
    "                else:\n",
    "                    file_error.write(str(value) + '\\n')\n",
    "            file_error.write( '__________________\\n')\n",
    "\n",
    "    print(len(error_list),'errors in ', file_name)\n",
    "\n",
    "exceptions = \"\"\"\n",
    "class=\"prettytable\"  <take not>\n",
    "class=\"prettytable\" <take not>\n",
    "|(6)|| bgcolor=\"#4682B4\"|<i><font color=white>traduction</font></i> |\n",
    "bgcolor=\"#4682B4\"\n",
    "''ur bern burbu'' || 'un tas de pustules' \n",
    "[[skuizh]].[[-entez]], n. f.\n",
    "''kibri'''en'''''\n",
    "kan'''er''' || 'chanteur' || vs. || kan'''iad''' ||\n",
    "''[[kelenn (V.)|kelenn]]'''adur''''' || '(une) instruction'\n",
    "(2)a. || ''c'hoant'''us''''' ||\n",
    "''deread'''elezh''''' ||\n",
    "|| ''dourek'' || 'qui contient de l'eau' ||\n",
    "|| geri'''ennoù''' || \n",
    "|| [[nom abstrait]] d'action || traduction ||\n",
    "|| ''kelennadur'' || '(une) instruction'\n",
    "|| ''Divert'''issañ''''' 'divertir'\n",
    "||<font color=green> [''''on''' ti] </font color=green> || eun ti || 'une maison'\n",
    "|| Prend '''unan arall''' || ''3 ans 10 mois, 7 mois d'exposition à la langue''\n",
    "|| Regarde '''heol 'zo bremañ'''. || ''4 ans 2 mois, 8 mois d'exposition à la langue''\n",
    "|| ''deuit '''gant n'in'''.'' || 'venez avec moi' ||\n",
    "|| ''ur GWIR vab'' || 'un vrai fils' ||<font color=green>[yr'gwi:rva:p]</font color=green> \n",
    "|| ''un hanTER torzh'' || 'une [[hanter|demi]] [[torzh|miche]]' ||<font color=green>\n",
    "|| ''hanTER-kant'' || [[hanter|demi]]-cent', 50|| || ''Léonard (Cleder)''\n",
    "|| ''ur gwir '''v'''ab || 'un vrai fils' || [[Kervella (1947)|Kervella (1947]]:§76)\n",
    "|| ''e berr '''g'''omzoù'' || [[Kervella (1947)|Kervella (1947]]:§513)\n",
    "|| ''ur brizh '''k'''risten'' || 'un mauvais chrétien' || [[Menard & Kadored (2001)|Menard & Kadored (2001]]:'brizh')\n",
    "|| '''am'''zor \n",
    "<font color=green>1SG</font color=green>\n",
    "(1)|| N'int || ket || aet || war-raok.\n",
    "|| mot 1' mot 2 || mot 3 || mot 4 || mot 5-mot 6.\n",
    "|| Pet || vloaz || en deus || ho || '''chas''' ? \n",
    "Ne || ra || ket || netra || vs. / || b.?? || Ne || ra || ket || NETRA\n",
    "|| c'hoant'''us''' || '(homme) disposé à\n",
    "|| cosas || que || '''no''' || sirven || || para || '''nada'''. |||| ''Castillan''\n",
    "<i><font color=green>1SG</font></i>\n",
    "|| ''[[kas]], '''de'''gas ||\n",
    "équipe permanente porteuse de projet > ||\n",
    "bgcolor=#5F9EA0\n",
    "''[[glav]]'' || 'pluie' || '''''di'''c'hlav''\n",
    "|||| Who do you think\n",
    "bgcolor=#ccffcc\n",
    "|| '''attestation avec /s-/''' ||\n",
    "(A)|| assertions fortes, verbes déclaratifs || ''dire, rapporter, être vrai que, être certain que, être sur, être évident...\n",
    "(3)|| ''X'' || lavar || ''un dra bennak'' || da ''unan bennak'' ||> || ''X'' [[lavarout|dire]] ''quelque chose'' à ''quelqu'un''\n",
    "(p.9)|| ASTERIKS (dezhañ e unan): || '''Ez an d''' 'e heuliañ a-bell....\n",
    "(1)|| colspan=\"15\" |  אני מדברת ברטונית, ומה אתך?\n",
    "(3)|| nom masculin || évidence |||| source\n",
    "(3)|| ''GWALL vad'' || 'très bon' ||\n",
    "(2)|| racine nominale ||> || reduplication\n",
    "(2)|| 'qui ?' || '''''siapa''''' ||> 'quiconque' || ''siapa saja'', || ou '''''sapia-sapia'''''\n",
    "(1)|| ''nom testé'' || ''accord'' || ''pronom'' || ''5  X'' || ''Niverus eo X||\n",
    "<i><font color=green>3SGM</font></i> \n",
    "(1)|| déterminant || (adjectif) || nom || (adjectif) \n",
    "|| les [[articles]] || ''an, al, ar, un, ul, ur''\n",
    "(1)|| Ar plant n' {int / [[*]] eo } ket glas. || (plantenn, || plantennoù)\n",
    "Classe A:|| ''say, report, exclaim, assert, claim, vow, be true, be certain, be sure, be obvious\n",
    "<u>Kapios fititis</u>\n",
    "Magazennoù a oa, peogwir ar brikoli a oa gwerzhet de ar penn kwir, ne peze james da gont.'' \n",
    "<font color=green>[</font color=green><sub>[[AspP]]</sub> || Sujet  '''Asp°''' \n",
    "(1)|| || singulier || pluriel interne || pluriel suffixal || sources\n",
    "* ar vugale || '''ne''' ||\n",
    "carte [http:\n",
    "(1)||<font color=green>/be/ || ''[[bezañ|beza]]'' || 'être'\n",
    "|| syllabe || = || [<sub>syllabe</sub> || attaque || [<sub>rime</sub> [ || noyau ]  [ || coda ] || ] ]\n",
    "(1)|| colspan=\"15\" |Inventaire (non exhaustif) des attaques\n",
    "Inventaire (non exhaustif) des codas\n",
    "(1)|| ... |||| verbe || ... || <u><font color=red>sujet</font color=red></u> |||| <fo\n",
    "(1) || ''Ober pas so hasou,'' || ''hep gaou pan dezraouet''\n",
    "(3)|| '''krag'''-[[aotrou]], 'petit monsieur' || [[Merser (2009)]]\n",
    "(1)|| Ha || c'hwi, || ma || flah || bihan, || ma || 'ho pije || gwelet || an traou-ze,\n",
    "(1)|| nom féminin || évidence || traduction || source\n",
    "'The boy raised his eyes.'\n",
    "'I am inclined to say that the teacher is right.'\n",
    "|(4)|| Sevel || a || reas || prim || he || brozh || du, ...\n",
    "(24)|| a. ||*|| Musa || ba || nakàn || '''ni:''' || '''o'''. \n",
    "(5)|| ''Je suis allé'' || : || oeit '''on''' || / || '''em es''' oeit ||\n",
    "(1)b. || '''''Deoc'h''' e ministraf a graf'' || 'Je vous \n",
    "|(3)a. || ''He dimiziff … '''ne''' riff '''quet''''' || 'Je ne l'épouserai pas.' \n",
    "(A)|| assertions fortes, [[verbes déclaratifs]] || '''''Lavaret''' en deus e vije deuet hirio''.\n",
    "||| [[art|un]] agneau [[bihan|petit]] || [[debriñ|mang]].[[-et (Adj.)|é]] || par || [[art|un]] pourceau\n",
    "Gris-'''en''' med lång svans grymtade. \n",
    "||| [[R]] [[kaout|avait]] || [[kaout|eu]] || [[c'hoant|envie]] || donner || [[da|à]].[[pronom incorporé|nous]] \n",
    "||| [[DEM|ceux.là]] || <font color=green>[</font color=green> [[evit|pour]] || [[art|le]] [[bloaz|an]] || [[les numéraux ordinaux|premier]] ||\n",
    "''skañvder'', ''skañvded''\n",
    "(1)|| ''rakkarvan'''egezh''''' || 'prognathisme' \n",
    "(2)|| 'A:ñi || ''' 'añ''' ||<font color=green>[</font color=green> S-kí'ikig || wui <font color=green>]</font color=green> || hí:.\n",
    "(2)|| || singulier || pluriel interne || sources\n",
    "(1)|| || singulier || pluriel interne || sources\n",
    "(1)|| Setu, en ur zont d'ar gêr e choment a-sav eno neuze. \n",
    "(1)|| ''un doez'''en''''' || 'un épi' || ''en toez'''ad''''' || 'les épis d'un champ'\n",
    "(3)|| '''''hur bout''' a ramb'' || 'nous avons'\n",
    "(5)|| Elle (ne) '''fait que''' dormir de toute la journée ! \n",
    "(9)|| «Skrivet eo er brofeted: « '''Oll''' e vezint kelennet gand Doue». \n",
    "(5)|| c'hoarzh'''us''' > '''di'''c'hoarzh || pleg'''us''' > '''di'''bleg\n",
    "(8)|| '''ki''' || > dour'''gi''', ''Standard'', [[Kervella (1947)|Kervella (1947]]:§299) || / || > '''ki'''dour, [[Favereau (1993)|Favereau (1993]]:'loutre')\n",
    "<font color=green>1PL\n",
    "(1)|| groupe nominal || > || reduplication\n",
    "||| an den-mañ > || an den-mañ-''''n''' den || ar c'hi-mañ > \n",
    "|| raedenn || OK|| _ || _ || [[*]] || _ || _ || OK||\n",
    "|| || 80 vl. || < 80 vl. || > 50 vl. || < 60 vl.\n",
    "(1)|| bgcolor=#66CDAA| Flav! || bgcolor=#66CDAA| Klak ! || bgcolor=#66CDAA| Badadav! ||\n",
    "(1)|| <font color=orange>La souris</font color=orange> || chatouille ||<font color=green>le chat</font color=green>. \n",
    "(2)|| ''nom testé'' || ''[[réciproques]]'' || ''[[quantifieurs flottants|Q flottants]] distr.'' || ''[[pep]] a X'' ||\n",
    "(1)|| ''[[seurt]]'' 'sorte' || > || '''''lïes'''seurt'' 'multiple'\n",
    "(1)|| Lavaret || o deus || || '''e veho''' || brao || an amzer || warc'hoazh.\n",
    "|| maen || _ || _ || _ || _ || _ || _ || SG/ [[*]] PL|| _ || OK|| [[*]]\n",
    "(9)|| { ? ur c'harredad / [[*]] ur bern } || gwez\n",
    "(10)|| [[*]] ur c'hilo || gwez\n",
    "(11)|| { [[*]] ul lodenn / [[*]] un tamm } || gwez\n",
    "|| fubu || PL || PL || [[*]] || OK || OK || OK || SG/PL|| _ || _ || _\n",
    "(4)|| '''ki''' || > dour'''gi''', ''Standard'', [[Kervella (1947)|Kervella (1947]]:§299) || / || > '''ki'''dour, [[Favereau (1993)|Favereau (1993]]:'loutre')\n",
    "(9)|| «Skrivet eo er brofeted: « '''Oll''' e vezint kelennet gand Doue».\n",
    "(1)|| ''[[-ed (N.)|-ed]]'' || ''elvo'''ed''''' 'peupleraie', [[Ménard (2012)]]\n",
    "(3)|| Lavaret || o deus || || '''e veho''' || brao || an amzer || warc'hoazh.\n",
    "(2)|| ''3SGM || 'M eus ket'''oñ''' || karet\n",
    "(4)|| suffixe celtique insulaire > || nom verbal\n",
    "(4)|| ''ar PaOur-kêz PaOtr'' || 'le pauvre garçon' ||<font color=green>[ar ˌpokɛz 'po:tr]</font color=green> || ''Plozévet'', [[Goyat (2012)|Goyat (2012]]:194)\n",
    "(2)|| '''hanter'''-'''v'''ezw. ||\n",
    "bgcolor=#FA8072\n",
    "|| colspan=\"15\" | <font color=orange>'''A'''</font color=orange> : 'Merci' / <font color=orange>'''B'''</font color=orange> : 'De rien.', 'C'est peu de chose.'\n",
    "(3)|| <u>Ar re-mañ</u>, || evid || ar bloaz || kenta || e oant || e Huelgoad, \n",
    "\"\"\".strip().split('\\n')\n",
    "\n",
    "\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "count_exceptions = 0\n",
    "alllineobjects = []\n",
    "error_exemple = []\n",
    "missing_translations = []\n",
    "exampleFile = open('fichiers_extraction/examples.txt', 'w')\n",
    "ignoredFile = open('fichiers_extraction/ignored.txt', 'w')\n",
    "allc=0\n",
    "for title in tqdm.tqdm(pages):\n",
    "    exampleFile.write(title + '\\n_____________________________\\n')\n",
    "    wikitext = pages[title]\n",
    "    counter = 0\n",
    "\n",
    "    for m in repretty.finditer(wikitext):\n",
    "        # special tables that are not examples:\n",
    "        # https://arbres.iker.cnrs.fr/index.php?title=-ach,_-aj&action=edit\n",
    "        cont = False\n",
    "        for ex in exceptions:\n",
    "            if ex in m.group(0):\n",
    "                count_exceptions += 1\n",
    "                # Verifier les exemples qui ne sont pas pris en compte vient de quelles exceptions\n",
    "                ignoredFile.write('https://arbres.iker.cnrs.fr/index.php?title='+title+'\\n'+m.group(0)+'\\n\\n')\n",
    "                cont = True\n",
    "                break\n",
    "        if cont: continue\n",
    "        exampleFile.write('https://arbres.iker.cnrs.fr/index.php?title='+title+'\\n')\n",
    "        exampleFile.write(m.group(0) + '\\n________________\\n')\n",
    "        result = wiki2lines(m.group(0), title)\n",
    "        result['link'] = 'https://arbres.iker.cnrs.fr/index.php?title='+title\n",
    "        alllineobjects.append(result)\n",
    "        if result.get('rest'):\n",
    "            error_exemple.append(result)\n",
    "        if not result.get('translation') :\n",
    "            missing_translations.append(result)\n",
    "        # if  not result.get('rest') and not result.get('translation') :\n",
    "        #     print('hello',result)\n",
    "        counter += 1\n",
    "\n",
    "    exampleFile.write('\\n' + str(counter) + ' exemples trouvés dans la page ' + title + '\\n================================\\n\\n\\n')\n",
    "    # Ajouter le nombre d'exemples trouvés à la variable totale\n",
    "    allc+=counter\n",
    "# Fermer le fichier\n",
    "exampleFile.close()\n",
    "# Afficher le nombre total d'exemples\n",
    "print(\"Nous avons trouvés\", allc,\"exemples(y compris doublons)\")\n",
    "\n",
    "grouped_data = {}\n",
    "for line_object in alllineobjects:\n",
    "    translation = line_object['translation']\n",
    "    if translation not in grouped_data:\n",
    "        grouped_data[translation] = []\n",
    "    grouped_data[translation].append(line_object)\n",
    "\n",
    "nombre_example_list_plus_1 = 0\n",
    "nombre_example_list_equal_1 = 0\n",
    "with open('fichiers_extraction/exemples_répétés.txt', 'w') as file:\n",
    "    for translation_key, example_list in grouped_data.items():\n",
    "        if len(example_list) > 1:\n",
    "            nombre_example_list_plus_1 += 1\n",
    "            file.write('Il y a ' + str(len(example_list)) + \" exemple(s) qui sont répétés dans ce groupe\" + '\\n')\n",
    "            for example_obj in example_list:\n",
    "                for k, v in example_obj.items():\n",
    "                    file.write(k + '\\n')\n",
    "                    file.write(str(v) + '\\n')\n",
    "                file.write( '__________________\\n')\n",
    "            file.write( '__________________end___________________\\n')\n",
    "        else:\n",
    "            nombre_example_list_equal_1 += 1\n",
    "# Écrire les dictionnaires dans un fichier texte\n",
    "with open('fichiers_extraction/all_line_objects.txt', 'w') as file:\n",
    "    for item in alllineobjects:\n",
    "        for key, value in item.items():\n",
    "            file.write(key + '\\n')\n",
    "            file.write(str(value) + '\\n')\n",
    "        file.write( '__________________\\n')\n",
    "\n",
    "write_errors(error_exemple,'fichiers_extraction/error_exemple.txt')\n",
    "write_errors(missing_translations,'fichiers_extraction/missing_translations.txt')\n",
    "print(count_exceptions, \"exemples in ignored.txt\")\n",
    "print(nombre_example_list_plus_1, \"groupes d'exemples répétés dans all_line_objects.txt\")\n",
    "print(nombre_example_list_equal_1, \"exemples uniques dans all_line_objects.txt\")\n",
    "print(nombre_example_list_plus_1+nombre_example_list_equal_1, \"exemples au total sans doublons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacac304",
   "metadata": {},
   "source": [
    "# Create a couple.txt (store text,its translation, its categories, its source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c7e46267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couple.txt created successfully!\n"
     ]
    }
   ],
   "source": [
    "def cleanToken(t):\n",
    "    t = t.replace(\"...\", \"\")\n",
    "    t = t.replace(\"tout.à.l'heure\", \"tout_à_l'heure\")\n",
    "\n",
    "    t = t.replace('<font color=green>', '')\n",
    "    t = t.replace('</font color=green>', '')\n",
    "\n",
    "    t = t.replace(\"<u>\", \"\")\n",
    "    t = t.replace(\"</u>\", \"\")\n",
    "    t = t.replace(\"<sub>\", \"\")\n",
    "    t = t.replace(\"</sub>\", \"\")\n",
    "    t = t.replace(\"<sub></sub>\", \"\")\n",
    "    return t\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "import re\n",
    "\n",
    "\n",
    "def alignTokLemmgloss(text, lemgloss, phonetic):\n",
    "    '''\n",
    "    takes three lines of an example, produces triples\n",
    "    '''\n",
    "    pattern = r'\\[\\[(\\w+)\\|\\|(\\w+)\\]\\]'\n",
    "    lemgloss = re.sub(pattern, r'[[\\1|\\2]]', lemgloss)\n",
    "\n",
    "    text_list = text.split(\"||\")\n",
    "    lemgloss_list = lemgloss.split(\"||\")\n",
    "    phon_list = phonetic.split(\"||\")\n",
    "    couple_list = []\n",
    "\n",
    "    if any(\"KLT\" in tok or \"Sujet\" in tok for tok in text_list) or any(\n",
    "            \"KLT\" in lem or \"Sujet\" in lem for lem in lemgloss_list):\n",
    "        return couple_list\n",
    "\n",
    "    text_list_stripped = [tok.strip() for tok in text_list]\n",
    "    lemgloss_list_stripped = [lem.strip() for lem in lemgloss_list]\n",
    "\n",
    "    text_new_list_stripped = []\n",
    "    lemme_new_list_stripped = []\n",
    "\n",
    "    for tok, lem in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "        if ']]-' in lem:\n",
    "            parts = lem.split(']]-')\n",
    "            if len(parts) > 2:\n",
    "                lemme_new_list_stripped.append(parts[0] + ']]')\n",
    "                lemme_new_list_stripped.append(parts[1] + ']]')\n",
    "                lemme_new_list_stripped.append(parts[2])\n",
    "            else:\n",
    "                lemme_new_list_stripped.append(parts[0] + ']]')\n",
    "                lemme_new_list_stripped.append('[[' + parts[1])\n",
    "\n",
    "            word_to_find = parts[1].split('|')[0].replace('[[', '')\n",
    "\n",
    "            if \"-\" + word_to_find in tok:\n",
    "                text_parts = tok.split('-')\n",
    "                if len(text_parts) > 2:\n",
    "                    text_new_list_stripped.append(text_parts[0])\n",
    "                    text_new_list_stripped.append(\"-\" + text_parts[1])\n",
    "                    text_new_list_stripped.append(\"-\" + text_parts[2])\n",
    "                else:\n",
    "                    text_new_list_stripped.append(text_parts[0])\n",
    "                    text_new_list_stripped.append(\"-\" + text_parts[1])\n",
    "            else:\n",
    "                text_new_list_stripped.append(tok)\n",
    "        elif any(word in tok.lower().split(\" \") for word in [\"un\", \"ul\", \"ur\", \"an\", \"al\", \"ar\"]) and ' ' in tok:\n",
    "        \n",
    "            parts = tok.split(' ')\n",
    "            new_tokens = []\n",
    "            for i, part in enumerate(parts):\n",
    "                if any(word in part.lower().split(\" \") for word in [\"un\", \"ul\", \"ur\", \"an\", \"al\", \"ar\"]):\n",
    "                \n",
    "                    new_tokens.append(part)\n",
    "\n",
    "                else:\n",
    "                    new_tokens.append(part)\n",
    "\n",
    "            text_new_list_stripped.extend(new_tokens)\n",
    "            if any(mark in lem for mark in [\"[[an, al, ar|le]]\", \"[[un, ul, ur|un]]\"]):\n",
    "        \n",
    "                for mark in [\"[[an, al, ar|le]]\", \"[[un, ul, ur|un]]\"]:\n",
    "                    if mark in lem:\n",
    "                        index = lem.find(mark)\n",
    "                        lem_parts = lem.split(mark)\n",
    "                        lemme_new_list_stripped.extend([lem_parts[0] + mark, mark.join(lem_parts[1:])])\n",
    "                        break\n",
    "            else:\n",
    "                lemme_new_list_stripped.append(lem)\n",
    "\n",
    "        else:\n",
    "            lemme_new_list_stripped.append(lem)\n",
    "            text_new_list_stripped.append(tok)\n",
    "\n",
    "    for index, lem in enumerate(lemme_new_list_stripped):\n",
    "       ##traiter les exemples suivants:\n",
    "       # <sup>[[1]]</sup>bouteill.[[-ad|ée]]\"\n",
    "       # <sup>[[1]]</sup>[[kêr|foyer]]\n",
    "       #<sup>[[1]]</sup>[[paotr|homme]].[[-ed (PL.)|s]]\n",
    "       # <sup>[[1]]</sup>[[moger|mur]].erie \n",
    "        pattern_mutchoice_right = r\"<sup>\\[\\[(\\d+)\\]\\]</sup>\"\n",
    "        if lem.startswith(\" <sup>\"):\n",
    "            matches = re.findall(pattern_mutchoice_right, lem)\n",
    "            if matches:\n",
    "                index_left = -1\n",
    "                for i in range(0, 4): \n",
    "                    index_left = lem.find('[', index_left + 1) \n",
    "                index_right = -1\n",
    "                for i in range(0, 3): \n",
    "                    index_right = lem.find(']', index_right + 1)\n",
    "                if index_left != -1 and index_right != -1:\n",
    "                    lemme_new_list_stripped[index]= lem[:index_right] + 'Mut=' + matches[0] + 'Mutchoice=right' + lem[index_right:]\n",
    "                    # print(lem)\n",
    "    text_list_stripped = text_new_list_stripped\n",
    "    lemgloss_list_stripped = lemme_new_list_stripped\n",
    "\n",
    "    phon_list_stripped = [phon.strip() for phon in phon_list]\n",
    "\n",
    "    if \"standard\" in text.lower() or \"standard\" in lemgloss.lower() or \"Équivalent\" in text or \"Graphie\" in text:\n",
    "        return couple_list\n",
    "\n",
    "    if text_list_stripped and text_list_stripped[-1] == \"\":\n",
    "        text_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"<elles>\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"_\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and re.search(r'\\d{4}', lemgloss_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and re.search(r'\\d{4}', text_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and \"colspan\" in text_list_stripped[-1]:\n",
    "        return couple_list\n",
    "    if \"[[*]]\" in text_list_stripped:\n",
    "        supprimer = text_list_stripped.index(\"[[*]]\")\n",
    "        text_list_stripped.pop(supprimer)\n",
    "        # this indicates that the sentence is ungrammatical, and we skip it\n",
    "        return couple_list\n",
    "\n",
    "    if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "\n",
    "        if phonetic == \"None\":\n",
    "            for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                tok = cleanToken(tok)\n",
    "                lemmgloss = cleanToken(lemmgloss)\n",
    "                if tok and lemmgloss:\n",
    "                    couple_list.append((tok, lemmgloss, 'None'))\n",
    "        elif phonetic != 'None':\n",
    "            for tok, lemmgloss, phone in zip(text_list_stripped, lemgloss_list_stripped, phon_list_stripped):\n",
    "                tok = cleanToken(tok)\n",
    "                lemmgloss = cleanToken(lemmgloss)\n",
    "                phone = cleanToken(phone)\n",
    "                if tok and lemmgloss and phone:\n",
    "                    couple_list.append((tok, lemmgloss, phone))\n",
    "\n",
    "    else:\n",
    "        punctuation_marks = ['!', '.', ',', '?', ';', ':']\n",
    "        for punctuation in punctuation_marks:\n",
    "            if text_list_stripped and text_list_stripped[-1].endswith(punctuation) and not lemgloss_list_stripped[\n",
    "                -1].endswith(punctuation):\n",
    "                lemgloss_list_stripped.append(punctuation)\n",
    "\n",
    "        if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "            if phonetic == \"None\":\n",
    "                for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                    tok = cleanToken(tok)\n",
    "                    lemmgloss = cleanToken(lemmgloss)\n",
    "                    if tok and lemmgloss:\n",
    "                        couple_list.append((tok, lemmgloss, 'None'))\n",
    "            elif phonetic != 'None':\n",
    "                for tok, lemmgloss, phone in zip(text_list_stripped, lemgloss_list_stripped, phon_list_stripped):\n",
    "                    tok = cleanToken(tok)\n",
    "                    lemmgloss = cleanToken(lemmgloss)\n",
    "                    phone = cleanToken(phone)\n",
    "                    if tok and lemmgloss and phone:\n",
    "                        couple_list.append((tok, lemmgloss, phone))\n",
    "        else:\n",
    "            for punctuation in punctuation_marks:\n",
    "                if lemgloss_list_stripped and lemgloss_list_stripped[-1].endswith(punctuation) and not \\\n",
    "                        text_list_stripped[-1].endswith(punctuation):\n",
    "                    text_list_stripped.append(punctuation)\n",
    "\n",
    "            if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "                if phonetic == \"None\":\n",
    "                    for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                        tok = cleanToken(tok)\n",
    "                        lemmgloss = cleanToken(lemmgloss)\n",
    "                        if tok and lemmgloss:\n",
    "                            couple_list.append((tok, lemmgloss, 'None'))\n",
    "                elif phonetic != 'None':\n",
    "                    for tok, lemmgloss, phone in zip(text_list_stripped, lemgloss_list_stripped, phon_list_stripped):\n",
    "                        tok = cleanToken(tok)\n",
    "                        lemmgloss = cleanToken(lemmgloss)\n",
    "                        phone = cleanToken(phone)\n",
    "                        if tok and lemmgloss and phone:\n",
    "                            couple_list.append((tok, lemmgloss, phone))\n",
    "\n",
    "            else:\n",
    "                with open(\"fichiers_traitement/not_couple.txt\", \"a\") as file:\n",
    "                    file.write(\"text\\n\")\n",
    "                    file.write(f\"{text}\\n\")\n",
    "                    file.write(\"lemgloss\\n\")\n",
    "                    file.write(f\"{lemgloss}\\n\")\n",
    "    return couple_list\n",
    "\n",
    "\n",
    "# make the couple into a dico including tok, lemma and gloss\n",
    "def couple2tokobjects(triple):\n",
    "    text, lem, phon = triple\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\].F<sup>\\[\\[1\\]\\]</sup>\", \"[[div|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|quatre\\]\\]<sup>\\[\\[2\\]\\]</sup>\", \"[[peder|quatre]]\", lem)\n",
    "\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup> \\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]\", \"[[gant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[kant|cent]]\", lem)\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup>\\[\\[cardinal\\|cent\\]\\]\", \"[[c'hant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[50|50]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[zaou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[dek\\|dix\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|dix\\]\\]\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]\", \"[[50|50]]\", lem)\n",
    "\n",
    "    if 'diou' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\.F\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[diou|deux.F]]\", lem)\n",
    "    if 'vil' in text:\n",
    "        lem = re.sub(r\"<sup>\\[\\[1\\]\\]</sup>\\[\\[cardinal\\|mille\\]\\]\", \"[[vil|mille]]\", lem)\n",
    "    if 'diouig' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]\", \"[[diou|2]]\", lem)\n",
    "    if \"daou-ha-daou\" in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "\n",
    "    lem = re.sub(r'<sup>(\\[\\[[^\\[\\]]+\\]\\])(.*?)</sup>',\n",
    "                 lambda m: ''.join(f'<sup>{x}</sup>' for x in re.findall(r'\\[\\[[^\\[\\]]+\\]\\]', m.group(0))), lem)\n",
    "    lem = re.sub('particule o', 'o', lem)\n",
    "\n",
    "    lem = re.sub(r'<font color=green>.*?</font color=green>', '', lem)\n",
    "\n",
    "    phon = re.sub(r'\\[', '', phon)\n",
    "    phon = re.sub(r'\\]', '', phon)\n",
    "\n",
    "    if '[[R]]' not in lem:\n",
    "        match = re.search(r'<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match:\n",
    "            k = match.group(2).strip(']')\n",
    "            lem = re.sub(r'<sup>\\[\\[(?!(1|4)\\]\\]).*?</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        if \"particule o\" not in lem:\n",
    "            lem = re.sub(r'<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' in lem:\n",
    "        match1 = re.search(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match1:\n",
    "            k = match1.group(2).strip(']')\n",
    "            lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' not in lem:\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]', '[[R|_]]', lem)\n",
    "        k = 'R'\n",
    "\n",
    "    if \"'''\" in text and '-' in lem and re.search(r'\"\"\".*?\"\"\"', text):\n",
    "        extra_lem = re.search(r\"'''(.*?)'''\", text).group(1)\n",
    "        match = re.search(r'(?<=\\|{2}).*?-(.*?)(?=\\|\\|)', lem)\n",
    "\n",
    "        extra_gloss = match.group(1).strip()\n",
    "        lem = re.sub(r'-(.*?)\\|\\|', f'[[{extra_lem}|{extra_gloss}]] ||', lem)\n",
    "\n",
    "    text_list = text.split('||')\n",
    "    lem_list = lem.split('||')\n",
    "    phone_list = phon.split('||')\n",
    "\n",
    "    tok_objects = []\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        tok = text_list[i].strip()\n",
    "        lem_parts = lem_list[i].strip().split(']]')\n",
    "        lem_sp = lem_list[i].strip().split('|')\n",
    "        phonetic = phone_list[i].strip()\n",
    "\n",
    "        import string\n",
    "\n",
    "        tok_list = [part for part in tok.split(' ') if not all(char in string.punctuation for char in part)]\n",
    "        phon_list = [part2 for part2 in phon.split(' ') if not all(char2 in string.punctuation for char2 in part2)]\n",
    "\n",
    "        if len(lem_sp) == 1 and tok.startswith(\"'''\"):\n",
    "            tok_objects.append({\n",
    "                'tok': tok.replace(\"'''\", ''),\n",
    "                'lemma': tok.replace(\"'''\", ''),\n",
    "                'gloss': lem_parts[0],\n",
    "                'phonetic': phonetic.replace(\"'''\", '')\n",
    "            })\n",
    "\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and ('SC' or 'VP' or 'PredP' or 'CP' or 'DP') in lem_sp[0]:\n",
    "            match = re.search(r'<sub>\\[\\[(.*?)\\]\\]', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'PhraseStructure': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and '[[' not in lem_sp[0]:\n",
    "            match = re.search(r'<sub>(.*?)</sub>', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'index': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 2 and '[[' in lem_sp[0]:\n",
    "\n",
    "            gloss = lem_sp[1].rstrip('].')\n",
    "\n",
    "            if 'cardinal' in lem_sp[0] and 'cardinal' not in tok:\n",
    "                lem_sp[0] = tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "            if \"cardinaux\" in lem_sp[0]:\n",
    "                lem_sp[0] = tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "            elif 'pronom in' in lem:\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'me', lem_sp[0]) if 'moi' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'te', lem_sp[0]) if 'toi' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'eñ', lem_sp[0]) if 'lui' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'hi', lem_sp[0]) if 'elle' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'hon', lem_sp[0]) if 'nous' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'hoc\\'h-unan', lem_sp[0]) if 'vous' in gloss else ''\n",
    "                lem_sp[0] = re.sub(r'\\[\\[pronom incorporé', 'int', lem_sp[0]) if 'eux' in gloss else ''\n",
    "\n",
    "\n",
    "\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0],\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "            else:\n",
    "                gloss = lem_sp[1].rstrip('].')\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0].strip('[]'),\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "\n",
    "        elif len(tok_list) == len(lem_parts) - 1:\n",
    "            for i in range(len(tok_list)):\n",
    "                lem_inside = lem_parts[i].split('|')[0]\n",
    "                gloss_inside = lem_parts[i].split('|')[-1]\n",
    "\n",
    "                if 'numéraux' in lem_inside:\n",
    "                    lem_inside = \"[[\" + tok\n",
    "\n",
    "                if \"cardinaux\" in lem_inside:\n",
    "                    lem_inside = \"[[\" + tok\n",
    "\n",
    "                if '-ig' in lem_inside:\n",
    "                    gloss_inside = 'petit'\n",
    "\n",
    "                if 'cardinal' in lem_inside and 'cardinal' not in tok:\n",
    "                    lem_inside = tok\n",
    "                    pattern = r\"[^\\w\\s]+\"\n",
    "                    lem_inside = re.sub(pattern, \"\", lem_inside)\n",
    "                    lem_inside = '[[' + lem_inside\n",
    "\n",
    "                if 'pronom incorporé' in lem_inside:\n",
    "                    lem_inside = tok\n",
    "\n",
    "                if len(tok_list) == len(phon_list):\n",
    "                    tok_objects.append({'tok': tok_list[i],\n",
    "                                        'lemma': lem_inside[2:],\n",
    "                                        'gloss': gloss_inside,\n",
    "                                        'phonetic': phon_list[i] if phon_list[i] else 'None'\n",
    "\n",
    "                                        })\n",
    "                else:\n",
    "                    tok_objects.append({'tok': tok_list[i],\n",
    "                                        'lemma': lem_inside[2:],\n",
    "                                        'gloss': gloss_inside\n",
    "\n",
    "                                        })\n",
    "                    if phonetic != 'None':\n",
    "                        with open('fichiers_traitement/error_phon.txt', 'a') as fi:\n",
    "                            fi.write(f'text\\n{text}\\n')\n",
    "                            fi.write(f'phonetic\\n{phon}\\n')\n",
    "                            fi.write((f'______________\\n'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            tok_objects.append({\n",
    "                'tok': tok,\n",
    "                'lemma': '_',\n",
    "                'gloss': '_',\n",
    "                'phonetic': phonetic if phonetic else '_',\n",
    "                'multiple': len(lem_parts) - 1})\n",
    "            tok_obj = {'tok': tok, 'lemma': '_', 'gloss': '_', 'multiple': len(lem_parts) - 1}\n",
    "            tok_n = 1\n",
    "\n",
    "            for j in range(len(lem_parts)):\n",
    "                if '[[' in lem_parts[j]:\n",
    "                    lemma_gloss = lem_parts[j].strip('[[').split('|')\n",
    "                    lemma = lemma_gloss[0].split('[[')[-1].strip()\n",
    "\n",
    "                    if re.search(r'^-\\w+', lemma):\n",
    "                        if lemma != '-ig':\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 and tok != '-ig' else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': 'petit'\n",
    "                            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    elif lemma_gloss == ['R', '_']:\n",
    "                        if k != '+C':\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else '',\n",
    "                                'Epenthesis': 'Yes'\n",
    "                            })\n",
    "\n",
    "                    elif lemma_gloss == ['a', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '1',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    elif lemma_gloss == ['e', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '4',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    elif 'cardinal' in lemma_gloss[0]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma_gloss[1],\n",
    "                            'lemma': lemma_gloss[1],\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardinaux\" in lemma_gloss[0] and 'cent' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'kant',\n",
    "                            'lemma': 'kant',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif 'cardi' in lemma_gloss[0] and '9' in lemma_gloss[1]:\n",
    "\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'nav',\n",
    "                            'lemma': 'nav',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'trois' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'vingt' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'ugent',\n",
    "                            'lemma': 'ugent',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '2' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'douze' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daouzek',\n",
    "                            'lemma': 'daouzek',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'deux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '3' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'un' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'unan',\n",
    "                            'lemma': 'unan',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'lui' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'eñ',\n",
    "                            'lemma': 'eñ',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'moi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'me',\n",
    "                            'lemma': 'me',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'toi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'te',\n",
    "                            'lemma': 'te',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'nous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hon',\n",
    "                            'lemma': 'hon',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'elle' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hi',\n",
    "                            'lemma': 'hi',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'vous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"hoc'h-unan\",\n",
    "                            'lemma': \"hoc'h-unan\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'eux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"int\",\n",
    "                            'lemma': \"int\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'il' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"enni\",\n",
    "                            'lemma': \"enni\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'ça' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"se\",\n",
    "                            'lemma': \"se\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma.strip(),\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                        if '-ig' in tok_obj['tok'] and '-ig' in tok_obj['lemma']:\n",
    "                            tok_obj['gloss'] = 'petit'\n",
    "\n",
    "                        if 'numéraux' in tok_obj['lemma']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "\n",
    "                        if 'cardinal' in tok_obj['lemma'] and 'cardinal' not in tok_obj['tok']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "                            pattern = r\"[^\\w\\s]+\"\n",
    "                            tok_obj['lemma'] = re.sub(pattern, \"\", tok_obj['lemma'])\n",
    "\n",
    "                    tok_n += 1\n",
    "\n",
    "    return tok_objects\n",
    "\n",
    "\n",
    "#########################################\n",
    "import re\n",
    "\n",
    "\n",
    "def whole_lines():\n",
    "    filename = \"fichiers_extraction/all_line_objects.txt\"\n",
    "    all_examples = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    example = {}\n",
    "    store_text = False\n",
    "    store_lemgloss = False\n",
    "    store_phonetic = False\n",
    "    store_translation = False\n",
    "    store_source = False\n",
    "    store_category = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if store_text:\n",
    "            if line.startswith(\"||\"):\n",
    "                example[\"text\"] = line[0:]\n",
    "            else:\n",
    "                example[\"text\"] = line[3:]\n",
    "            store_text = False\n",
    "        elif store_lemgloss:\n",
    "            example[\"lemgloss\"] = line\n",
    "            store_lemgloss = False\n",
    "        elif store_phonetic:\n",
    "            example[\"phonetic\"] = line\n",
    "            store_phonetic = False\n",
    "        elif store_translation:\n",
    "            example[\"translation\"] = line\n",
    "            store_translation = False\n",
    "        elif store_category:\n",
    "            example[\"category\"] = line\n",
    "            store_category = False\n",
    "        elif store_source:\n",
    "            example[\"source\"] = line\n",
    "            store_source = False\n",
    "            if example[\"text\"] != \"\" and example[\"lemgloss\"] != \"None\":\n",
    "                all_examples.append(example)\n",
    "                example = {}  # Create a new dictionary for the next example\n",
    "\n",
    "        if line == \"text\":\n",
    "            store_text = True\n",
    "        elif line == \"lemgloss\":\n",
    "            store_lemgloss = True\n",
    "        elif line == \"phonetic\":\n",
    "            store_phonetic = True\n",
    "        elif line == \"translation\":\n",
    "            store_translation = True\n",
    "        elif line == \"category\":\n",
    "            store_category = True\n",
    "        elif line == \"source\":\n",
    "            store_source = True\n",
    "\n",
    "    with open(\"fichiers_traitement/couple.txt\", \"w\") as file:\n",
    "        for example in all_examples:\n",
    "            text = example[\"text\"]\n",
    "            lemgloss = example[\"lemgloss\"]\n",
    "            phonetic = example.get(\"phonetic\", \"None\")\n",
    "            translation = example.get(\"translation\", \"None\")\n",
    "            source = example.get(\"source\", None)\n",
    "            category = example.get(\"category\", None)\n",
    "\n",
    "            triplets = alignTokLemmgloss(text, lemgloss, phonetic)\n",
    "            if triplets != []:\n",
    "                file.write(\"text\\n\")\n",
    "                file.write(cleanToken(text) + \"\\n\")\n",
    "                file.write(\"translation\\n\")\n",
    "                file.write(cleanToken(translation)+ \"\\n\")\n",
    "                file.write(\"source\\n\")\n",
    "                file.write(cleanToken(source)+ \"\\n\")\n",
    "                file.write(\"category\\n\")\n",
    "                file.write(cleanToken(category)+ \"\\n\")\n",
    "    print(\"couple.txt created successfully!\")\n",
    "\n",
    "\n",
    "whole_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef6393",
   "metadata": {},
   "source": [
    "# Create triple.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a51967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def couple2tokobjects(triple):\n",
    "    text, lem, phon = triple\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\].F<sup>\\[\\[1\\]\\]</sup>\", \"[[div|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|quatre\\]\\]<sup>\\[\\[2\\]\\]</sup>\", \"[[peder|quatre]]\", lem)\n",
    "    lem = re.sub(r\"pronom incorporé\",\"pronom_incorporé\",lem)\n",
    "\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup> \\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]\", \"[[gant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[kant|cent]]\", lem)\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup>\\[\\[cardinal\\|cent\\]\\]\", \"[[c'hant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[50|50]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[zaou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[dek\\|dix\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|dix\\]\\]\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[DIM]]\",\"[[-ig|petit]]\",lem)\n",
    "    lem = re.sub(r'\\[\\[(.*?)\\]\\]', lambda m: '[[' + re.sub(r'\\s+', '', m.group(1)) + ']]', lem)\n",
    "    lem = re.sub(r\"\\[\\[tout]]\",\"[[tout|tout]]\",lem)\n",
    "    lem = re.sub(r\"\\[\\[R\\]\\]<sup>\\[\\[\\+C\\]\\],\\[\\[4\\]\\]</sup>\", \"[[R]]<sup>[[4]]</sup>\",lem)\n",
    "    lem = re.sub(r\"\\[\\[R\\]\\]<sup>\\[\\[4\\]\\] /(\\+C)?</sup>\",'[[R]]<sup>[[4]]</sup>',lem)\n",
    "    lem = re.sub(r\"\\[\\[R\\]\\]<sup>\\[\\[4\\]\\] / \\[\\[\\+C\\]\\]</sup>\",'[[R]]<sup>[[4]]</sup>',lem)\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[ne\\|ne\\]\\]<sup>\\[\\[1\\]\\]</sup>\\[R\\]\", \"[[ne|ne]][[a|_]]\",lem)\n",
    "    lem = re.sub(r\"\\[\\[ne\\|ne\\]\\]<sup>\\[\\[4\\]\\]</sup>\\[R\\]\", \"[[ne|ne]][[e|_]]\",lem)\n",
    "    lem = re.sub(r\"\\[\\[ne\\|ne\\]\\]<sup>\\[\\[R\\]\\]</sup>\", \"[[ne|ne]][[a|_]]\",lem)\n",
    "    lem = re.sub(r\"\\[\\[ne\\|ne\\]\\]<sup>\\[\\[1\\]\\]</sup>\",\"[[ne|ne]]\",lem)\n",
    "    lem = re.sub(r\"\\[\\[ne\\|ne\\]\\]<sup>\\[\\[4\\]\\]</sup>\",\"[[ne|ne]]\",lem)\n",
    "\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|moi]]\",\"[[me|moi]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|toi]]\",\"[[te|toi]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|lui]]\",\"[[eñ|lui]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|elle]]\",\"[[he|elle]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|nous]]\",\"[[ni|nous]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|vous]]\",\"[[c’hwi|vous]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|eux]]\",\"[[int|eux]]\",lem)\n",
    "    lem=re.sub(r\"\\[\\[pronom_incorporé\\|elles]]\",\"[[int|elles]]\",lem)\n",
    "\n",
    "    lem=re.sub(r\"1SG\",'[[-m|moi]]',lem)\n",
    "    lem=re.sub(r\"2SG\",\"[[-c'h|toi]]\",lem)\n",
    "    lem=re.sub(r\"3SGM\",\"[[en|lui]]\",lem)\n",
    "    lem=re.sub(r\"3SGF\",\"[[he|elle]]\",lem)\n",
    "    lem=re.sub(r\"1PL\",\"[[hon|nous]]\",lem)\n",
    "    lem=re.sub(r\"2PL\",\"[[hoc'h|vous]]\",lem)\n",
    "    lem=re.sub(r\"3PL\",\"[[o|eux,elles]]\",lem)\n",
    "\n",
    "    lem=re.sub(r\"2\\.\\[\\[kaout\", \"[[kaout2\", lem)\n",
    "    lem=re.sub(r\"1\\.\\[\\[kaout\", \"[[kaout1\", lem)\n",
    "    lem = re.sub(r\"3\\.\\[\\[kaout\", \"[[kaout3\", lem)\n",
    "\n",
    "    text=re.sub(r\"am-eus\",'am eus',text)\n",
    "    text=re.sub(r\"ac'h-eus\",\"ac'h eus\",text)\n",
    "    text=re.sub(r\"en-deus\",'en deus',text)\n",
    "    text=re.sub(r'he-deus','he deus',text)\n",
    "    text=re.sub(r'hon-deus','hon deus',text)\n",
    "    text=re.sub(r'ho-peus','ho peus',text)\n",
    "    text=re.sub(r'o-deus','o deus',text)\n",
    "    text = re.sub(r\"\\('n\\)\", \"'n\", text)\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]\", \"[[50|50]]\", lem)\n",
    "\n",
    "    if 'diou' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\.F\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[diou|deux.F]]\", lem)\n",
    "    if 'vil' in text:\n",
    "        lem = re.sub(r\"<sup>\\[\\[1\\]\\]</sup>\\[\\[cardinal\\|mille\\]\\]\", \"[[vil|mille]]\", lem)\n",
    "    if 'diouig' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]\", \"[[diou|2]]\", lem)\n",
    "    if \"daou-ha-daou\" in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "\n",
    "    lem = re.sub(r'<sup>(\\[\\[[^\\[\\]]+\\]\\])(.*?)</sup>',\n",
    "                 lambda m: ''.join(f'<sup>{x}</sup>' for x in re.findall(r'\\[\\[[^\\[\\]]+\\]\\]', m.group(0))), lem)\n",
    "    lem = re.sub('particule o', 'o', lem)\n",
    "\n",
    "    lem = re.sub(r'<font color=green>.*?</font color=green>', '', lem)\n",
    "\n",
    "    phon = re.sub('\\\\[', '', phon)\n",
    "    phon = re.sub(']', '', phon)\n",
    "\n",
    "    if '[[R]]' not in lem:\n",
    "        lem = re.sub(r'<sup>.*?</sup>', '', lem)\n",
    "\n",
    "        if \"particule o\" not in lem:\n",
    "            lem = re.sub(r'<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' in lem:\n",
    "        match1 = re.search(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match1:\n",
    "            k = match1.group(2).strip(']')\n",
    "            lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' not in lem:\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]', '[[R|_]]', lem)\n",
    "        k = 'R'\n",
    "\n",
    "    text_list = text.split('||')\n",
    "    lem_list = lem.split('||')\n",
    "    phone_list = phon.split('||')\n",
    "\n",
    "    tok_objects = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "\n",
    "        tok = text_list[i].strip()\n",
    "        lem_parts = lem_list[i].strip().split(']]')\n",
    "        lem_sp = lem_list[i].strip().split('|')\n",
    "        phonetic = phone_list[i].strip()\n",
    "\n",
    "        new_lem=lem_list[i].strip()\n",
    "\n",
    "        if not (text_list[i].startswith(\"'''\") and text_list[i].endswith(\"'''\")):\n",
    "\n",
    "            ex_lem= re.findall(r\"'''(.*?)'''\", text_list[i]) if re.findall(r\"'''(.*?)'''\", text_list[i]) else \"\"\n",
    "            ex_gloss_r = re.findall(r'(.*?)\\.\\]\\]', lem_list[i][::-1]) if re.findall(r'(.*?)\\.\\]\\]',\n",
    "                                                                                     lem_list[i][::-1]) else ''\n",
    "            ex_gloss = ex_gloss_r[0][::-1] if ex_gloss_r else ''\n",
    "\n",
    "            if ex_lem and ex_gloss and '|' not in ex_gloss:\n",
    "\n",
    "                text_list[i] = re.sub(\"'''\", ' ', text_list[i])\n",
    "                text_list[i] = re.sub(\"'''\", ' ', text_list[i])\n",
    "                escaped_ex_gloss = re.escape(ex_gloss)\n",
    "                lem_list[i] = re.sub(f'{escaped_ex_gloss}$', f\"[[{ex_lem[0]}|{escaped_ex_gloss}]]\", lem_list[i])\n",
    "                tok = text_list[i].strip()\n",
    "                lem_parts = lem_list[i].strip().split(']]')\n",
    "                lem_sp = lem_list[i].strip().split('|')\n",
    "                phonetic = phone_list[i].strip()\n",
    "\n",
    "                new_lem = lem_list[i].strip()\n",
    "\n",
    "\n",
    "\n",
    "        import string\n",
    "\n",
    "        tok_list = [part for part in tok.split(' ') ]\n",
    "        phon_list = [part2 for part2 in phon.split(' ') if not all(char2 in string.punctuation for char2 in part2)]\n",
    "        lemma_list= [part3 for part3 in new_lem.split(' ') ]\n",
    "\n",
    "\n",
    "        if len(lem_sp) == 1 and tok.startswith(\"'''\") :\n",
    "            tok_objects.append({\n",
    "                'tok': tok.replace(\"'''\", ''),\n",
    "                'lemma': tok.replace(\"'''\", ''),\n",
    "                'gloss': lem_parts[0],\n",
    "                'phonetic': phonetic.replace(\"'''\", '')\n",
    "            })\n",
    "\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and ('SC' or 'VP' or 'PredP' or 'CP' or 'DP') in lem_sp[0]:\n",
    "            match = re.search(r'<sub>\\[\\[(.*?)\\]\\]', lem_sp[0])\n",
    "            gloss_unique=lem_sp[0].split()[0]\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'tok':tok,\n",
    "                    'lemma':tok,\n",
    "                    'gloss':gloss_unique,\n",
    "                    'PhraseStructure': content\n",
    "                })\n",
    "\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and '[[' not in lem_sp[0]:\n",
    "            match = re.search(r'<sub>(.*?)</sub>', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'tok':tok,\n",
    "                    'lemma':tok,\n",
    "                    'gloss':lem_sp[0].split()[0],\n",
    "                    'index': content.strip(\"'\")\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 2 and '[[' in lem_sp[0]:\n",
    "\n",
    "            gloss = lem_sp[1]\n",
    "            if not re.match(r'\\b\\w+\\b', gloss):\n",
    "                gloss = gloss[:-1]\n",
    "\n",
    "            if 'cardinal' in lem_sp[0] and 'cardinal' not in tok:\n",
    "                lem_sp[0] = tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "            if \"cardinaux\" in lem_sp[0]:\n",
    "                lem_sp[0] = tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0],\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "            else:\n",
    "                gloss = lem_sp[1].rstrip('].')\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0].strip('[]'),\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic': phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "\n",
    "        elif len(tok_list) == len(lemma_list) and len(tok_list)>1:\n",
    "\n",
    "            for i in range(len(tok_list)):\n",
    "                if \"]]-[[\" not in lemma_list[i] and lemma_list[i].count(\"]]\")==1:\n",
    "\n",
    "                    lem_inside=lemma_list[i].split('|')[0]\n",
    "                    gloss_inside=lemma_list[i].split('|')[-1]\n",
    "                    if len(tok_list) == len(phon_list):\n",
    "                        tok_objects.append({'tok': tok_list[i],\n",
    "                                            'lemma': lem_inside[2:],\n",
    "                                            'gloss': gloss_inside.strip(']'),\n",
    "                                            'phonetic': phon_list[i] if phon_list[i] else 'None'\n",
    "\n",
    "                                            })\n",
    "                    else:\n",
    "                        tok_objects.append({'tok': tok_list[i],\n",
    "                                            'lemma': lem_inside[2:],\n",
    "                                            'gloss': gloss_inside.rstrip(']')\n",
    "\n",
    "                                            })\n",
    "                elif \"]]-[[\" in lemma_list[i] and '-' in tok_list[i]:\n",
    "                    comp_list2 = lemma_list[i].split(']]-[[')\n",
    "                    tok_objects.append({\n",
    "                        'tok': tok_list[i].rstrip(']'),\n",
    "                        'lemma': '_',\n",
    "                        'gloss': '_',\n",
    "                        'phonetic': phonetic if phonetic else '_',\n",
    "                        'multiple': len(lemma_list[i].split(']]-[['))})\n",
    "\n",
    "                    for i in range(len(comp_list2)):\n",
    "                        if '|' in comp_list2[i]:\n",
    "                            parts2 = comp_list2[i].split('|')\n",
    "                            tok_objects.append({\n",
    "                                'tok': parts2[0].lstrip('['),\n",
    "                                'lemma': parts2[0].lstrip('['),\n",
    "                                'gloss': parts2[1].rstrip(']'),\n",
    "                                'phonetic': phonetic if phonetic else '_'})\n",
    "\n",
    "                elif \"]].[[\" in lemma_list[i]:\n",
    "\n",
    "                    comp_list=lemma_list[i].split(']].[[')\n",
    "                    tok_objects.append({\n",
    "                        'tok': tok_list[i].rstrip(']'),\n",
    "                        'lemma': '_',\n",
    "                        'gloss': '_',\n",
    "                        'phonetic': phonetic if phonetic else '_',\n",
    "                        'multiple': len(lemma_list[i].split(']].[['))})\n",
    "\n",
    "                    for i in range(len(comp_list)):\n",
    "                        if '|' in comp_list[i]:\n",
    "                            parts = comp_list[i].split('|')\n",
    "                            tok_objects.append({\n",
    "                                'tok': parts[0].lstrip('['),\n",
    "                                'lemma': parts[0].lstrip('['),\n",
    "                                'gloss': parts[1].rstrip(']'),\n",
    "                                'phonetic': phonetic if phonetic else '_'})\n",
    "\n",
    "\n",
    "        else:\n",
    "            if len(lem_parts) > 1:\n",
    "\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': '_',\n",
    "                    'gloss': '_',\n",
    "                    'phonetic': phonetic if phonetic else '_',\n",
    "                    'multiple': len(lem_parts) - 1})\n",
    "                tok_obj = {'tok': tok, 'lemma': '_', 'gloss': '_', 'multiple': len(lem_parts) - 1}\n",
    "                tok_n = 1\n",
    "            else:\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': '_',\n",
    "                    'gloss': '_',\n",
    "                    'phonetic': phonetic if phonetic else '_'})\n",
    "                tok_obj = {'tok': tok, 'lemma': '_', 'gloss': '_'}\n",
    "                tok_n = 1\n",
    "\n",
    "            for j in range(len(lem_parts)):\n",
    "                if '[[' in lem_parts[j]:\n",
    "                    lemma_gloss = lem_parts[j].strip('[[').split('|')\n",
    "                    lemma = lemma_gloss[0].split('[[')[-1].strip()\n",
    "\n",
    "                    if re.search(r'^-\\w+', lemma):\n",
    "                        if lemma != '-ig':\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 and tok != '-ig' else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': 'petit'\n",
    "                            })\n",
    "\n",
    "                    elif lemma_gloss == ['R', '_']:\n",
    "                        if k != '+C':\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else '',\n",
    "                                'Epenthesis': 'Yes'\n",
    "                            })\n",
    "\n",
    "                    elif lemma_gloss == ['a', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'a',\n",
    "                            'lemma': 'R',\n",
    "                            'gloss': '_'\n",
    "                        })\n",
    "\n",
    "                    elif lemma_gloss == ['e', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'e',\n",
    "                            'lemma': 'R',\n",
    "                            'gloss': '_'\n",
    "                        })\n",
    "\n",
    "                    elif 'cardinal' in lemma_gloss[0]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma_gloss[1],\n",
    "                            'lemma': lemma_gloss[1],\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardinaux\" in lemma_gloss[0] and 'cent' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'kant',\n",
    "                            'lemma': 'kant',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif 'cardi' in lemma_gloss[0] and '9' in lemma_gloss[1]:\n",
    "\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'nav',\n",
    "                            'lemma': 'nav',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'trois' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'vingt' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'ugent',\n",
    "                            'lemma': 'ugent',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '2' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'douze' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daouzek',\n",
    "                            'lemma': 'daouzek',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'deux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '3' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'un' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'unan',\n",
    "                            'lemma': 'unan',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'lui' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'eñ',\n",
    "                            'lemma': 'eñ',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'moi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'me',\n",
    "                            'lemma': 'me',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'toi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'te',\n",
    "                            'lemma': 'te',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'nous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hon',\n",
    "                            'lemma': 'hon',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'elle' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hi',\n",
    "                            'lemma': 'hi',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'vous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"hoc'h-unan\",\n",
    "                            'lemma': \"hoc'h-unan\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'eux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"int\",\n",
    "                            'lemma': \"int\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'il' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"enni\",\n",
    "                            'lemma': \"enni\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'ça' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"se\",\n",
    "                            'lemma': \"se\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma.strip(),\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                        if '-ig' in tok_obj['tok'] and '-ig' in tok_obj['lemma']:\n",
    "                            tok_obj['gloss'] = 'petit'\n",
    "\n",
    "                        if 'numéraux' in tok_obj['lemma']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "\n",
    "                        if 'cardinal' in tok_obj['lemma'] and 'cardinal' not in tok_obj['tok']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "                            pattern = r\"[^\\w\\s]+\"\n",
    "                            tok_obj['lemma'] = re.sub(pattern, \"\", tok_obj['lemma'])\n",
    "\n",
    "                    tok_n += 1\n",
    "\n",
    "    return tok_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05f162",
   "metadata": {},
   "source": [
    "# Store the output in triple.txt (attention : clean the existed triple.txt file before running the following code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca8c8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def whole_lines():\n",
    "    filename = \"fichiers_extraction/all_line_objects.txt\"\n",
    "    all_examples = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    example = {}\n",
    "    store_text = False\n",
    "    store_lemgloss = False\n",
    "    store_phonetic = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if store_text:\n",
    "            if line.startswith(\"||\"):\n",
    "                example[\"text\"] = line[0:]\n",
    "            else:\n",
    "                example[\"text\"] = line[3:]\n",
    "            store_text = False\n",
    "        elif store_lemgloss:\n",
    "            example[\"lemgloss\"] = line\n",
    "            store_lemgloss = False\n",
    "\n",
    "            if example[\"text\"] != \"\" and example[\"lemgloss\"] != \"None\":\n",
    "                all_examples.append(example)\n",
    "\n",
    "            example = {}\n",
    "        elif store_phonetic:\n",
    "            example[\"phonetic\"] = line\n",
    "            store_phonetic = False\n",
    "\n",
    "        if line == \"text\":\n",
    "            store_text = True\n",
    "        elif line == \"lemgloss\":\n",
    "            store_lemgloss = True\n",
    "        elif line == \"phonetic\":\n",
    "            store_phonetic = True\n",
    "\n",
    "    # Process each example\n",
    "    for example in all_examples:\n",
    "        text = example[\"text\"]\n",
    "\n",
    "        lemgloss = example[\"lemgloss\"]\n",
    "\n",
    "\n",
    "        phonetic = example.get(\"phonetic\", None)\n",
    "\n",
    "        triplets = alignTokLemmgloss(text, lemgloss,phonetic)\n",
    "        tok_objects_list = []\n",
    "\n",
    "\n",
    "        for triple in triplets:\n",
    "            tok_objects = couple2tokobjects(triple)\n",
    "            tok_objects_list.extend(tok_objects)\n",
    "\n",
    "        if tok_objects_list:\n",
    "            with open('fichiers_traitement/triple.txt', 'a') as file:\n",
    "                print(tok_objects_list,file=file)\n",
    "\n",
    "\n",
    "whole_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06119de",
   "metadata": {},
   "source": [
    "# Create n_triple.txt for punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "046a8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_triple.txt created successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def process_triple_file():\n",
    "    with open(\"fichiers_traitement/triple.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        dicos = eval(line.strip())\n",
    "\n",
    "        new_dicos = []\n",
    "        pending_punctuation = None\n",
    "        pending_punctuation_count = 0\n",
    "        for dico in dicos:\n",
    "            tok = dico.get(\"tok\", \"\")\n",
    "            gloss=dico.get('gloss','')\n",
    "            lemma=dico.get('lemma','')\n",
    "            lemma=re.sub('^<','',lemma)\n",
    "\n",
    "            # Check if there is a pending punctuation to be added\n",
    "            if pending_punctuation is not None:\n",
    "                if pending_punctuation_count > 0:\n",
    "                    pending_punctuation_count -= 1\n",
    "                if pending_punctuation_count == 0:\n",
    "                    new_dicos.append(pending_punctuation)\n",
    "                    pending_punctuation = None           \n",
    "            if tok.endswith((\".\", \",\", \"!\", \"?\", \";\", \":\", \"…\")):\n",
    "                if 'multiple' not in dico.keys():\n",
    "                # Create a new dico with tok value without punctuation\n",
    "                    new_dico = {\n",
    "                        \"tok\": re.sub(r'\\([^)]*\\)', '', tok).rstrip(\".!,?;:><…\"),\n",
    "                        \"lemma\": lemma.rstrip(\".!,?;:><…\"),\n",
    "                        \"gloss\": gloss.rstrip(\"<>…\"),\n",
    "                        \"phonetic\": \"None\"\n",
    "                    }\n",
    "                    if new_dico[\"tok\"]: \n",
    "                        new_dicos.append(new_dico)\n",
    "\n",
    "                    # Create a dico for the punctuation\n",
    "                    punctuation_dico = {\n",
    "                        \"tok\": tok[-1],\n",
    "                        \"lemma\": tok[-1],\n",
    "                        \"gloss\": tok[-1],\n",
    "                        \"phonetic\": \"None\"\n",
    "                    }\n",
    "                    new_dicos.append(punctuation_dico)\n",
    "                else:\n",
    "                    new_dico = {\n",
    "                        \"tok\": re.sub(r'\\([^)]*\\)', '', tok).rstrip(\".!,?;:><…\"),\n",
    "                        \"lemma\": lemma.rstrip(\".!,?;:><…\"),\n",
    "                        \"gloss\": gloss.rstrip(\"<>…\"),\n",
    "                        \"phonetic\": \"None\",\n",
    "                        \"multiple\": dico[\"multiple\"]\n",
    "                    }\n",
    "                    if new_dico[\"tok\"]:\n",
    "                        new_dicos.append(new_dico)\n",
    "\n",
    "                    # Create a dico for the punctuation\n",
    "                    punctuation_dico = {\n",
    "                        \"tok\": tok[-1],\n",
    "                        \"lemma\": tok[-1],\n",
    "                        \"gloss\": tok[-1],\n",
    "                        \"phonetic\": \"None\"\n",
    "                    }\n",
    "                    pending_punctuation = punctuation_dico\n",
    "                    pending_punctuation_count = dico[\"multiple\"]+ 1\n",
    "            else:\n",
    "                new_dicos.append(dico)\n",
    "\n",
    "\n",
    "\n",
    "        if pending_punctuation is not None:\n",
    "            new_dicos.append(pending_punctuation)\n",
    "        new_line = str(new_dicos) + \"\\n\"\n",
    "        new_lines.append(new_line)\n",
    "\n",
    "    with open(\"fichiers_traitement/n_triple.txt\", \"w\") as file:\n",
    "        file.writelines(new_lines)\n",
    "\n",
    "    print(\"n_triple.txt created successfully!\")\n",
    "\n",
    "\n",
    "process_triple_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcfce0",
   "metadata": {},
   "source": [
    "# Get the pos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "640fb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "\n",
    "def get_wikicode(title):\n",
    "    wikicode=''\n",
    "    # on traite les redirections vers d'autres pages\n",
    "    if '#REDIRECTION' in pages.get(title, ''):             \n",
    "        #on suit la redirection\n",
    "        newtitle = relinks.search(pages[title]).group(1)                \n",
    "        newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "        if '_' in newtitle: \n",
    "            newtitle = newtitle.replace(\"_\", \" \")            \n",
    "        #et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "        if newtitle in pages:\n",
    "            wikicode = pages[newtitle]              \n",
    "        else: \n",
    "            newtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "            if newtitle in pages:\n",
    "                wikicode = pages[newtitle]\n",
    "            else:\n",
    "                newtitle = newtitle.split(',')[0]\n",
    "                newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                if newtitle in pages:\n",
    "                    wikicode = pages[newtitle]\n",
    "                else:\n",
    "                    wikicode='__currentPage:\\nstrange redirect to page that does not exist: '+newtitle\n",
    "                    print(wikicode+'\\n')\n",
    "\n",
    "    else:\n",
    "        wikicode = pages.get(title, '')  \n",
    "    return wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b6e30d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "rePOS = re.compile(\n",
    "    r'verbes?|auxiliaires?|copules?|adverbes?|complémenteurs?|conjonctions?|prépositions?|adjectifs?|noms?|particules verbales?|interjections?|postpositions?|déterminants?|quantifieurs?|pronoms?|noms? propres?|suffixe|interrogatifs?|préfixes?|modaux|pluriels?|indéfinis?|particules? de discours|finales?|exclamatifs?',\n",
    "    re.IGNORECASE)\n",
    "\n",
    "lefffl = [li.strip() for li in open('fichiers_lefff/lefff-2.1.txt').read().split('\\n') if li.strip() and li[0] != '#']\n",
    "lefff = {li.split('\\t')[0]: li.split('\\t')[2] for li in lefffl}\n",
    "\n",
    "\n",
    "# print(lefff)\n",
    "\n",
    "import re\n",
    "\n",
    "def add_pos(tok_obj):\n",
    "    if 'pos' in tok_obj and tok_obj['pos']:\n",
    "        return tok_obj  # pos already there\n",
    "\n",
    "    # Check if the tok is a punctuation mark\n",
    "    if 'tok' in tok_obj and re.match(r'^[\\.,!?:;…]$', tok_obj['tok']):\n",
    "        tok_obj['pos'] = 'PUNCT'\n",
    "        return tok_obj\n",
    "\n",
    "    if 'tok' in tok_obj and tok_obj['tok']=='R':\n",
    "        tok_obj['pos']='SCONJ'\n",
    "        return tok_obj\n",
    "    tok=tok_obj.get('tok','')\n",
    "\n",
    "    if tok == '-m':\n",
    "        tok_obj['lemma'] = 'me'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == \"-c'h\":\n",
    "        tok_obj['lemma'] = 'te'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == 'en':\n",
    "        tok_obj['lemma']= 'eñ'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == 'he':\n",
    "        tok_obj['lemma'] = 'he'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == 'hon':\n",
    "        tok_obj['lemma']= 'ni'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == \"hoc'h\":\n",
    "        tok_obj['lemma']= \"c'hwi\"\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "    if tok == 'o':\n",
    "        tok_obj['lemma']= 'int'\n",
    "        tok_obj['pos'] = 'PRON'\n",
    "        return tok_obj\n",
    "\n",
    "    lemma=tok_obj.get('lemma','')\n",
    "    if lemma.startswith('-'):\n",
    "        tok_obj['pos']='suffixe'\n",
    "        return tok_obj\n",
    "\n",
    "    if lemma.endswith('-'):\n",
    "        tok_obj['pos']='prefixe'\n",
    "        return tok_obj\n",
    "\n",
    "    if 'lemma' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['lemma'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    # is the pos in the gloss?\n",
    "    if 'gloss' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['gloss'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    # let's check the page for the lemma\n",
    "    wikicode = get_wikicode(str.capitalize(tok_obj.get('lemma', '')))  # TODO: improve as this might fail sometimes\n",
    "    # print(wikicode[:222])\n",
    "    m = rePOS.search(wikicode)\n",
    "    if m:\n",
    "        tok_obj['pos'] = m.group(0)\n",
    "        return tok_obj\n",
    "\n",
    "\n",
    "    # let's use the gloss to make an educated guess about the pos using lefff\n",
    "    if tok_obj.get('gloss', '') in lefff:\n",
    "        tok_obj['pos'] = lefff[tok_obj.get('gloss', '')]\n",
    "\n",
    "    return tok_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21424cb6",
   "metadata": {},
   "source": [
    "# Store the output in nt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1ff18afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_processed_output(file_path, output_file_path):\n",
    "    import ast\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for line in lines:\n",
    "            tok_objects = ast.literal_eval(line)\n",
    "            modified_tok_objects = [add_pos(tok_obj) for tok_obj in tok_objects]\n",
    "\n",
    "            # Write the processed tok_obj objects to the output file line by line\n",
    "            output_file.write(str(modified_tok_objects) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "write_processed_output('fichiers_traitement/n_triple.txt','fichiers_traitement/nt.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664baae",
   "metadata": {},
   "source": [
    "# Preparation for the conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "129db527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_conll_output(tok_objects, text, text_fr, source, sent_id, category):\n",
    "    conll_output = \"\"\n",
    "    conll_id = 1\n",
    "    inside_id = 1\n",
    "\n",
    "    for tok_obj in tok_objects:\n",
    "        if isinstance(tok_obj, dict) and len(tok_obj) == 1:\n",
    "            field_key, field_value = next(iter(tok_obj.items()))\n",
    "            conll_line = f\"{conll_id}\\t{field_key}\\t{field_value}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "        else:\n",
    "\n",
    "\n",
    "            tok_obj['tok'] = re.sub(r'\\([^()]*\\)', '', tok_obj['tok']).rstrip(\".!,?;:\") if re.search(r'\\([^()]*\\)',tok_obj['tok']) else tok_obj['tok']\n",
    "\n",
    "            if 'multiple' not in tok_obj:\n",
    "                if 'pos' in tok_obj and 'phonetic' in tok_obj:\n",
    "                    if tok_obj['phonetic'] != 'None':\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t{tok_obj['phonetic']}\\t_\\t_\\t_\\tGloss={tok_obj['gloss']}\\n\"\n",
    "                    else:\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tGloss={tok_obj['gloss']}\\n\"\n",
    "\n",
    "                    conll_id += 1\n",
    "                    inside_id = conll_id\n",
    "                else:\n",
    "                    if 'pos' in tok_obj:\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tGloss={tok_obj['gloss']}\\n\"\n",
    "                        conll_id += 1\n",
    "                        inside_id = conll_id\n",
    "                    else:\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t_\\t_\\t_\\t_\\t_\\tGloss={tok_obj['gloss']}\\n\"\n",
    "                        conll_id += 1\n",
    "                        inside_id = conll_id\n",
    "            elif 'multiple' in tok_obj:\n",
    "                if tok_obj['phonetic'] != 'None':\n",
    "                    conll_line = f\"{inside_id}-{inside_id + tok_obj['multiple'] - 1}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t{tok_obj['phonetic']}\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "                else:\n",
    "                    conll_line = f\"{inside_id}-{inside_id + tok_obj['multiple'] - 1}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "                # # print all the mots composés\n",
    "                # print(\"sent_id:\", sent_id)\n",
    "                # print(\"text:\", text)\n",
    "                # print(\"text_fr:\", text_fr)\n",
    "                # print(\"conll_line:\", conll_line)\n",
    "\n",
    "                \n",
    "            conll_line = re.sub(']]', ' ', conll_line)\n",
    "            conll_line = re.sub('\\\\[\\\\[kaout', 'kaout', conll_line)\n",
    "            conll_line = re.sub(\"'''\", '', conll_line)\n",
    "            conll_line = re.sub('\\\\[', '', conll_line)\n",
    "\n",
    "            lemma=tok_obj.get('lemma','')\n",
    "            gloss=tok_obj.get('gloss','')\n",
    "            tok=tok_obj.get('tok','')\n",
    "            pos=tok_obj.get('pos','_')\n",
    "\n",
    "            if tok == '-m' and lemma =='me':\n",
    "                conll_line = re.sub(f\"-m\\tme\\tPRON\\t_\",f\"-m\\tme\\tPRON\\tNumber=Sing|Person=1|PronType=Prs\",conll_line)\n",
    "            if tok == \"-c'h\" and lemma =='te':\n",
    "                conll_line = re.sub(f\"-c'h\\tte\\tPRON\\t_\",f\"-c'h\\tte\\tPRON\\tNumber=Sing|Person=2|PronType=Prs\",conll_line)\n",
    "            if tok == 'en' and lemma =='eñ':\n",
    "                conll_line = re.sub(f\"en\\teñ\\tPRON\\t_\",f\"en\\teñ\\tPRON\\tGender=Masc|Number=Sing|Person=3|PronType=Prs\",conll_line)\n",
    "            if tok == 'he' and lemma =='he':\n",
    "                conll_line = re.sub(f\"he\\the\\tPRON\\t_\",f\"he\\the\\tPRON\\tGender=Fem|Number=Sing|Person=3|PronType=Prs\",conll_line)\n",
    "            if tok == 'hon' and lemma =='ni':\n",
    "                conll_line = re.sub(f\"hon\\tni\\tPRON\\t_\",f\"hon\\tni\\tPRON\\tNumber=Plur|Person=1|PronType=Prs\",conll_line)\n",
    "            if tok == \"hoc'h\" and lemma ==\"c'hwi\":\n",
    "                conll_line = re.sub(f\"hoc'h\\tc'hwi\\tPRON\\t_\",f\"hoc'h\\tc'hwi\\tPRON\\tNumber=Plur|Person=2|PronType=Prs\",conll_line)\n",
    "            if tok == \"o\" and lemma ==\"int\":\n",
    "                conll_line = re.sub(f\"o\\tint\\tPRON\\t_\",f\"o\\tint\\tPRON\\tNumber=Plur|Person=3|PronType=Prs\",conll_line)\n",
    "\n",
    "            if tok_obj.get('PhraseStructure','')!='':\n",
    "                conll_line=re.sub(f\"{lemma}\\t{pos}\\t_\",f\"{lemma}\\t{pos}\\tPhraseStructure={tok_obj['PhraseStructure']}\",conll_line)\n",
    "            if tok_obj.get('index','')!='':\n",
    "                conll_line=re.sub(f\"{lemma}\\t{pos}\\t_\",f\"{lemma}\\t{pos}\\tindex={tok_obj['index']}\",conll_line)\n",
    "            if (lemma=='a' and gloss=='_') or (tok=='a' and lemma=='R'):\n",
    "                conll_line=re.sub(f\"{lemma}\\t{pos}\\t_\",f\"{lemma}\\t{pos}\\tMut=1\",conll_line)\n",
    "            if (lemma=='e' and gloss=='_') or (tok=='e' and lemma=='R'):\n",
    "                conll_line=re.sub(f\"{lemma}\\t{pos}\\t_\",f\"{lemma}\\t{pos}\\tMut=4\",conll_line)\n",
    "            \n",
    "            if lemma=='kaout1':\n",
    "                conll_line=re.sub(f\"{tok}\\t{lemma}\\t{pos}\\t_\",f'kaout\\tkaout\\t{pos}\\tPerson=1',conll_line)\n",
    "            if lemma=='kaout2':\n",
    "                conll_line=re.sub(f\"{tok}\\t{lemma}\\t{pos}\\t_\",f'kaout\\tkaout\\t{pos}\\tPerson=2',conll_line)\n",
    "            if lemma=='kaout3':\n",
    "                conll_line=re.sub(f\"{tok}\\t{lemma}\\t{pos}\\t_\",f'kaout\\tkaout\\t{pos}\\tPerson=3',conll_line)\n",
    "\n",
    "\n",
    "        conll_output += conll_line.strip() + \"\\n\"\n",
    "\n",
    "    # Check if the last element of the text line is a punctuation mark\n",
    "    text_elements = text.strip().split()\n",
    "    if text_elements:  # Check if the list is not empty\n",
    "        last_text_element = text_elements[-1][-1]\n",
    "    # last_text_element = text.strip().split()[-1][-1]\n",
    "        if last_text_element in (\".\", \",\", \"!\", \"?\", \";\", \":\",\"…\"):\n",
    "            # Check if the last line of the conll output has this punctuation\n",
    "            last_line_tokens = conll_output.strip().split('\\n')[-1]\n",
    "\n",
    "            if 'PUNCT' not in last_line_tokens:\n",
    "                # Add the punctuation token to the conll output\n",
    "                conll_id += 1\n",
    "                conll_line = f\"{conll_id}\\t{last_text_element}\\t{last_text_element}\\tPUNCT\\t_\\t_\\t_\\t_\\t_\\tgloss={last_text_element}\\n\"\n",
    "                conll_output += conll_line.strip() + \"\\n\"\n",
    "    if \"(\"in category:\n",
    "        parts = category.split(' (')\n",
    "        if len(parts) > 1:\n",
    "            dialect = parts[0]\n",
    "            location_parts = parts[1].split(')')\n",
    "            if location_parts:\n",
    "                location = location_parts[0]\n",
    "            else:\n",
    "                location = 'None'\n",
    "                pass\n",
    "        else:\n",
    "            dialect = category\n",
    "            location = 'None'\n",
    "            pass\n",
    "    else:\n",
    "        dialect = category\n",
    "        location = 'None'\n",
    "    conll_output = f\"# sent_id = {source.replace(' ', '') +'__'+str(sent_id)}\\n\" + \\\n",
    "                f\"# text = {text.replace('||', '')}\\n\" + \\\n",
    "                f\"# text_fr = {text_fr}\\n\" + \\\n",
    "                f\"# dialect = {dialect}\\n\" + \\\n",
    "                f\"# location = {location}\\n\" + \\\n",
    "                f\"# source = {source}\\n\" + \\\n",
    "                f\"# status = WIP\\n\" + \\\n",
    "                conll_output + \"\\n\"\n",
    "    \n",
    "\n",
    "    return conll_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47088df5",
   "metadata": {},
   "source": [
    "# Store the output in the conllu file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "10818fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def process_tokens_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    conll_output = \"\"\n",
    "    sent_id = 1  # Initialize sent_id to 1\n",
    "\n",
    "    with open(\"fichiers_traitement/couple.txt\", \"r\") as couple_file:\n",
    "        couple_lines = couple_file.readlines()\n",
    "        couple_lines = [line.strip() for line in couple_lines]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if i * 8 + 1 >= len(couple_lines) or i * 8 + 3 >= len(couple_lines) or i * 8 + 5 >= len(couple_lines) or i * 8 + 7 >= len(couple_lines):\n",
    "            # Skip this iteration if indices are out of range\n",
    "            continue\n",
    "\n",
    "        tok_objects = ast.literal_eval(line)\n",
    "        text = couple_lines[i * 8 + 1].replace(\"text\", \"\").strip()\n",
    "        text_fr = couple_lines[i * 8 + 3].replace(\"translation\", \"\").strip()\n",
    "        text = re.sub(\"'''\", '', text)\n",
    "        source = couple_lines[i * 8 + 5].replace(\"source\", \"\").strip()\n",
    "        category = couple_lines[i * 8 + 7].replace(\"category\", \"\").strip()\n",
    "        # Assuming generate_conll_output is a function defined elsewhere\n",
    "        conll_output += generate_conll_output(tok_objects, text, text_fr, source, sent_id,category)\n",
    "        sent_id += 1  # Increment sent_id for the next example\n",
    "\n",
    "    return conll_output\n",
    "\n",
    "# Assuming 'nt.txt' and 'generate_conll_output' are correctly set up\n",
    "output = process_tokens_file('fichiers_traitement/nt.txt')\n",
    "with open('fichiers_traitement/none2_original.conllu', 'w') as file:\n",
    "    print(output, file=file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869093e",
   "metadata": {},
   "source": [
    "# Ajouter des modifications sur le fichier none2_original.txt généré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5d6c5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_trailing_punctuation(word):\n",
    "    # Expression régulière pour supprimer la ponctuation en fin de mot\n",
    "    return re.sub(r'[^\\w\\s-]+$', '', word)\n",
    "\n",
    "def apply_replacement_rules(parts):  \n",
    "    # Appliquer différentes règles de remplacement en fonction de la position de la colonne et de la valeur cible\n",
    "    if len(parts) > 3:\n",
    "        # Règle 1 : Si la troisième colonne est \"an,al,ar\", alors remplacez la quatrième colonne par \"défini\"\n",
    "        if parts[5] != \"_\":\n",
    "            parts[5]= \"_\"\n",
    "        elif parts[9] == \"Gloss=leur\":\n",
    "            parts[4] = \"Number=Plur|Person=3|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=son\" and parts[4]==\"_\":\n",
    "            parts[4] = \"Number=Sing|Person=3|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=sa\" and parts[4]== \"_\":\n",
    "            parts[4] = \"Number=Sing|Person=3|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=ton\" and parts[4]==\"_\":\n",
    "            parts[4] = \"Number=Sing|Person=2|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=elle\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Gender=Fem|Number=Sing|Person=3|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=lui\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Gender=Masc|Number=Sing|Person=3|PronType=Prs\"\n",
    "            if parts[1] == \"eñ\" and parts[2] == \"eñ\":\n",
    "                parts[1] = \"añ\"\n",
    "                parts[3] = \"pronom\"\n",
    "        elif parts[9] == \"Gloss=il\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Gender=Masc|Number=Sing|Person=3|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=nous\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Number=Plur|Person=1|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=je\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Number=Sing|Person=1|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=vous\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Number=Plur|Person=2|PronType=Prs\"\n",
    "        elif parts[9] == \"Gloss=tu\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Number=Sing|Person=2|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=me\" and parts[4]==\"_\":\n",
    "            parts[4]=\"Number=Sing|Person=1|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=te\" and parts[4]==\"_\":\n",
    "            parts[4]=\"Number=Sing|Person=2|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=toi\" and parts[4]==\"_\":\n",
    "            parts[4]=\"Number=Sing|Person=2|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=moi\" and parts[4]==\"_\":\n",
    "            parts[4]=\"Number=Sing|Person=1|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=elles\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Gender=Fem|Number=Plur|Person=3|PronType=Prs\"\n",
    "        elif parts[9]==\"Gloss=ils\" and parts[4]==\"_\":\n",
    "            parts[4]= \"Gender=Masc|Number=Plur|Person=3|PronType=Prs\"\n",
    "        elif parts[2] == \"an,al,ar\":\n",
    "            parts[3] = \"défini\"\n",
    "        elif parts[2] == \"un,ul,ur\":\n",
    "            parts[3] = \"indéfini\"\n",
    "        # Règles 4-7 : Remplacer la quatrième colonne en fonction de certaines valeurs de la troisième colonne\n",
    "        elif parts[2] in [\"POP\", \"COP\", \"zo\", \"eo\"]:\n",
    "            replacement = \"pronom\" if parts[2] == \"POP\" else \"AUX\"\n",
    "            parts[3] = replacement\n",
    "\n",
    "        # Règle 8 : Remplacement basé sur des conditions impliquant plusieurs colonnes spécifiques\n",
    "        elif len(parts) > 4 and parts[2] == \"e\" and parts[3] == \"copule\":\n",
    "            parts[3] = \"SCONJ\"\n",
    "\n",
    "        # Règle 9 : Si la deuxième colonne est \"a\" et la troisième colonne est \"R\", alors remplacez la quatrième colonne par \"SCONJ\"\n",
    "        elif parts[1] == \"a\" and parts[2] == \"R\":\n",
    "            parts[3] = \"SCONJ\"\n",
    "        elif parts[9] == \"Gloss=er\" and parts[1] == \"a\" and parts[2] == \"a\":\n",
    "            parts[1] = \"-a\"\n",
    "            parts[2] = \"-a\"\n",
    "            parts[3] = \"verbe\"\n",
    "        elif parts[9] == \"Gloss=ramasser\" and parts[1] == \"a\" and parts[2] == \"a\":\n",
    "            parts[1] = \"-a\"\n",
    "            parts[2] = \"-a\"\n",
    "            parts[3] = \"verbe\"\n",
    "        elif parts[1]== \"a\" and parts[2]== \"a\":\n",
    "            parts[3] = \"préposition\"\n",
    "        elif parts[1]== \"P.e\" and parts[2]== \"P.e\":\n",
    "            parts[1] = \"e\"\n",
    "        elif parts[1]== \"e\" and parts[2]== \"POSS\":\n",
    "            parts[3] = \"POSS\"\n",
    "        elif parts[1]== \"aat\" and parts[2]== \"aat\":\n",
    "            parts[1]= \"-aat\"\n",
    "            parts[2]= \"-aat\" \n",
    "            parts[3] = \"suffixe\"\n",
    "        elif parts[1]== \"ât\" and parts[2]== \"ât\":\n",
    "            parts[1]= \"-ât\" \n",
    "            parts[2]= \"-ât\" \n",
    "            parts[3] = \"suffixe\"\n",
    "        elif parts[1]== \"at\" and parts[2]== \"at\":\n",
    "            parts[1]= \"-at\" \n",
    "            parts[2]= \"-at\" \n",
    "            parts[3] = \"suffixe\"\n",
    "        elif parts[1]== \"aad\":\n",
    "            parts[2]= \"-aat\" \n",
    "            parts[3] = \"suffixe\"\n",
    "        elif parts[1]== \"ad\" and parts[2]== \"ad\":\n",
    "            parts[1]= \"-ad\" \n",
    "            parts[2]= \"-ad\" \n",
    "            parts[3] = \"suffixe\"\n",
    "\n",
    "        elif parts[2] == \"kaout\" and parts[3] == \"verbe\":\n",
    "            parts[1] = \"eus\"\n",
    "        elif parts[1] == \".\" and parts[2] == \".\":\n",
    "            parts[9] = \"Gloss=.\"\n",
    "        elif (\"…\" in parts[1] and len(parts[1]) > 3) or (\"…\" in parts[2] and len(parts[2]) > 3):\n",
    "            parts[1]=parts[1].replace(\"…\", \"\")\n",
    "            parts[2]=parts[2].replace(\"…\", \"\")\n",
    "    # Traiter toujours la quatrième colonne, quelle que soit la longueur des parties\n",
    "        # Règles 2-3 : Si la quatrième colonne est \"adverbes\" ou \"pronoms\", effectuer le remplacement correspondant\n",
    "        elif parts[3] == \"adverbes\":\n",
    "            parts[3] = \"adverbe\"\n",
    "        elif parts[3] == \"pronoms\":\n",
    "            parts[3] = \"pronom\"\n",
    "        elif parts[3] == \"complémenteurs\":\n",
    "            parts[3] = \"complémenteur\"\n",
    "        elif parts[9]== \"Gloss=N\":\n",
    "            parts[9] = \"Gloss=Nom\"\n",
    "        elif \"Gloss=\" in parts[9]:\n",
    "            gloss_value = parts[9].split('=',1)[1]\n",
    "            new_gloss_value = gloss_value.replace('.', ' ').replace(\"Mut=\", \"|Mut=\").replace(\"Mutchoice=\", \"|Mutchoice=\")\n",
    "            parts[9] = \"Gloss=\" + new_gloss_value\n",
    "    return parts\n",
    "\n",
    "def modify_column_with_wordform(parts):\n",
    "    # Si la deuxième colonne et la troisième colonne sont toutes deux \"P.e\", effectuer un traitement spécial\n",
    "    if parts[1] == \"P.e\" and parts[2] == \"P.e\":\n",
    "        parts[1] = \"e\"  # Remplacez la deuxième colonne par \"e\"\n",
    "    return parts\n",
    "\n",
    "def remove_middle_spaces(word):\n",
    "    \"\"\"Supprimer les espaces au milieu de la chaîne, sans affecter les espaces en début et en fin de chaîne\"\"\"\n",
    "    # Supprimer les espaces en début et en fin de chaîne, puis reconstituer la chaîne sans espaces au milieu\n",
    "    stripped_word = word.strip()\n",
    "    return word.replace(stripped_word, ''.join(stripped_word.split()))\n",
    "\n",
    "# Début du programme principal\n",
    "updated_lines = []\n",
    "\n",
    "# Ouvrir et lire le fichier\n",
    "with open('fichiers_traitement/none2_original.conllu', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parcourir chaque ligne du fichier\n",
    "for line in lines:\n",
    "    stripped_line = line.strip()\n",
    "    parts = stripped_line.split(\"\\t\")# Séparer le contenu de la ligne\n",
    "        \n",
    "    # Vérifier si la ligne commence par \"# text_fr\", et si c'est le cas, remplacer \"'''\" par un espace\n",
    "    if \"'''\" in stripped_line:\n",
    "        stripped_line = stripped_line.replace(\"'''\", \" \")\n",
    "    if \"# text = )\" in stripped_line:\n",
    "        stripped_line = stripped_line.replace(\"# text = )\", \"# text = \")\n",
    "    \n",
    "    parts = stripped_line.split(\"\\t\")  # Séparer le contenu de la ligne\n",
    "    if stripped_line.startswith(\"# text = )\"):\n",
    "        stripped_line = stripped_line.replace(\"# text = )\", \"# text = \")  \n",
    "    # Si la ligne n'est pas vide et n'est pas un commentaire, et qu'il y a suffisamment de colonnes pour effectuer les modifications nécessaires\n",
    "    if parts and not stripped_line.startswith(\"#\"):\n",
    "        \n",
    "        # Traitement des caractères spéciaux \"-\" dans la première colonne, nous avons besoin d'au moins trois colonnes pour effectuer ces opérations\n",
    "        if \"-\" in parts[0] and len(parts) > 2:  \n",
    "            parts[1] = remove_trailing_punctuation(parts[1])\n",
    "            parts[1] = remove_middle_spaces(parts[1])\n",
    "            parts[2] = '_'  # Supprimer le contenu de la troisième colonne\n",
    "            \n",
    "        # Appliquer toutes les règles de remplacement prédéfinies\n",
    "        parts = apply_replacement_rules(parts)\n",
    "        \n",
    "    # Reconstituer la ligne et l'ajouter à la liste des résultats\n",
    "    updated_line = \"\\t\".join(parts)\n",
    "    updated_lines.append(updated_line)\n",
    "\n",
    "# Écrire les lignes mises à jour dans un nouveau fichier\n",
    "with open('none2.conllu', 'w', encoding='utf-8') as output_file:\n",
    "    for updated_line in updated_lines:\n",
    "        output_file.write(updated_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310efe3",
   "metadata": {},
   "source": [
    "# Ajouter des traits pour mots composés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267a23d",
   "metadata": {},
   "source": [
    "### Chercher les éléments dans page pour identifier les lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bdca9a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noms: [\"-ac'h\", '-ach,-aj', '-ach', '-aj', '-achenn', '-achoù', '-ad,-iad', '-ad', '-iad', '-adeg', '-adegezh', '-adell', '-adenn', '-adez,-iadez', '-adez', '-iadez', '-adoù', '-adur', '-adurezh', '-adus', '-adusted', '-aer,-aour', '-aer', '-aour', '-af', '-ailh (N.)', '-ailhez', '-ailhoù', '-amant,-mant', '-amant', '-mant', '-an (Adj.)', '-ant,-iant', '-ant', '-iant', '-antez,-entez', '-antez', '-entez', '-aou-,-où (N.)', '-aou-', '-où (N.)', '-aoueg', '-aouennoù', '-ard,-er', '-ard', '-er', '-ardenn', '-as', '-asenn', '-atadur', '-añs,-iañs', '-añs', '-iañs', '-choù', '-ded,-ted', '-ded', '-ted', '-der,-ter', '-der', '-ter', '-ed (N.)', '-edeg', '-edigezh,-idigezh', '-edigezh', '-idigezh', '-edigoù', '-edoù', '-eg (langue)', '-eg,-og (N.)', '-eg', '-og (N.)', '-eg,-og (N.A.)', '-eg', '-og (N.A.)', '-egell,-igell,-ikell', '-egell', '-igell', '-ikell', '-egez', '-egezed', '-egezh', '-egi', '-el,-ol', '-el', '-ol', '-elezh', '-ell', '-elloù', '-en (N. coll.)', '-enenn', '-enn', '-ennad', '-ennigoù', '-enniñ', '-ennoù', '-ent (PL.)', '-enti,-inti', '-enti', '-inti', '-er,-our', '-er', '-our', '-erell,-orell', '-erell', '-orell', '-erez,-ourez', '-erez', '-ourez', '-erezed,-ourezed', '-erezed', '-ourezed', '-erezh,-ereah,-ourezh', '-erezh', '-ereah', '-ourezh', '-erien,-ierien,-erion,-ierion', '-erien', '-ierien', '-erion', '-ierion', '-eris', '-et,-iet (Adj.)', '-et', '-iet (Adj.)', '-eul', '-ez (nom collectif)', '-ezed', '-ezh (N.)', '-ezon', '-henn', '-i (N.)', '-iadenn', '-ianed', '-ichoù (PL.)', '-id', '-idell', '-idi,-iri,-eri,-euri', '-idi', '-iri', '-eri', '-euri', '-idik', '-ien,-ion (N. coll.)', '-ien', '-ion (N. coll.)', '-ienn,-ijenn,-ïon', '-ienn', '-ijenn', '-ïon', '-iennadoù', '-iennoù', '-ierigoù', '-iezh', '-igan,-egan', '-igan', '-egan', '-igenn,-egenn', '-igenn', '-egenn', '-ijennad', '-il,-nil', '-il', '-nil', '-ilh', '-in', '-ionez', '-ision', '-itaj', '-itell', '-iz (N.)', '-izien', '-izion', '-lann', \"-lec'h\", '-nezh', '-ni,-oni', '-ni', '-oni', '-od', '-odenn', '-ogez', '-on (N.)', '-onenn', '-oniezh', '-onsi', '-or,-our', '-or', '-our', '-orenn', '-ourien', '-ouriezh', '-ourion', '-ozh', '-oùachoù,-oùajoù', '-oùachoù', '-oùajoù', '-oùier', '-oùigoù', '-ur', '-va,-van', '-va', '-van', '-van', '-vedenn', '-vet', '-vezh', '-vezhienn', '-> page au hasard', '-> thème au hasard']\n",
      "Verb: ['-a', '-aat', '-al (Inf.)', '-aoua', '-aouiñ', '-at (Inf.)', '-ata', '-añ,-iañ (Inf.)', '-añ', '-iañ (Inf.)', '-eal', '-eiñ', '-ek (Inf.)', '-el (Inf.)', '-ellat,-ellañ', '-ellat', '-ellañ', '-en (Inf.)', '-et (Inf.)', '-eta', '-etaer', '-ezh (Inf.)', '-igellat,-igellañ', '-igellat', '-igellañ', '-ika', '-ikat', '-isañ', '-iñ (Inf.)', '-o', '-out (Inf.)', '-> page au hasard', '-> thème au hasard']\n",
      "Adjectif: ['-abl,-apl', '-abl', '-apl', '-adek', '-adurus', '-ailh (Adj.)', '-an (Adj.)', '-ant (Adj.)', '-ant,-iant', '-ant', '-iant', '-ard,-er', '-ard', '-er', '-at (Adj.)', '-at,-et (excl.)', '-at', '-et (excl.)', '-aus', '-edik,-idik,-ik', '-edik', '-idik', '-ik', '-ek (Adj.)', '-el (Adj.)', '-ian (Adj.)', '-idant', '-idik', '-ion (Adj.)', '-ous', '-ubl,-upl', '-ubl', '-upl', '-us,-ius,-uz', '-us', '-ius', '-uz', '-ø (Adj.)', '-> page au hasard', '-> thème au hasard']\n",
      "Adverbe: ['Gwall-', 'Hogos-']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_words_from_url(url):\n",
    "    # obtenir le contenu de la page\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # HTMl analyse et extraction des mots\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    words = []\n",
    "    for word in soup.find_all('a', href=True):  \n",
    "        if word.text.startswith('-'):\n",
    "            modified_word = word.text\n",
    "            # supprimer tous les espaces après la virgule dans tous les mots\n",
    "            modified_word = modified_word.replace(', ', ',')\n",
    "            words.append(modified_word)\n",
    "    return words\n",
    "def extract_words_from_adverbe(url):\n",
    "    # obtenir le contenu de la page\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # HTMl analyse et extraction des mots\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    words = []\n",
    "    for word in soup.find_all('a', href=True):  # trouver tous les liens\n",
    "        if word.text.endswith('-'):\n",
    "            modified_word = word.text\n",
    "            # supprimer tous les espaces après la virgule dans tous les mots\n",
    "            modified_word = modified_word.replace(', ', ',')\n",
    "            words.append(modified_word)\n",
    "    return words\n",
    "# extraire les mots de la page\n",
    "noms = extract_words_from_url('https://arbres.iker.cnrs.fr/index.php?title=Cat%C3%A9gorie:Noms')\n",
    "verbs = extract_words_from_url('https://arbres.iker.cnrs.fr/index.php?title=Cat%C3%A9gorie:Verbes&pageuntil=Categories%0AFoeta%C3%B1#mw-pages')\n",
    "adjectifs = extract_words_from_url('https://arbres.iker.cnrs.fr/index.php?title=Cat%C3%A9gorie:Adjectifs')\n",
    "adverbes = extract_words_from_adverbe('https://arbres.iker.cnrs.fr/index.php?title=Cat%C3%A9gorie:Adverbes&pageuntil=Categories%0APenn-da-benn#mw-pages')\n",
    "\n",
    "# traiter les mots avec des virgules\n",
    "processed_list_noms = []\n",
    "for item in noms:\n",
    "    if ',' in item:\n",
    "        # ajouter la chaîne d'origine\n",
    "        processed_list_noms.append(item)\n",
    "        # ajouter les éléments divisés\n",
    "        processed_list_noms.extend(item.split(','))\n",
    "    else:\n",
    "        processed_list_noms.append(item)\n",
    "\n",
    "# supprimer les espaces avant et après les éléments divisés\n",
    "processed_list_noms = [element.strip() for element in processed_list_noms]\n",
    "\n",
    "processed_list_verbs = []\n",
    "for item in verbs:\n",
    "    if ',' in item:\n",
    "        # ajouter la chaîne d'origine\n",
    "        processed_list_verbs.append(item)\n",
    "        # ajouter les éléments divisés\n",
    "        processed_list_verbs.extend(item.split(','))\n",
    "    else:\n",
    "        processed_list_verbs.append(item)\n",
    "\n",
    "# supprimer les espaces avant et après les éléments divisés\n",
    "processed_list_verbs = [element.strip() for element in processed_list_verbs]\n",
    "\n",
    "processed_list_adjectifs = []\n",
    "for item in adjectifs:\n",
    "    if ',' in item:\n",
    "        # ajouter la chaîne d'origine\n",
    "        processed_list_adjectifs.append(item)\n",
    "        # ajouter les éléments divisés\n",
    "        processed_list_adjectifs.extend(item.split(','))\n",
    "    else:\n",
    "        processed_list_adjectifs.append(item)\n",
    "\n",
    "# supprimer les espaces avant et après les éléments divisés\n",
    "processed_list_adjectifs = [element.strip() for element in processed_list_adjectifs]\n",
    "\n",
    "# afficher les résultats\n",
    "print(\"Noms:\",processed_list_noms)\n",
    "print(\"Verb:\", processed_list_verbs)\n",
    "print(\"Adjectif:\", processed_list_adjectifs)\n",
    "print(\"Adverbe:\", adverbes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeabba0",
   "metadata": {},
   "source": [
    "### Ajouter les gloses et lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "90045b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('none2.conllu', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "current_entry = None\n",
    "pattern = re.compile(r'^(\\d+)-(\\d+)\\t')\n",
    "\n",
    "for line_number, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    \n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        start, end = map(int, match.groups())\n",
    "        gloss_values = []\n",
    "        is_nom = False\n",
    "        is_verb = False\n",
    "        is_adjectif = False\n",
    "        is_adverbe = False\n",
    "        for i in range(start, end + 1):\n",
    "            subsequent_line_index = line_number + (i - start + 1)\n",
    "            subsequent_line = lines[subsequent_line_index].strip()\n",
    "            if i == end:\n",
    "                parts_subsequent_line = subsequent_line.split('\\t')\n",
    "                if len(parts_subsequent_line) > 1:\n",
    "                    if parts_subsequent_line[1] == \"a\":\n",
    "                        parts_subsequent_line[1] = \"-a\"\n",
    "                        parts_subsequent_line[2] = \"-a\"\n",
    "                        parts_subsequent_line[3] = \"verb\"\n",
    "                        subsequent_line = '\\t'.join(parts_subsequent_line)\n",
    "\n",
    "            gloss_info = subsequent_line.split('\\t')[-1]\n",
    "            gloss_part = gloss_info.partition('Gloss=')[2].split('|', 1)[0]\n",
    "            if gloss_part not in [\"P\", \"p\", \"Nom\", \"sfx\", \"Adj\",\"pfx\",\"PL\"]:\n",
    "                gloss_values.append(gloss_part)\n",
    "                ## si le mot apparait dans la liste processed_list_noms, on ajoute la valeur \"nom\" à la colonne 4:\n",
    "            if subsequent_line.split('\\t')[2] in processed_list_noms or \"(PL.)\" in subsequent_line.split('\\t')[2] or \"pluriel\" in subsequent_line.split('\\t')[3]:\n",
    "                is_nom = True\n",
    "            if subsequent_line.split('\\t')[2] in processed_list_verbs:\n",
    "                is_verb = True\n",
    "            if subsequent_line.split('\\t')[2] in processed_list_adjectifs or \"(Adj.)\" in subsequent_line.split('\\t')[2]:\n",
    "                is_adjectif = True\n",
    "            if subsequent_line.split('\\t')[2] in adverbes:\n",
    "                is_adverbe = True\n",
    "            \n",
    "        if subsequent_line.split('\\t')[1] in [\"-oc'h\", \"-ig\", \"IMP\", \"superlatif\",\"-erezh\"] or subsequent_line.split('\\t')[9]==\"Gloss=ramasser\" or subsequent_line.split('\\t')[9]==\"Gloss=tout-à-fait\":\n",
    "            gloss_values.reverse()\n",
    "        original_line_parts = line.split('\\t')\n",
    "        original_line_parts[-1] = 'Gloss=' + ' '.join(gloss_values)\n",
    "        if is_nom:\n",
    "            original_line_parts[3] = 'nom'\n",
    "        if is_verb:\n",
    "            original_line_parts[3] = 'verbe'\n",
    "        if is_adjectif:\n",
    "            original_line_parts[3] = 'adjectif'\n",
    "        if is_adverbe:\n",
    "            original_line_parts[3] = 'adverbe'\n",
    "        lines[line_number] = '\\t'.join(original_line_parts) + '\\n'\n",
    "\n",
    "        \n",
    "with open('none2.conllu', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c3021",
   "metadata": {},
   "source": [
    "### Extraire tous les mots composés(optional, can pass directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dde919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Ouvrir et lire le fichier\n",
    "with open('none2.conllu', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parcourir chaque ligne du fichier\n",
    "entries = []\n",
    "current_entry = None\n",
    "pattern = re.compile(r'^(\\d+)-(\\d+)\\t')\n",
    "\n",
    "# Parcourir chaque ligne du fichier\n",
    "for line_index, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "\n",
    "    if line.startswith(\"#\"):  # Commentaire, ignorer\n",
    "        if current_entry is None:\n",
    "            current_entry = {'header': [], 'lines': []}\n",
    "        current_entry['header'].append(line)\n",
    "    elif line == \"\":  # Ligne vide, ajouter l'entrée actuelle à la liste des entrées et réinitialiser l'entrée actuelle\n",
    "        if current_entry and current_entry['lines']:\n",
    "            entries.append(current_entry)\n",
    "        current_entry = None\n",
    "    else:\n",
    "        # Vérifier si la ligne commence par un nombre suivi d'un tiret\n",
    "        match = pattern.match(line)\n",
    "        if match and current_entry is not None:\n",
    "            start, end = map(int, match.groups())\n",
    "            current_entry['lines'].append(line)  # Ajouter la ligne actuelle à l'entrée actuelle\n",
    "            \n",
    "            \n",
    "            # Ajouter les lignes précédentes à l'entrée actuelle\n",
    "            for i in range(start, end + 1):\n",
    "                current_line = lines[line_index + i - (start - 1)].strip() # Récupérer la ligne correspondante\n",
    "                current_entry['lines'].append(current_line)\n",
    "\n",
    "# Ajouter la dernière entrée à la liste des entrées\n",
    "if current_entry and current_entry['lines']:\n",
    "    entries.append(current_entry)\n",
    "\n",
    "# Écrire les entrées dans un nouveau fichier\n",
    "with open('fichiers_traitement/mots_composes.conllu', 'w', encoding='utf-8') as output_file:\n",
    "    for entry in entries:\n",
    "        for header_line in entry['header']:\n",
    "            output_file.write(header_line + '\\n')\n",
    "        for line in entry['lines']:\n",
    "            output_file.write(line + '\\n')\n",
    "        output_file.write('\\n')  # Ajouter une ligne vide entre les entrées\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e3cef",
   "metadata": {},
   "source": [
    "## Déplacer la 4ème colonne pour les UPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d6b0f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Ouvrir le fichier et lire son contenu\n",
    "with open('none2.conllu', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "pattern = re.compile(r'^\\d+(-\\d+)?\\t')  # Matcher les numéros au début de la ligne, indiquant l'ID des mots ou des tokens\n",
    "\n",
    "# Initialiser une liste vide pour stocker les lignes modifiées\n",
    "modified_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.rstrip()  # Supprimer les espaces ou sauts de ligne à la fin\n",
    "    # Vérifier et corriger les séquences de '\\t'\n",
    "    line = re.sub(r'\\t\\t\\t', r'\\t_\\t_\\t', line)\n",
    "    line = re.sub(r'\\t\\t', r'\\t_\\t', line)\n",
    "    if line.startswith(\"#\"):  # Ajouter directement les lignes de commentaires\n",
    "        modified_lines.append(line)\n",
    "        continue\n",
    "\n",
    "    match = pattern.match(line)\n",
    "    if match:  # Si la ligne contient un ID de mot ou de token\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) > 4:  # Assurer que la ligne contient suffisamment de parties pour être traitée\n",
    "            # Déplacer le contenu du 5ème champ au 6ème champ, si le 6ème champ n'existe pas, l'ajouter\n",
    "            if len(parts) > 5:\n",
    "                parts[5] = parts[4]  # Si un 6ème champ existe déjà, remplacer son contenu par celui du 5ème champ\n",
    "            else:\n",
    "                parts.append(parts[4])  # Si aucun 6ème champ, ajouter le contenu du 5ème champ\n",
    "\n",
    "            # Déplacer le contenu du 4ème champ au 5ème et mettre \"_\" dans le 4ème champ\n",
    "            parts[4] = parts[3]\n",
    "            parts[3] = \"_\"\n",
    "            \n",
    "            modified_line = '\\t'.join(parts)  # Recombiner la ligne\n",
    "            modified_lines.append(modified_line)\n",
    "        else:\n",
    "            # Si la ligne ne contient pas assez de parties, l'ajouter telle quelle\n",
    "            modified_lines.append(line)\n",
    "    else:\n",
    "        # Pour les lignes qui ne commencent pas par un numéro, les ajouter directement\n",
    "        modified_lines.append(line)\n",
    "\n",
    "# Écrire le contenu modifié dans le fichier\n",
    "with open('none2.conllu', 'w', encoding='utf-8') as file:\n",
    "    for line in modified_lines:\n",
    "        file.write(line + '\\n')  # Écrire les lignes modifiées, en ajoutant un saut de ligne à la fin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85044759",
   "metadata": {},
   "source": [
    "# Séparer les exemples dans none2.conllu selon les dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bf8eea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of your dialect categories\n",
    "dialect_categories = ['léonard', 'cornouaillais', 'trégorrois', 'vannetais', 'breton central', 'standard']\n",
    "\n",
    "# Adding 'inconnu' category for unmatched examples\n",
    "dialect_categories.append('inconnu')\n",
    "\n",
    "# Dictionary to hold the examples for each dialect\n",
    "examples_by_dialect = {dialect: [] for dialect in dialect_categories}\n",
    "\n",
    "# Function to check if the dialect line contains any of the specified dialects\n",
    "def contains_dialect(line, dialects):\n",
    "    for dialect in dialects:\n",
    "        if dialect.lower() in line.lower():\n",
    "            return dialect\n",
    "    return 'inconnu'  # Return 'inconnu' if no match is found\n",
    "\n",
    "# Read the file and collect examples\n",
    "with open('none2.conllu', 'r', encoding='utf-8') as file:\n",
    "    examples = file.read().strip().split('\\n\\n')\n",
    "\n",
    "# Process each example\n",
    "for example in examples:\n",
    "    lines = example.split('\\n')\n",
    "    dialect_line = next((line for line in lines if line.startswith('# dialect =')), None)\n",
    "    matched_dialect = contains_dialect(dialect_line, dialect_categories[:-1]) if dialect_line else 'inconnu'\n",
    "    examples_by_dialect[matched_dialect].append(example + '\\n')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "output_directory = \"bretonconlls\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save examples into separate files based on dialect and update the sent_id\n",
    "for dialect, examples in examples_by_dialect.items():\n",
    "    filename = os.path.join(output_directory, f\"{dialect.lower().replace(' ', '_')}.conllu\")\n",
    "    with open(filename, 'w', encoding='utf-8') as output_file:\n",
    "        for index, example in enumerate(examples, 1):\n",
    "            lines = example.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith(\"# sent_id =\"):\n",
    "                    parts = line.split(\"__\")\n",
    "                    lines[i] = f\"{parts[0]}__{index}\"\n",
    "            updated_example = '\\n'.join(lines)\n",
    "            output_file.write(updated_example + '\\n')  # Double newline to separate examples in the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "breton_27_07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
