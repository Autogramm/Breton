{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanToken(t):\n",
    "    t = t.replace(\"...\", \"…\")\n",
    "    t = t.replace(\"tout.à.l'heure\", \"tout_à_l'heure\")\n",
    "\n",
    "\n",
    "\n",
    "    t = t.replace(\"<u>\", \"\")\n",
    "    t = t.replace(\"</u>\", \"\")\n",
    "\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def alignTokLemmgloss(text, lemgloss):\n",
    "    pattern = r'\\[\\[(\\w+)\\|\\|(\\w+)\\]\\]'\n",
    "    lemgloss = re.sub(pattern, r'[[\\1|\\2]]', lemgloss)\n",
    "\n",
    "    text_list = text.split(\"||\")\n",
    "    lemgloss_list = lemgloss.split(\"||\")\n",
    "    couple_list = []\n",
    "\n",
    "    if any(\"KLT\" in tok or \"Sujet\" in tok for tok in text_list) or any(\"KLT\" in lem or \"Sujet\" in lem for lem in lemgloss_list):\n",
    "        return couple_list\n",
    "\n",
    "    text_list_stripped = [tok.strip() for tok in text_list]\n",
    "    lemgloss_list_stripped = [lem.strip() for lem in lemgloss_list]\n",
    "\n",
    "    if \"standard\" in text.lower() or \"standard\" in lemgloss.lower() or \"Équivalent\" in text or \"Graphie\" in text:\n",
    "        return couple_list\n",
    "\n",
    "    if text_list_stripped and text_list_stripped[-1] == \"\":\n",
    "        text_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"<elles>\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"_\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and re.search(r'\\d{4}', lemgloss_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and re.search(r'\\d{4}', text_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and \"colspan\" in text_list_stripped[-1]:\n",
    "        return couple_list\n",
    "    if \"[[*]]\" in text_list_stripped:\n",
    "        return couple_list\n",
    "\n",
    "    if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "        for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "            tok = cleanToken(tok)\n",
    "            lemmgloss = cleanToken(lemmgloss)\n",
    "            if tok and lemmgloss:\n",
    "                couple_list.append((tok, lemmgloss))\n",
    "    else:\n",
    "        punctuation_marks = ['!', '.', ',', '?', ';', ':']\n",
    "        for punctuation in punctuation_marks:\n",
    "            if text_list_stripped and text_list_stripped[-1].endswith(punctuation) and not lemgloss_list_stripped[-1].endswith(punctuation):\n",
    "                lemgloss_list_stripped.append(punctuation)\n",
    "\n",
    "        if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "            for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                tok = cleanToken(tok)\n",
    "                lemmgloss = cleanToken(lemmgloss)\n",
    "                if tok and lemmgloss:\n",
    "                    couple_list.append((tok, lemmgloss))\n",
    "        else:\n",
    "            for punctuation in punctuation_marks:\n",
    "                if lemgloss_list_stripped and lemgloss_list_stripped[-1].endswith(punctuation) and not \\\n",
    "                text_list_stripped[-1].endswith(punctuation):\n",
    "                    text_list_stripped.append(punctuation)\n",
    "\n",
    "            if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "                for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                    tok = cleanToken(tok)\n",
    "                    lemmgloss = cleanToken(lemmgloss)\n",
    "                    if tok and lemmgloss:\n",
    "                        couple_list.append((tok, lemmgloss))\n",
    "            else:\n",
    "                with open(\"not_couple.txt\", \"a\") as file:\n",
    "                    file.write(\"text\\n\")\n",
    "                    file.write(f\"{text}\\n\")\n",
    "                    file.write(\"lemgloss\\n\")\n",
    "                    file.write(f\"{lemgloss}\\n\")\n",
    "    return couple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_lines():\n",
    "    filename = \"all_line_objects.txt\"\n",
    "    all_examples = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    example = {}\n",
    "    store_text = False\n",
    "    store_lemgloss = False\n",
    "    store_trans = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if store_text:\n",
    "            if line.startswith(\"||\"):\n",
    "                example[\"text\"] = line[0:]\n",
    "            else:\n",
    "                example[\"text\"] = line[3:]\n",
    "            store_text = False\n",
    "        elif store_lemgloss:\n",
    "            example[\"lemgloss\"] = line\n",
    "            store_lemgloss = False\n",
    "        elif store_trans:\n",
    "            example['translation'] = line\n",
    "            store_trans = False\n",
    "\n",
    "            if example[\"text\"] != \"\" and example[\"lemgloss\"] != \"None\":\n",
    "                all_examples.append(example)\n",
    "\n",
    "            example = {}\n",
    "        elif line == \"text\":\n",
    "            store_text = True\n",
    "        elif line == \"lemgloss\":\n",
    "            store_lemgloss = True\n",
    "        elif line == \"translation\":\n",
    "            store_trans=True\n",
    "\n",
    "    # Process each example\n",
    "    for example in all_examples:\n",
    "        text = example[\"text\"]\n",
    "        lemgloss = example[\"lemgloss\"]\n",
    "        trans = example['translation']\n",
    "\n",
    "\n",
    "        example_info = alignTokLemmgloss(text, lemgloss)\n",
    "        if example_info:\n",
    "            with open('couple.txt','a') as t:\n",
    "                t.write('text\\n')\n",
    "                t.write(f'{text}\\n')\n",
    "                t.write('translation\\n')\n",
    "                t.write(f'{trans}\\n')\n",
    "\n",
    "            with open('all_info.txt','a') as nfile:\n",
    "                nfile.write('text\\n')\n",
    "                nfile.write(f'{text}\\n')\n",
    "                nfile.write('lemgloss\\n')\n",
    "                nfile.write(f'{lemgloss}\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "whole_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple2tokobjects(couple):\n",
    "    text, lem = couple\n",
    "\n",
    "    lem = re.sub(r'<sup>(\\[\\[[^\\[\\]]+\\]\\])(.*?)</sup>',\n",
    "                 lambda m: ''.join(f'<sup>{x}</sup>' for x in re.findall(r'\\[\\[[^\\[\\]]+\\]\\]', m.group(0))), lem)\n",
    "    lem = re.sub('particule o', 'o', lem)\n",
    "\n",
    "    lem = re.sub(r'<font color=green>.*?</font color=green>', '', lem)\n",
    "\n",
    "    if '[[R]]' not in lem:\n",
    "        match = re.search(r'<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match:\n",
    "            k = match.group(2).strip(']')\n",
    "            lem = re.sub(r'<sup>\\[\\[(?!(1|4)\\]\\]).*?</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        if \"particule o\" not in lem:\n",
    "            lem = re.sub(r'<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' in lem:\n",
    "        match1 = re.search(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match1:\n",
    "            k = match1.group(2).strip(']')\n",
    "            lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' not in lem:\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]', '[[R|_]]', lem)\n",
    "        k = 'R'\n",
    "\n",
    "    if \"'''\" in text and '-' in lem and re.search(r'\"\"\".*?\"\"\"', text):\n",
    "        extra_lem = re.search(r\"'''(.*?)'''\", text).group(1)\n",
    "        match = re.search(r'(?<=\\|{2}).*?-(.*?)(?=\\|\\|)', lem)\n",
    "\n",
    "        extra_gloss = match.group(1).strip()\n",
    "        lem = re.sub(r'-(.*?)\\|\\|', f'[[{extra_lem}|{extra_gloss}]] ||', lem)\n",
    "\n",
    "    text_list = text.split('||')\n",
    "    lem_list = lem.split('||')\n",
    "\n",
    "    tok_objects = []\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        tok = text_list[i].strip()\n",
    "        lem_parts = lem_list[i].strip().split(']]')\n",
    "        lem_sp = lem_list[i].strip().split('|')\n",
    "        tok_list = tok.split(' ')\n",
    "\n",
    "        if len(lem_sp) == 1 and tok.startswith(\"'''\"):\n",
    "            tok_objects.append({\n",
    "                'tok': tok.replace(\"'''\", ''),\n",
    "                'lemma': tok.replace(\"'''\", ''),\n",
    "                'gloss': lem_parts[0]\n",
    "            })\n",
    "\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and ('SC' or 'VP' or 'PredP' or 'CP' or 'DP') in lem_sp[0]:\n",
    "            match = re.search(r'<sub>\\[\\[(.*?)\\]\\]', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'PhraseStructure': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 1 and \"<sub>\" in lem_sp[0] and '[[' not in lem_sp[0]:\n",
    "            match = re.search(r'<sub>(.*?)</sub>', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'index': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 2 and '[[' in lem_sp[0]:\n",
    "\n",
    "            gloss = lem_sp[1].rstrip('].')\n",
    "            tok_objects.append({\n",
    "                'tok': tok,\n",
    "                'lemma': lem_sp[0].strip('[]'),\n",
    "                'gloss': gloss\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif len(tok_list) == len(lem_parts) - 1:\n",
    "\n",
    "            for i in range(len(tok_list)):\n",
    "                lem_inside = lem_parts[i].split('|')[0]\n",
    "                gloss_inside = lem_parts[i].split('|')[-1]\n",
    "\n",
    "                tok_objects.append({'tok': tok_list[i],\n",
    "                                    'lemma': lem_inside[3:],\n",
    "                                    'gloss': gloss_inside\n",
    "\n",
    "                                    })\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            tok_objects.append({\n",
    "                'tok': tok,\n",
    "                'lemma': '_',\n",
    "                'gloss': '_',\n",
    "                'multiple': len(lem_parts) - 1})\n",
    "            tok_obj = {'tok': tok, 'lemma': '_', 'gloss': '_', 'multiple': len(lem_parts) - 1}\n",
    "            tok_n = 1\n",
    "\n",
    "            for j in range(len(lem_parts)):\n",
    "                if '[[' in lem_parts[j]:\n",
    "                    lemma_gloss = lem_parts[j].strip('[[').split('|')\n",
    "                    lemma = lemma_gloss[0].split('[[')[-1].strip()\n",
    "\n",
    "                    if re.search(r'^-\\w+', lemma):\n",
    "                        if lemma != '-ig':\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 and tok != '-ig' else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': 'petit'\n",
    "                            })\n",
    "\n",
    "\n",
    "                    elif lemma_gloss == ['R', '_']:\n",
    "                        if k != '+C':\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else '',\n",
    "                                'Epenthesis': 'Yes'\n",
    "                            })\n",
    "\n",
    "                    elif lemma_gloss == ['a', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '1',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    elif lemma_gloss == ['e', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '4',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    else:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma.strip(),\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    if '-ig' in tok_obj['tok'] and '-ig' in tok_obj['lemma']:\n",
    "                        tok_obj['gloss'] = 'petit'\n",
    "\n",
    "                    tok_n += 1\n",
    "\n",
    "    return tok_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "\n",
    "def get_wikicode(title):\n",
    "    wikicode=''\n",
    "    # on traite les redirections vers d'autres pages\n",
    "    if '#REDIRECTION' in pages.get(title, ''):             \n",
    "        #on suit la redirection\n",
    "        newtitle = relinks.search(pages[title]).group(1)                \n",
    "        newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "        if '_' in newtitle: \n",
    "            newtitle = newtitle.replace(\"_\", \" \")            \n",
    "        #et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "        if newtitle in pages:\n",
    "            wikicode = pages[newtitle]              \n",
    "        else: \n",
    "            newtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "            if newtitle in pages:\n",
    "                wikicode = pages[newtitle]\n",
    "            else:\n",
    "                newtitle = newtitle.split(',')[0]\n",
    "                newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                if newtitle in pages:\n",
    "                    wikicode = pages[newtitle]\n",
    "                else:\n",
    "                    wikicode='__currentPage:\\nstrange redirect to page that does not exist: '+newtitle\n",
    "                    print(wikicode+'\\n')\n",
    "\n",
    "    else:\n",
    "        wikicode = pages.get(title, '')  \n",
    "    return wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9163 pages\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "with open('Pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rePOS = re.compile(r'articles?|art|verbes?|auxiliaires?|copules?|adverbes?|complémenteurs?|conjonctions?|prépositions?|adjectifs?|noms?|particules verbales?|interjections?|postpositions?|déterminants?|quantifieurs?|pronoms?|noms? propres?|suffixe|interrogatifs?|préfixes?|modaux|pluriels?|indéfinis?|particules? de discours|finales?|exclamatifs?', re.IGNORECASE)\n",
    "\n",
    "lefffl = [li.strip() for li in open('lefff-2.1.txt').read().split('\\n') if li.strip() and li[0]!='#']\n",
    "lefff = {li.split('\\t')[0]:li.split('\\t')[2] for li in lefffl}\n",
    "# print(lefff)\n",
    "\n",
    "\n",
    "def add_pos(tok_obj):\n",
    "    if 'pos' in tok_obj and tok_obj['pos'] : return tok_obj # pos already there\n",
    "\n",
    "    # is the pos in the lemma?\n",
    "    if 'lemma' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['lemma'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    \n",
    "    # is the pos in the gloss?\n",
    "    if 'gloss' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['gloss'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    \n",
    "    # let's check the page for the lemma\n",
    "    wikicode = get_wikicode(str.capitalize(tok_obj.get('lemma',''))) # TODO: improve as this might fail sometimes\n",
    "    # print(wikicode[:222])\n",
    "    m = rePOS.search(wikicode)\n",
    "    if m:\n",
    "        tok_obj['pos'] = m.group(0)\n",
    "        return tok_obj\n",
    "    \n",
    "    # let's use the gloss to make an educated guess about the pos using lefff\n",
    "    if tok_obj.get('gloss','') in lefff:\n",
    "        tok_obj['pos'] = lefff[tok_obj.get('gloss','')]\n",
    "    \n",
    "    return tok_obj\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_conll_output(tok_objects, text, text_fr, sent_id):\n",
    "    conll_output = \"\"\n",
    "    conll_id = 1\n",
    "    inside_id = 1\n",
    "\n",
    "    for tok_obj in tok_objects:\n",
    "        if isinstance(tok_obj, dict) and len(tok_obj) == 1:\n",
    "            field_key, field_value = next(iter(tok_obj.items()))\n",
    "            conll_line = f\"{conll_id}\\t{field_key}\\t{field_value}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "        else:\n",
    "            if 'multiple' not in tok_obj:\n",
    "                \n",
    "                conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\n",
    "                conll_id += 1\n",
    "                inside_id = conll_id\n",
    "            else:\n",
    "                conll_line = f\"{inside_id}-{inside_id + tok_obj['multiple'] - 1}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "\n",
    "            conll_line = re.sub(']]', ' ', conll_line)\n",
    "            conll_line = re.sub('\\[\\[kaout', 'kaout', conll_line)\n",
    "            conll_line = re.sub(\"'''\", '', conll_line)\n",
    "            conll_line = re.sub('\\[','',conll_line)\n",
    "\n",
    "        conll_output += conll_line.strip() + \"\\n\"\n",
    "\n",
    "    conll_output = f\"# sent_id = None__{sent_id}\\n\" + \\\n",
    "                   f\"# text = {text.replace('||', '')}\\n\" + \\\n",
    "                   f\"# text_fr = {text_fr}\\n\" + \\\n",
    "                   f\"# status = WIP\\n\" + \\\n",
    "                   conll_output + \"\\n\"\n",
    "\n",
    "    return conll_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22612/1811248548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tokens_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22612/1811248548.py\u001b[0m in \u001b[0;36mprocess_tokens_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mconll_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerate_conll_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0msent_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Increment sent_id for the next example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22612/2492288328.py\u001b[0m in \u001b[0;36mgenerate_conll_output\u001b[0;34m(tok_objects, text, text_fr, sent_id)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'multiple'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtok_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mconll_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mconll_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0minside_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconll_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pos'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def process_tokens_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    conll_output = \"\"\n",
    "    sent_id = 1  # Initialize sent_id to 1\n",
    "\n",
    "    with open(\"couple.txt\", \"r\") as couple_file:\n",
    "        couple_lines = couple_file.readlines()\n",
    "        couple_lines = [line.strip() for line in couple_lines]\n",
    "\n",
    "    modified_tok_objects = []  # List to store modified token objects\n",
    "\n",
    "    with open(\"couple_n.txt\", \"w\") as output_file:  # Open the output file for writing\n",
    "        for i, line in enumerate(lines):\n",
    "            tok_objects = ast.literal_eval(line)\n",
    "            modified_tok_objects.extend([add_pos(tok_obj) for tok_obj in tok_objects])\n",
    "\n",
    "            text = couple_lines[i * 4 + 1].replace(\"text\", \"\").strip()\n",
    "            text = re.sub(\"'''\", '', text)\n",
    "            text_fr = couple_lines[i * 4 + 3].replace(\"translation\", \"\").strip()\n",
    "\n",
    "            # Update the sent_id value for each example\n",
    "            conll_output += generate_conll_output\n",
    "\n",
    "\n",
    "        conll_output += generate_conll_output(tok_objects, text, text_fr,sent_id)\n",
    "        sent_id += 1  # Increment sent_id for the next example\n",
    "\n",
    "    return conll_output\n",
    "\n",
    "\n",
    "output = process_tokens_file('tokens.txt')\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
