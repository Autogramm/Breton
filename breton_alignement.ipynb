{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanToken(t):\n",
    "    t = t.replace(\"...\", \"\")\n",
    "    t = t.replace(\"tout.à.l'heure\", \"tout_à_l'heure\")\n",
    "\n",
    "    t=t.replace('<font color=green>','')\n",
    "    t = t.replace('</font color=green>','')\n",
    "\n",
    "    t = t.replace(\"<u>\", \"\")\n",
    "    t = t.replace(\"</u>\", \"\")\n",
    "\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def alignTokLemmgloss(text, lemgloss, phonetic):\n",
    "    '''\n",
    "    takes three lines of an example, produces triples\n",
    "    '''\n",
    "    pattern = r'\\[\\[(\\w+)\\|\\|(\\w+)\\]\\]'\n",
    "    lemgloss = re.sub(pattern, r'[[\\1|\\2]]', lemgloss)\n",
    "\n",
    "\n",
    "    text_list = text.split(\"||\")\n",
    "    lemgloss_list = lemgloss.split(\"||\")\n",
    "    phon_list = phonetic.split(\"||\")\n",
    "    couple_list = []\n",
    "\n",
    "\n",
    "\n",
    "    if any(\"KLT\" in tok or \"Sujet\" in tok for tok in text_list) or any(\"KLT\" in lem or \"Sujet\" in lem for lem in lemgloss_list):\n",
    "        return couple_list\n",
    "\n",
    "    text_list_stripped = [tok.strip() for tok in text_list]\n",
    "    lemgloss_list_stripped = [lem.strip() for lem in lemgloss_list]\n",
    "    phon_list_stripped = [phon.strip() for phon in phon_list]\n",
    "\n",
    "    if \"standard\" in text.lower() or \"standard\" in lemgloss.lower() or \"Équivalent\" in text or \"Graphie\" in text:\n",
    "        return couple_list\n",
    "\n",
    "    if text_list_stripped and text_list_stripped[-1] == \"\":\n",
    "        text_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"<elles>\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and lemgloss_list_stripped[-1] == \"_\":\n",
    "        lemgloss_list_stripped.pop()\n",
    "    if lemgloss_list_stripped and re.search(r'\\d{4}', lemgloss_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and re.search(r'\\d{4}', text_list_stripped[-1]):\n",
    "        return couple_list\n",
    "    if text_list_stripped and \"colspan\" in text_list_stripped[-1]:\n",
    "        return couple_list\n",
    "    if \"[[*]]\" in text_list_stripped:\n",
    "        # this indicates that the sentence is ungrammatical, and we skip it\n",
    "        return couple_list\n",
    "\n",
    "    if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "        \n",
    "        if phonetic==\"None\" :\n",
    "            for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                tok = cleanToken(tok)\n",
    "                lemmgloss = cleanToken(lemmgloss)\n",
    "                if tok and lemmgloss:\n",
    "                    couple_list.append((tok, lemmgloss,'None'))\n",
    "        elif phonetic !='None':\n",
    "            for tok, lemmgloss,phone in zip(text_list_stripped, lemgloss_list_stripped,phon_list_stripped):\n",
    "                tok = cleanToken(tok)\n",
    "                lemmgloss = cleanToken(lemmgloss)\n",
    "                phone=cleanToken(phone)\n",
    "                if tok and lemmgloss and phone:\n",
    "                    couple_list.append((tok, lemmgloss,phone))\n",
    "\n",
    "    else:\n",
    "        punctuation_marks = ['!', '.', ',', '?', ';', ':']\n",
    "        for punctuation in punctuation_marks:\n",
    "            if text_list_stripped and text_list_stripped[-1].endswith(punctuation) and not lemgloss_list_stripped[-1].endswith(punctuation):\n",
    "                lemgloss_list_stripped.append(punctuation)\n",
    "\n",
    "        if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "            if phonetic == \"None\":\n",
    "                for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                    tok = cleanToken(tok)\n",
    "                    lemmgloss = cleanToken(lemmgloss)\n",
    "                    if tok and lemmgloss:\n",
    "                        couple_list.append((tok, lemmgloss, 'None'))\n",
    "            elif phonetic != 'None':\n",
    "                for tok, lemmgloss, phone in zip(text_list_stripped, lemgloss_list_stripped, phon_list_stripped):\n",
    "                    tok = cleanToken(tok)\n",
    "                    lemmgloss = cleanToken(lemmgloss)\n",
    "                    phone = cleanToken(phone)\n",
    "                    if tok and lemmgloss and phone:\n",
    "                        couple_list.append((tok, lemmgloss, phone))\n",
    "        else:\n",
    "            for punctuation in punctuation_marks:\n",
    "                if lemgloss_list_stripped and lemgloss_list_stripped[-1].endswith(punctuation) and not \\\n",
    "                text_list_stripped[-1].endswith(punctuation):\n",
    "                    text_list_stripped.append(punctuation)\n",
    "\n",
    "            if len(text_list_stripped) == len(lemgloss_list_stripped):\n",
    "                if phonetic == \"None\":\n",
    "                    for tok, lemmgloss in zip(text_list_stripped, lemgloss_list_stripped):\n",
    "                        tok = cleanToken(tok)\n",
    "                        lemmgloss = cleanToken(lemmgloss)\n",
    "                        if tok and lemmgloss:\n",
    "                            couple_list.append((tok, lemmgloss, 'None'))\n",
    "                elif phonetic != 'None':\n",
    "                    for tok, lemmgloss, phone in zip(text_list_stripped, lemgloss_list_stripped, phon_list_stripped):\n",
    "                        tok = cleanToken(tok)\n",
    "                        lemmgloss = cleanToken(lemmgloss)\n",
    "                        phone = cleanToken(phone)\n",
    "                        if tok and lemmgloss and phone:\n",
    "                            couple_list.append((tok, lemmgloss, phone))\n",
    "\n",
    "            else:\n",
    "                with open(\"not_couple.txt\", \"a\") as file:\n",
    "                    file.write(\"text\\n\")\n",
    "                    file.write(f\"{text}\\n\")\n",
    "                    file.write(\"lemgloss\\n\")\n",
    "                    file.write(f\"{lemgloss}\\n\")\n",
    "    return couple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def couple2tokobjects(triple):\n",
    "    text, lem,phon = triple\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\].F<sup>\\[\\[1\\]\\]</sup>\", \"[[div|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|quatre\\]\\]<sup>\\[\\[2\\]\\]</sup>\", \"[[peder|quatre]]\", lem)\n",
    "\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup> \\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]\", \"[[gant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|cent\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[kant|cent]]\", lem)\n",
    "    lem = re.sub(r\"<sup>\\[\\[2\\]\\]</sup>\\[\\[cardinal\\|cent\\]\\]\", \"[[c'hant|cent]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[50|50]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[zaou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[dek\\|dix\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|dix\\]\\]\", \"[[dek|dix]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|2\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "    lem = re.sub(r\"\\[\\[cardinaux\\|mille\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[mil|mille]]\", lem)\n",
    "\n",
    "    lem = re.sub(r\"\\[\\[cardinal\\|50\\]\\]\", \"[[50|50]]\", lem)\n",
    "\n",
    "    if 'diou' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\.F\\]\\]<sup>\\[\\[1\\]\\]</sup>\", \"[[diou|deux.F]]\", lem)\n",
    "    if 'vil' in text:\n",
    "        lem = re.sub(r\"<sup>\\[\\[1\\]\\]</sup>\\[\\[cardinal\\|mille\\]\\]\", \"[[vil|mille]]\", lem)\n",
    "    if 'diouig' in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|2\\]\\]\", \"[[diou|2]]\", lem)\n",
    "    if \"daou-ha-daou\" in text:\n",
    "        lem = re.sub(r\"\\[\\[cardinal\\|deux\\]\\]\", \"[[daou|deux]]\", lem)\n",
    "\n",
    "    lem = re.sub(r'<sup>(\\[\\[[^\\[\\]]+\\]\\])(.*?)</sup>', lambda m: ''.join(f'<sup>{x}</sup>' for x in re.findall(r'\\[\\[[^\\[\\]]+\\]\\]', m.group(0))), lem)\n",
    "    lem = re.sub('particule o','o',lem)\n",
    "\n",
    "    lem = re.sub(r'<font color=green>.*?</font color=green>', '', lem)\n",
    "\n",
    "    phon=re.sub('\\[','',phon)\n",
    "    phon = re.sub(']', '', phon)\n",
    "\n",
    "    if '[[R]]' not in lem :\n",
    "        match = re.search(r'<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match:\n",
    "            k = match.group(2).strip(']')\n",
    "            lem = re.sub(r'<sup>\\[\\[(?!(1|4)\\]\\]).*?</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        if \"particule o\" not in lem:\n",
    "            lem = re.sub(r'<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' in lem:\n",
    "        match1 = re.search(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', lem)\n",
    "        if match1:\n",
    "            k = match1.group(2).strip(']')\n",
    "            lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[(?!(1|4)\\]\\])(.*?)</sup>', '[[R|_]]', lem)\n",
    "\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[4\\]\\]</sup>', '[[e|_]]', lem)\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]<sup>\\[\\[1\\]\\]</sup>', '[[a|_]]', lem)\n",
    "\n",
    "    if '[[R]]' in lem and '<sup>' not in lem:\n",
    "        lem = re.sub(r'\\[\\[R\\]\\]','[[R|_]]',lem)\n",
    "        k='R'\n",
    "\n",
    "    if \"'''\" in text and '-'in lem and re.search(r'\"\"\".*?\"\"\"', text):\n",
    "        extra_lem=re.search(r\"'''(.*?)'''\", text).group(1)\n",
    "        match = re.search(r'(?<=\\|{2}).*?-(.*?)(?=\\|\\|)', lem)\n",
    "\n",
    "        extra_gloss = match.group(1).strip()\n",
    "        lem = re.sub(r'-(.*?)\\|\\|', f'[[{extra_lem}|{extra_gloss}]] ||', lem)\n",
    "\n",
    "\n",
    "    text_list = text.split('||')\n",
    "    lem_list = lem.split('||')\n",
    "    phone_list = phon.split('||')\n",
    "\n",
    "    tok_objects = []\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        tok = text_list[i].strip()\n",
    "        lem_parts = lem_list[i].strip().split(']]')\n",
    "        lem_sp = lem_list[i].strip().split('|')\n",
    "        phonetic = phone_list[i].strip()\n",
    "\n",
    "        import string\n",
    "\n",
    "        tok_list = [part for part in tok.split(' ') if not all(char in string.punctuation for char in part)]\n",
    "        phon_list = [part2 for part2 in phon.split(' ') if not all(char2 in string.punctuation for char2 in part2)]\n",
    "\n",
    "\n",
    "        if len(lem_sp)== 1 and tok.startswith(\"'''\") :\n",
    "            tok_objects.append({\n",
    "                'tok': tok.replace(\"'''\",''),\n",
    "                'lemma': tok.replace(\"'''\",''),\n",
    "                'gloss': lem_parts[0],\n",
    "                'phonetic':phonetic.replace(\"'''\",'')\n",
    "            })\n",
    "\n",
    "\n",
    "        elif len(lem_sp)==1 and \"<sub>\" in lem_sp[0] and ('SC' or 'VP' or 'PredP' or 'CP' or 'DP') in lem_sp[0]:\n",
    "            match = re.search(r'<sub>\\[\\[(.*?)\\]\\]', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'PhraseStructure': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp)==1 and \"<sub>\" in lem_sp[0] and '[[' not in lem_sp[0]:\n",
    "            match = re.search(r'<sub>(.*?)</sub>', lem_sp[0])\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                tok_objects.append({\n",
    "                    'index': content\n",
    "                })\n",
    "\n",
    "        elif len(lem_sp) == 2 and '[[' in lem_sp[0]:\n",
    "\n",
    "            gloss = lem_sp[1].rstrip('].')\n",
    "\n",
    "\n",
    "            if 'cardinal' in lem_sp[0] and 'cardinal' not in tok:\n",
    "\n",
    "                lem_sp[0]=tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic':phonetic if phonetic else '_'\n",
    "                })\n",
    "            if \"cardinaux\" in lem_sp[0] :\n",
    "                lem_sp[0] = tok\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': tok,\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic':phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "            elif 'pronom in' in lem:\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"me\",lem_sp[0]) if \"moi\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"te\", lem_sp[0]) if \"toi\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"eñ\", lem_sp[0]) if \"lui\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"hi\", lem_sp[0]) if \"elle\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"hon\", lem_sp[0]) if \"nous\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"hoc'h-unan\", lem_sp[0]) if \"vous\" in gloss else ''\n",
    "                lem_sp[0] = re.sub(\"\\[\\[pronom incorporé\", \"int\", lem_sp[0]) if \"eux\" in gloss else ''\n",
    "\n",
    "\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0],\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic':phonetic if phonetic else '_'\n",
    "                })\n",
    "            else:\n",
    "                gloss = lem_sp[1].rstrip('].')\n",
    "                tok_objects.append({\n",
    "                    'tok': tok,\n",
    "                    'lemma': lem_sp[0].strip('[]'),\n",
    "                    'gloss': gloss,\n",
    "                    'phonetic':phonetic if phonetic else '_'\n",
    "                })\n",
    "\n",
    "\n",
    "        elif len(tok_list)==len(lem_parts)-1:\n",
    "            for i in range(len(tok_list)):\n",
    "                lem_inside=lem_parts[i].split('|')[0]\n",
    "                gloss_inside=lem_parts[i].split('|')[-1]\n",
    "\n",
    "                if 'numéraux' in lem_inside:\n",
    "                    lem_inside=\"[[\"+tok\n",
    "\n",
    "                if \"cardinaux\" in lem_inside:\n",
    "                    lem_inside=\"[[\"+tok\n",
    "\n",
    "\n",
    "                if '-ig' in lem_inside:\n",
    "                    gloss_inside='petit'\n",
    "\n",
    "                if 'cardinal' in lem_inside and 'cardinal' not in tok:\n",
    "                    lem_inside=tok\n",
    "                    pattern = r\"[^\\w\\s]+\"\n",
    "                    lem_inside=re.sub(pattern,\"\",lem_inside)\n",
    "                    lem_inside='[['+lem_inside\n",
    "\n",
    "                if 'pronom incorporé' in lem_inside:\n",
    "                    lem_inside=tok\n",
    "\n",
    "                if len(tok_list) == len(phon_list):\n",
    "                    tok_objects.append({'tok':tok_list[i],\n",
    "                                        'lemma': lem_inside[2:],\n",
    "                                        'gloss': gloss_inside,\n",
    "                                        'phonetic':phon_list[i] if phon_list[i] else 'None'\n",
    "\n",
    "                    })\n",
    "                else:\n",
    "                    tok_objects.append({'tok': tok_list[i],\n",
    "                                        'lemma': lem_inside[2:],\n",
    "                                        'gloss': gloss_inside\n",
    "\n",
    "                                        })\n",
    "                    if phonetic != 'None':\n",
    "                        with open('error_phon.txt','a')as fi:\n",
    "                            fi.write(f'text\\n{text}\\n')\n",
    "                            fi.write(f'phonetic\\n{phon}\\n')\n",
    "                            fi.write((f'______________\\n'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            tok_objects.append({\n",
    "                'tok': tok,\n",
    "                'lemma': '_',\n",
    "                'gloss': '_',\n",
    "                'phonetic':phonetic if phonetic else '_',\n",
    "                'multiple': len(lem_parts) - 1})\n",
    "            tok_obj = {'tok': tok, 'lemma': '_', 'gloss': '_','multiple':len(lem_parts)-1}\n",
    "            tok_n = 1\n",
    "\n",
    "            for j in range(len(lem_parts)):\n",
    "                if '[[' in lem_parts[j]:\n",
    "                    lemma_gloss = lem_parts[j].strip('[[').split('|')\n",
    "                    lemma = lemma_gloss[0].split('[[')[-1].strip()\n",
    "\n",
    "                    if re.search(r'^-\\w+', lemma):\n",
    "                        if lemma !='-ig':\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 and tok != '-ig' else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': lemma.strip(),\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': 'petit'\n",
    "                            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    elif lemma_gloss == ['R', '_']:\n",
    "                        if k !='+C':\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                            })\n",
    "\n",
    "                        else:\n",
    "                            tok_objects.append({\n",
    "                                'tok': k,\n",
    "                                'lemma': lemma.strip(),\n",
    "                                'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else '',\n",
    "                                                      'Epenthesis':'Yes'\n",
    "                            })\n",
    "\n",
    "                    elif lemma_gloss == ['a', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '1',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    elif lemma_gloss == ['e', '_']:\n",
    "                        tok_objects.append({\n",
    "                            'tok': '4',\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''\n",
    "                        })\n",
    "\n",
    "                    elif 'cardinal' in lemma_gloss[0]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma_gloss[1],\n",
    "                            'lemma': lemma_gloss[1],\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardinaux\" in lemma_gloss[0] and 'cent' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'kant',\n",
    "                            'lemma': 'kant',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif 'cardi' in lemma_gloss[0] and '9' in lemma_gloss[1]:\n",
    "\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'nav',\n",
    "                            'lemma': 'nav',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'trois' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'vingt' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'ugent',\n",
    "                            'lemma': 'ugent',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '2' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'douze' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daouzek',\n",
    "                            'lemma': 'daouzek',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'deux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'daou',\n",
    "                            'lemma': 'daou',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "                    elif \"cardi\" in lemma_gloss[0] and '3' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'tri',\n",
    "                            'lemma': 'tri',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"cardi\" in lemma_gloss[0] and 'un' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'unan',\n",
    "                            'lemma': 'unan',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'lui' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'eñ',\n",
    "                            'lemma': 'eñ',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'moi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'me',\n",
    "                            'lemma': 'me',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'toi' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'te',\n",
    "                            'lemma': 'te',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'nous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hon',\n",
    "                            'lemma': 'hon',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'elle' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': 'hi',\n",
    "                            'lemma': 'hi',\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'vous' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"hoc'h-unan\",\n",
    "                            'lemma': \"hoc'h-unan\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'eux' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"int\",\n",
    "                            'lemma': \"int\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'il' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"enni\",\n",
    "                            'lemma': \"enni\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "                    elif \"pronom inco\" in lemma_gloss[0] and 'ça' in lemma_gloss[1]:\n",
    "                        tok_objects.append({\n",
    "                            'tok': \"se\",\n",
    "                            'lemma': \"se\",\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''})\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        tok_objects.append({\n",
    "                            'tok': lemma.strip(),\n",
    "                            'lemma': lemma.strip(),\n",
    "                            'gloss': lemma_gloss[1].strip() if len(lemma_gloss) > 1 else ''}\n",
    "                        )\n",
    "\n",
    "\n",
    "                        if '-ig' in tok_obj['tok'] and '-ig' in tok_obj['lemma']:\n",
    "                            tok_obj['gloss'] = 'petit'\n",
    "\n",
    "                        if 'numéraux' in tok_obj['lemma']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "\n",
    "                        if 'cardinal' in tok_obj['lemma'] and 'cardinal' not in tok_obj['tok']:\n",
    "                            tok_obj['lemma'] = tok_obj['tok']\n",
    "                            pattern = r\"[^\\w\\s]+\"\n",
    "                            tok_obj['lemma'] = re.sub(pattern, \"\", tok_obj['lemma'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    tok_n += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return tok_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "\n",
    "def get_wikicode(title):\n",
    "    wikicode=''\n",
    "    # on traite les redirections vers d'autres pages\n",
    "    if '#REDIRECTION' in pages.get(title, ''):             \n",
    "        #on suit la redirection\n",
    "        newtitle = relinks.search(pages[title]).group(1)                \n",
    "        newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "        if '_' in newtitle: \n",
    "            newtitle = newtitle.replace(\"_\", \" \")            \n",
    "        #et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "        if newtitle in pages:\n",
    "            wikicode = pages[newtitle]              \n",
    "        else: \n",
    "            newtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "            if newtitle in pages:\n",
    "                wikicode = pages[newtitle]\n",
    "            else:\n",
    "                newtitle = newtitle.split(',')[0]\n",
    "                newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                if newtitle in pages:\n",
    "                    wikicode = pages[newtitle]\n",
    "                else:\n",
    "                    wikicode='__currentPage:\\nstrange redirect to page that does not exist: '+newtitle\n",
    "                    print(wikicode+'\\n')\n",
    "\n",
    "    else:\n",
    "        wikicode = pages.get(title, '')  \n",
    "    return wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9163 pages\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "with open('Pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rePOS = re.compile(r'articles?|art|verbes?|auxiliaires?|copules?|adverbes?|complémenteurs?|conjonctions?|prépositions?|adjectifs?|noms?|particules verbales?|interjections?|postpositions?|déterminants?|quantifieurs?|pronoms?|noms? propres?|suffixe|interrogatifs?|préfixes?|modaux|pluriels?|indéfinis?|particules? de discours|finales?|exclamatifs?', re.IGNORECASE)\n",
    "\n",
    "lefffl = [li.strip() for li in open('lefff-2.1.txt').read().split('\\n') if li.strip() and li[0]!='#']\n",
    "lefff = {li.split('\\t')[0]:li.split('\\t')[2] for li in lefffl}\n",
    "# print(lefff)\n",
    "\n",
    "\n",
    "def add_pos(tok_obj):\n",
    "    if 'pos' in tok_obj and tok_obj['pos'] : return tok_obj # pos already there\n",
    "\n",
    "    # is the pos in the lemma?\n",
    "    if 'lemma' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['lemma'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    \n",
    "    # is the pos in the gloss?\n",
    "    if 'gloss' in tok_obj:\n",
    "        m = rePOS.search(tok_obj['gloss'])\n",
    "        if m:\n",
    "            tok_obj['pos'] = m.group(0)\n",
    "            return tok_obj\n",
    "\n",
    "    \n",
    "    # let's check the page for the lemma\n",
    "    wikicode = get_wikicode(str.capitalize(tok_obj.get('lemma',''))) # TODO: improve as this might fail sometimes\n",
    "    # print(wikicode[:222])\n",
    "    m = rePOS.search(wikicode)\n",
    "    if m:\n",
    "        tok_obj['pos'] = m.group(0)\n",
    "        return tok_obj\n",
    "    \n",
    "    # let's use the gloss to make an educated guess about the pos using lefff\n",
    "    if tok_obj.get('gloss','') in lefff:\n",
    "        tok_obj['pos'] = lefff[tok_obj.get('gloss','')]\n",
    "    \n",
    "    return tok_obj\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_conll_output(tok_objects, text, text_fr, sent_id):\n",
    "    conll_output = \"\"\n",
    "    conll_id = 1\n",
    "    inside_id = 1\n",
    "\n",
    "    for tok_obj in tok_objects:\n",
    "        if isinstance(tok_obj, dict) and len(tok_obj) == 1:\n",
    "            field_key, field_value = next(iter(tok_obj.items()))\n",
    "            conll_line = f\"{conll_id}\\t{field_key}\\t{field_value}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "        else:\n",
    "            if 'multiple' not in tok_obj:\n",
    "                if 'pos' in tok_obj and 'phonetic' in tok_obj :\n",
    "                    if tok_obj['phonetic'] != 'None':\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t{tok_obj['phonetic']}\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\n",
    "                    else:\n",
    "                        conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\n",
    "\n",
    "                    conll_id += 1\n",
    "                    inside_id = conll_id\n",
    "                else:\n",
    "                    conll_line = f\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t_\\t_\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\n",
    "                    conll_id += 1\n",
    "                    inside_id = conll_id\n",
    "            else:\n",
    "                conll_line = f\"{inside_id}-{inside_id + tok_obj['multiple'] - 1}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t_\\t{tok_obj['phonetic']}\\t_\\t_\\t_\\t_\\t_\\n\"\n",
    "\n",
    "            conll_line = re.sub(']]', ' ', conll_line)\n",
    "            conll_line = re.sub('\\[\\[kaout', 'kaout', conll_line)\n",
    "            conll_line = re.sub(\"'''\", '', conll_line)\n",
    "            conll_line = re.sub('\\[','',conll_line)\n",
    "\n",
    "        conll_output += conll_line.strip() + \"\\n\"\n",
    "\n",
    "    conll_output = f\"# sent_id = None__{sent_id}\\n\" + \\\n",
    "                   f\"# text = {text.replace('||', '')}\\n\" + \\\n",
    "                   f\"# text_fr = {text_fr}\\n\" + \\\n",
    "                   f\"# status = WIP\\n\" + \\\n",
    "                   conll_output + \"\\n\"\n",
    "\n",
    "    return conll_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22612/1811248548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tokens_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22612/1811248548.py\u001b[0m in \u001b[0;36mprocess_tokens_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mconll_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerate_conll_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0msent_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Increment sent_id for the next example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22612/2492288328.py\u001b[0m in \u001b[0;36mgenerate_conll_output\u001b[0;34m(tok_objects, text, text_fr, sent_id)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'multiple'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtok_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mconll_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{conll_id}\\t{tok_obj['tok']}\\t{tok_obj['lemma']}\\t{tok_obj['pos']}\\t_\\t_\\t_\\t_\\t_\\tgloss={tok_obj['gloss']}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mconll_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0minside_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconll_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pos'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_tokens_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    conll_output = \"\"\n",
    "    sent_id = 1  # Initialize sent_id to 1\n",
    "\n",
    "    with open(\"couple.txt\", \"r\") as couple_file:\n",
    "        couple_lines = couple_file.readlines()\n",
    "        couple_lines = [line.strip() for line in couple_lines]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        tok_objects = ast.literal_eval(line)\n",
    "        text = couple_lines[i * 4 + 1].replace(\"text\", \"\").strip()\n",
    "        text_fr = couple_lines[i * 4 + 3].replace(\"translation\", \"\").strip()\n",
    "        text = re.sub(\"'''\",'',text)\n",
    "\n",
    "        # Update the sent_id value for each example\n",
    "\n",
    "\n",
    "        conll_output += generate_conll_output(tok_objects, text, text_fr,sent_id)\n",
    "        sent_id += 1  # Increment sent_id for the next example\n",
    "\n",
    "    return conll_output\n",
    "\n",
    "\n",
    "output = process_tokens_file('nt.txt')\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
