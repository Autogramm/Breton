{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
   "metadata": {
    "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
    "tags": []
   },
   "source": [
    "# exporting the Breton example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6",
   "metadata": {
    "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6"
   },
   "outputs": [],
   "source": [
    "import datetime, tqdm, re, time, string\n",
    "!pip install mwclient\n",
    "!pip install mwparserfromhell\n",
    "!pip install mwcomposerfromhell\n",
    "from mwclient import Site\n",
    "#myparserfromhell : parser pour le code wiki\n",
    "import mwparserfromhell, mwcomposerfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f90d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "got all 9098 pages\n"
     ]
    }
   ],
   "source": [
    "print('connecting...')\n",
    "#site contenant tous les exemples\n",
    "site = Site('arbres.iker.cnrs.fr/', path='/')\n",
    "outf = open('problemPages.html','w')\n",
    "allpages = list(site.allpages())\n",
    "print('got all', len(allpages), 'pages')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
   "metadata": {
    "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
    "tags": []
   },
   "source": [
    "#### Si on veut charger toutes les pages du site: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6e0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9098/9098 [27:31<00:00,  5.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction done. it took 1651.94335603714 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "#dictionnaire contenant toutes les pages aspirées\n",
    "pages = {}\n",
    "#on peut ici modifier le nombre de pages à charger pour charger petit à petit ex : allpages[:3000] pour les 3000 premières pages\n",
    "# newlist=allpages\n",
    "for p in tqdm.tqdm(allpages):\n",
    "# for p in tqdm.tqdm(newlist):\n",
    "\t#si les pages ne sont pas de la documentation\n",
    "\tif not('/documentation' in p.name or '/docname' in p.name or 'Module:' in p.name):\n",
    "\t\t#on ajoute le nom de la page comme clé, son contenu comme valeur\n",
    "\t\tpages[p.name] = p.text() \n",
    "print('extraction done. it took', time.time()-t, 'seconds.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
   "metadata": {
    "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
    "tags": []
   },
   "source": [
    "#### Si on veut utiliser un fichier Pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d0685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour enregistrer les pages chargées dans un fichier Pickle: \n",
    "import pickle\n",
    "# une fois toutes les pages chargées, on le met dans un pickle\n",
    "with open(\"Pages.pickle\", \"wb\") as myFile:\n",
    "   pickle.dump(pages, myFile)\n",
    "with open(\"Pages.txt\", \"w\") as myFile:\n",
    "    for title,text in pages.items():\n",
    "        myFile.write(title+'\\n'+text+'\\n------------------------\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
    "outputId": "119a6b13-11d3-41cc-e22e-302399c2a06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9098 pages\n"
     ]
    }
   ],
   "source": [
    "# print(', '.join(pages.keys()))\n",
    "#on créé un fichier txt contenant tous les titres de pages\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dce06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35cca27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9098/9098 [00:00<00:00, 13668.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we got 14322 exemples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "\n",
    "exampleFile = open('examples.txt','w')\n",
    "allc=0\n",
    "for title in tqdm.tqdm(pages):\n",
    "    exampleFile.write(title+'\\n_____________________________\\n')\n",
    "    wikitext = pages[title]\n",
    "    counter = 0\n",
    "    for m in repretty.finditer(wikitext):\n",
    "        exampleFile.write(m.group(0)+'\\n________________\\n')\n",
    "        counter += 1\n",
    "    exampleFile.write('\\n'+str(counter)+' examples found in page '+title+'\\n================================\\n\\n\\n')\n",
    "    allc+=counter\n",
    "print(\"we got\", allc,\"exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b",
   "metadata": {
    "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exemple de tableau standard (4 lignes):\n",
    "\n",
    "{| class=\"prettytable\"\n",
    "|(1)|| N'eo || ket || dleet || koduiñ || hag || evañ || er || memes || amzer.\n",
    "|-\n",
    "||| [[ne]] [[COP|est]] || [[ket|pas]] || [[dleout|dû]] || conduire || [[&|et]] || [[evañ|boire]] || [[P.e|en]].[[art|le]] || [[memes|même]] || temps\n",
    "|-\n",
    "|||colspan=\"10\" | 'On ne doit pas boire et conduire en même temps /Il ne faut pas boire et conduire' \n",
    "|-\n",
    "|||||||||colspan=\"10\" | ''Lesneven/Kerlouan'', [[Y. M. (04/2016)]]\n",
    "|}\n",
    "\"\"\"\n",
    "#dictionnaire des dialectes à récupérer\n",
    "dialects = {\"léonard\" : [], \"cornouaillais\" : [], \"trégorrois\": [], \"vannetais\" : [], \"standard\" : [], \"inconnu\" : [],\n",
    "            \"breton central\" : []}\n",
    "references = {} # keeps track of the references of the sentences: ref:[sentence]\n",
    "sentences = {} # keeps the actual texts\n",
    "dic = {} # token to lemma, pos, gloss\n",
    "lispos = {}\n",
    "#set pour éviter les doublons\n",
    "unique_sentences = set()\n",
    "currentPage=''\n",
    "\n",
    "\n",
    "## OUVERTURE DES FICHIERS\n",
    "#fichier d'erreur\n",
    "errorout = open('breton.errors.txt','w', encoding='utf-8')\n",
    "bilan = open('bilan.txt','w', encoding='utf-8')\n",
    "#fichier contenant les tableaux qu'on ne veut pas traiter\n",
    "non_examples = open('non_examples.txt','w', encoding='utf-8')\n",
    "\n",
    "\n",
    "### EXPRESSIONS REGULIERES\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "rePOS = re.compile(r'verbes|auxiliaires|copules|adverbes|complémenteurs|conjonctions|prépositions|adjectifs|noms|particules verbales|interjections|postpositions|déterminants|quantifieurs|pronoms|noms propres|suffixe|interrogatifs|préfixes|modaux|pluriels|indéfinis|particules de discours|finales|exclamatifs', re.IGNORECASE)\n",
    "reDialect = re.compile(r'léonard|cornouaillais|trégorrois|vannetais|breton central|standard', re.IGNORECASE)\n",
    "reLanguage = re.compile(r\"basque|français de Basse-Bretagne|franco-breton|gaulois|gallo|tohono o'odham|chalcatongo mixtec|tchèque|gallois|roumain|italien|espagnol|hébreu|arabe|français|anglais|allemand|moyen breton|vieux breton|breton pré-moderne\", re.IGNORECASE)\n",
    "reType = re.compile(r'ouvrages de recherche|références de corpus|élicitations|ouvrages pédagogiques|dictionnaires|grammaires', re.IGNORECASE)\n",
    "reNoms = re.compile(r'Yann|Marijo|Anna|Angela|Alejandro|Odile|Anastazi|Nolwenn|Solange|Bastien|Soazig|Marie|Marsel|Lukaz|Lenaig|Kristof|Julie|Jenovefa|Simone|Mona|Mari', re.IGNORECASE)\n",
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "relinks2 = re.compile(r\"\\[\\[.*\\]\\]\") \n",
    "reAlternatives = re.compile(r'\\([^\\W\\d_].* */ *[^\\W\\d_].*\\)', re.U)\n",
    "reWord = re.compile(r'\\[\\[[a-z]+')\n",
    "nospace2 = re.compile(r\".*(-.*\\(.*)\")\n",
    "nospace3 = re.compile(r\".*(.*-,.*-).*\")\n",
    "\n",
    "\n",
    "#éléments de la glose dont on veut supprimer les espaces (pour éviter de splitter sur les espaces)\n",
    "nospace = {\"numéraux cardinaux\": \"numéraux_cardinaux\",\"nom propre\": \"nom_propre\" , \"pronom incorporé\": \"pronom_incorporé\" ,\n",
    "\"nom de titre\": \"nom_de_titre\", 'pronom réfléchi': \"pronom_réfléchi\", \n",
    "\"nom nu\": \"nom_nu\", \"pluriel interne\": \"pluriel_interne\", \"Les pronoms d'@incise contrastifs\": \"Les_pronoms_d'@incise_contrastifs\", \n",
    "\"particule o\": \"particule_o\", \"auxiliaire ober\": \"auxiliaire_ober\" , \"IMP on\": \"IMP_on\",\n",
    "\"bezañ préverbal\": \"bezañ_préverbal\", \"Nep X\":  \"Nep_X\", \"Objet postverbal d'@un infinitif\": \"Objet_postverbal_d'un_infinitif\", \n",
    "\"Objet postverbal avec le verbe '@avoir\": \"Objet_postverbal_avec_le_verbe_'avoir\", \n",
    "\"Indéfini de choix libre par reduplication\": \"Indéfini_de_choix_libre_par_reduplication\", \n",
    "\"châtaigne électrique\": \"châtaigne_électrique\", \"numéral ordinal\": \"numéral_ordinal\",\n",
    "\"avant de\": \"avant_de\", \"Pronom réfléchi\": \"Pronom_réfléchi\", \"Gwazh, gwashañ\": \"Gwazh,_gwashañ\",\n",
    "\"jours de la semaine\" : \"jours_de_la_semaine\", \"jour de la semaine\": \"jour_de_la_semaine\" } \n",
    "\n",
    "#catégories renseignées entre parenthèses dans la glose. Ex: -et(Adj.)\n",
    "categories = {\"V\": \"verbe\", \"Adj\": \"adjectif\", \"PL\": \"suffixe\", \"N\": \"nom\", \"Adv\": \"adverbe\"}\n",
    "# dictionnaire de correspondance pour trouver les dialectes dans les lignes source: Léon --> Léonard \n",
    "regions = {\"Léon\": \"Léonard\", \"Cornouailles\": \"Cornouaillais\", \"Vannes\": \"Vannetais\", \"Trégor\": \"Trégorrois\" }\n",
    "# regex pour supprimer les espaces dans les interjections dans la glose\n",
    "interjections = re.compile(\"Eh bien quoi|Ma parole|eh bien\", re.IGNORECASE)\n",
    "# Rannig + verbe avoir à ne pas séparer \n",
    "one_token = [\"am-oa\",\"he-devoa\", \"en em\", \"on-oa\", \"em-eus\", \"am-eus\", \"e-nije\", \"e-nevoa\", \"he-doa\", \n",
    "             \"en ur\", \"'m eus\", \"em-bije\", \"en-deus\", \"o-deus\", \"em-bije\", \"em-oa\", \"am-bez\", \"am-bo\", \"o-devoa\", \"o-devez\", \"o-deveze\",\n",
    "\t\t\t\t\t\t\"o-doa\"]\n",
    "\n",
    "\t\t\t\t\t\t\n",
    "#compteurs: nombre de tableaux trouvés, nombre de tableaux dans le fichier d'erreur, nombre de tableaux à ne pas traiter, \n",
    "#nombre de tableaux traités et nombre de doublons\n",
    "total_example = 0\n",
    "error_count = 0\n",
    "other_table = 0\n",
    "converted_tables = 0\n",
    "doublon = 0\n",
    "\n",
    "\n",
    "\n",
    "# getFirstPos: RECHERCHE DE POS DANS UNE PAGE TITRE \n",
    "# la fonction cherche le pos dans toutes les pages selon le lemme donné et retourne le pos associé\n",
    "# si aucun pos n'est trouvé, on met un point d'interrogation\n",
    "def getFirstPos(title, gloss, currentPage):\n",
    "\tmpos=''  \n",
    "\tif not title: \n",
    "\t\treturn '?' \n",
    "\telif title.isupper() and title != 'R':\n",
    "\t\tmpos = title  \n",
    "\t# si on cherche la Page \"a\", on aura une catégorie erronée (préposition au lieu de particule verbale) \n",
    "\telif title == 'a' or title == 'e' and 'R' in gloss :\n",
    "\t\tmpos='particule_verbale'\n",
    "\telif title == 'P.e' or title == 'P/e':     \n",
    "\t\tmpos='préposition'\n",
    "\t# on vérifie si la catégorie est présente dans la glose\n",
    "\telif re.match(r\".*\\((.*\\.)\\)\", title) and re.match(r\".*\\((.*\\.)\\)\", title).group(1) in categories:                    \n",
    "\t\ttest_pos = re.match(r\".*\\((.*\\.)\\)\", title).group(1)                   \n",
    "\t\tif test_pos.replace(\".\", \"\") in categories:                       \n",
    "\t\t\tmpos = categories[test_pos.replace(\".\", \"\")]\t\t\t     \n",
    "\telse:        \n",
    "\t#si le titre contient un tiret bas, on le remplace par un espace pour le trouver dans les titres de pages\n",
    "\t\tif '_' in title: \n",
    "\t\t\ttitle = title.replace(\"_\", \" \")\n",
    "\t\tif '#' in title: \n",
    "\t\t\ttitle = title.replace(\"#\", \"-\")            \n",
    "#sinon on met une majuscule à la première lettre du titre\n",
    "\t\telse:             \n",
    "\t\t\ttitle = title[0].upper()+title[1:]\n",
    "\t#si le titre est contenu dans le dictionnaire pages\n",
    "\t\tif title in pages:           \n",
    "\t\t\twikicode=''\n",
    "\t\t\t# on traite les redirections vers d'autres pages\n",
    "\t\t\tif '#REDIRECTION' in pages[title]:             \n",
    "\t\t\t\t#on suit la redirection\n",
    "\t\t\t\tnewtitle = relinks.search(pages[title]).group(1)                \n",
    "\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\tif '_' in newtitle: \n",
    "\t\t\t\t\tnewtitle = newtitle.replace(\"_\", \" \")            \n",
    "\t\t\t\t#et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\twikicode = pages[newtitle]              \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnewtitle = newtitle.split(',')[0]\n",
    "\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\terrorout.write('__currentPage:\\nstrange redirect to page that does not exist: '+newtitle+'\\n')\n",
    "\t\t\t\t\t\t\terror_count += 1                            \n",
    "\t\t\telse:\n",
    "\t\t\t\twikicode = pages[title]   \n",
    "\t\t\t# on cherche les catégories renseignées sur la page\n",
    "\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "\t\t\t\tcats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)          \n",
    "\t\t\t\tlist_cats = set()\n",
    "\t\t\t\tfor cat in cats:   \n",
    "\t\t\t\t\t# on cherche les POS dans les catégories          \n",
    "\t\t\t\t\tif rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "\t\t\t\t\t\tlist_cats.add(rePOS.search(cat).group(0).rstrip('s'))                    \n",
    "\t\t\t\t\t\tif len(list_cats) == 1:                                          \n",
    "\t\t\t\t\t\t\tmpos = list(list_cats)[0]                         \n",
    "\t\t\t\t\t\telif len(list_cats) >0: \n",
    "\t\t\t\t\t\t\tmpos = \"/\".join(list_cats)    \n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\tmpos=\"?\"                              \n",
    "\t\t\tif mpos:              \n",
    "\t\t\t\tmpos = mpos\n",
    "\t\t\telse:\n",
    "\t\t\t\tmpos = \"?\"\n",
    "\t\telse:          \n",
    "\t\t\tif reNoms.search(title):             \n",
    "\t\t\t\tmpos = \"nom_propre\"\n",
    "\t\t\t# lorsqu'on se trouve sur la page du mot, le mot est entourés de trois guillemets (remplacés par %)\n",
    "\t\t\t# on cherche les POS sur la page où on se trouve\n",
    "\t\t\telif \"%\" in title:             \n",
    "\t\t\t\twikicode = pages[currentPage]                               \n",
    "\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "\t\t\t\t\tcats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)\n",
    "\t\t\t\t\tlist_cats = set()\n",
    "\t\t\t\t\tfor cat in cats:\n",
    "\t\t\t\t\t\tif cat not in lispos: \n",
    "\t\t\t\t\t\t\tlispos[cat] = 0\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tlispos[cat] += 1                            \n",
    "\t\t\t\t\t\tif rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "\t\t\t\t\t\t\tlist_cats.add(rePOS.search(cat).group(0).rstrip('s'))\n",
    "\t\t\t\t\t\t\tif len(list_cats) == 1:                                          \n",
    "\t\t\t\t\t\t\t\tmpos = list(list_cats)[0]\n",
    "\t\t\t\t\t\t\telif len(list_cats) >0: \n",
    "\t\t\t\t\t\t\t\tmpos = \"/\".join(list_cats)                               \n",
    "\t\t\telse:               \n",
    "\t\t\t\tmpos = \"?\"              \n",
    "\treturn mpos\n",
    "\n",
    "\n",
    "# Suppression des espaces dans les [[ ]] sinon, le split avec espace comme délimiteur espace va mal se passer...\n",
    "# case : [[koll|perd]].[[-et (Adj.)|u]]\n",
    "def sansespace(ch):\n",
    "\tfor e in relinks.finditer(ch):\n",
    "\t\tif \" \" in e.group(0):\n",
    "\t\t\tstart, end = e.start(), e.end()\n",
    "\t\t\tnewch = re.sub(r\"\\[\\[(.*?) (.*?)\\]\\]\", \"[[\\\\1_\\\\2]]\", e.group(0))\n",
    "\t\t\tnewch = newch.replace(' ', '_') \n",
    "\t\t\tch = ch[:start] + newch + ch[start+len(newch)+1:]\n",
    "\treturn ch\n",
    "#exemple :\n",
    "#print(sansespace(\"[[art|le]] _[[1]]_[[hey j]]_[[maouez | femme]]\"))\n",
    "#resultat : \"[[art|le]] _[[1]]_[[heyj]]_[[maouez| femme]]\")\n",
    "#print(sansespace(\"[[R]] [[+C]] [[ez eus|est]]\"))\n",
    "\n",
    "\n",
    "## REGEX POUR LES MUTATIONS ET CONSONNES EPENTHETIQUES\n",
    "mutation1 = re.compile(r\"_\\[\\[([12345])\\]\\]_\\[\\[(.*)\\]\\]\")\n",
    "mutation2 = re.compile(r\"\\[?\\[?(.*)\\]?\\]?_\\[\\[([12345])\\]\\]_\")\n",
    "\n",
    "#ex: \"[[ne]]_[[+C]]_\"\n",
    "epenthetique1 = re.compile(r\"\\[\\[(.*)\\]\\]_,?/?(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[ma|que]]_[[4]],+C_\n",
    "epenthetique2 = re.compile(r\"\\[\\[(.*)\\]\\]_\\[\\[([12345])\\]\\],?/?(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[R]]_[[+C]],[[4]]_  \n",
    "epenthetique3 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?),\\[\\[([12345])\\]\\]_\")\n",
    "#ex : [[R]]_[[+C]]_[[ez eus|est]]\n",
    "#epenthetique4 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?)_[\\[(.*)\\]\\]\") \n",
    "mutation = False\n",
    "epenthetique = False\n",
    "\n",
    "groupes_pipe = re.compile(r\"(.*?)\\|(.*)\")\n",
    "\n",
    "\n",
    "\n",
    "# tokentrans2lemposglossmorph: FONCTION QUI PREND UN TOKEN ET RETOURNE LEMME, POS, GLOSE\n",
    "# NB : tr = traduction ; t = token\n",
    "\n",
    "def tokentrans2lemposglossmorph(tr, t, title, currentPage):\n",
    "\tglobal error_count\n",
    "\t# print(f't:{t}, tr:{tr}')         \n",
    "\tlem, pos, gloss, morph = '', '', '', ''\n",
    "\tm = relinks.search(tr)     \n",
    "\ttest_mutation1 = mutation1.search(tr)\n",
    "\ttest_mutation2 = mutation2.search(tr)\n",
    "\ttest_epenthetique1 = epenthetique1.search(tr)  \n",
    "\ttest_epenthetique2 = epenthetique2.search(tr)  \n",
    "\ttest_epenthetique3 = epenthetique3.search(tr)\n",
    "    \n",
    "\n",
    "\tif test_mutation1 :     \n",
    "\t\tmutation = True        \n",
    "\t\tif ('.[[' or ']].' or '-[[' or ']]-' ) in tr:      \n",
    "\t\t\tlems, glosss, poss, ponct = [], [], [], [] \n",
    "\t\t\tlemt, glosst, post = [], [], []\n",
    "\t\t\tif '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "\t\t\t\ttr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')                 \n",
    "\t\t\t#s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "\t\t\t#ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]]             \n",
    "\t\t\tif re.search(r\"\\[ ?- ?[A-Za-z]\", tr):               \n",
    "\t\t\t\tsuffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "\t\t\t\tsuffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "\t\t\t\ttr = tr.replace(suffixe, suffixe_transforme)             \n",
    "\t\t\ttr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "\t\t\tfor i in range(len(tr.split())):         \n",
    "\t\t\t\ttest = tr.split()[i]           \n",
    "\t\t\t\tif relinks2.search(test):                   \n",
    "\t\t\t\t\tele = relinks2.search(test).group(0)                                        \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\telse:                                  \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "\t\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t#on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "\t\t\t\tif tr.split()[i].startswith(\".\"):\n",
    "\t\t\t\t\tlemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\".{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\".{poss[i]}\")  \n",
    "\t\t\t\telif tr.split()[i].startswith(\"-\"): \n",
    "\t\t\t\t\tlemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\"-{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\"-{poss[i]}\")  \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tlemt.append(lems[i].replace('#', '-')) \n",
    "\t\t\t\t\tglosst.append(glosss[i])                    \n",
    "\t\t\t\t\tpost.append(poss[i]) \n",
    "        \n",
    "\t\telse:         \n",
    "\t\t\tmorph = test_mutation1.group(1) + \" \" + \"target\"\n",
    "\t\t\ttoken_mute = test_mutation1.group(2)       \n",
    "\t\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\t\tif token_mute_pipe: \n",
    "\t\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\t\tif gloss == 'R':  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:   \n",
    "\t\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)\n",
    "\t\t\telse :\n",
    "\t\t\t\tlem = t.lower()        \n",
    "\t\t\t\tgloss = token_mute   \n",
    "\t\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:                          \n",
    "\t\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\t\tif pos == '?':\n",
    "\t\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)\n",
    "                    \n",
    "                    \n",
    "\telif test_mutation2 :     \n",
    "\t\tmutation = True        \n",
    "\t\tif '.[[' or ']].' or '-[[' or ']]-' in tr:            \n",
    "\t\t\tlems, glosss, poss, ponct = [], [], [], [] \n",
    "\t\t\tlemt, glosst, post = [], [], []\n",
    "\t\t\tif '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "\t\t\t\ttr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')    \n",
    "\t\t\t#s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "\t\t\t#ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]]             \n",
    "\t\t\tif re.search(r\"\\[ ?- ?[A-Za-z]\", tr):               \n",
    "\t\t\t\tsuffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "\t\t\t\tsuffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "\t\t\t\ttr = tr.replace(suffixe, suffixe_transforme)             \n",
    "\t\t\ttr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "\t\t\tfor i in range(len(tr.split())):                \n",
    "\t\t\t\ttest = tr.split()[i]  \n",
    "\t\t\t\tif relinks2.search(test):                 \n",
    "\t\t\t\t\tele = relinks2.search(test).group(0)                                        \n",
    "\t\t\t\t\ttry: \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "\t\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\t\texcept:                                   \n",
    "\t\t\t\t\t\t\terrorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\telse:                                  \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "\t\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\") \n",
    "\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t#on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "\t\t\t\tif tr.split()[i].startswith(\".\"): \n",
    "\t\t\t\t\tlemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\".{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\".{poss[i]}\")  \n",
    "\t\t\t\telif tr.split()[i].startswith(\"-\"): \n",
    "\t\t\t\t\tlemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\"-{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\"-{poss[i]}\")  \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\ttry:                     \n",
    "\t\t\t\t\t\tlemt.append(lems[i].replace('#', '-')) \n",
    "\t\t\t\t\t\tglosst.append(glosss[i])                    \n",
    "\t\t\t\t\t\tpost.append(poss[i])\n",
    "\t\t\t\t\texcept:                        \n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}:\\nErreur dans les morphèmes du mot suivant : {tr} et {t}\")\n",
    "\t\t\t\t\t\terror_count += 1\n",
    "        \n",
    "\t\telse:              \n",
    "\t\t\tmorph = test_mutation2.group(2) + \" \" + \"trigger\"\n",
    "\t\t\ttoken_mute = test_mutation2.group(1)       \n",
    "\t\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)          \n",
    "\t\t\tif token_mute_pipe:      \n",
    "\t\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:                \n",
    "\t\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)\n",
    "\t\t\telse :\n",
    "\t\t\t\tlem = t.lower() \n",
    "\t\t\t\tgloss = token_mute\n",
    "\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\t\tif pos == '?':\n",
    "\t\t\t\t\tpos = getFirstPos(tr, gloss,  currentPage)\n",
    "\n",
    "\n",
    "#S'il y a une consonne épenthétique (+C)             \n",
    "\telif test_epenthetique1 : \n",
    "\t\tepenthetique = True    \n",
    "\t\tmorph = test_epenthetique1.group(2) \n",
    "\t\ttoken_mute = test_epenthetique1.group(1)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)\n",
    "\t\telse :\n",
    "\t\t\tlem = t.lower() \n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)\n",
    "\n",
    "\telif test_epenthetique2 :\n",
    "\t\tepenthetique = True    \n",
    "\t\tmorph = test_epenthetique2.group(2) + '|' +  test_epenthetique2.group(3) \n",
    "\t\ttoken_mute = test_epenthetique2.group(1)    \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)\n",
    "\t\telse :\n",
    "\t\t\tlem = t.lower() \n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)\n",
    "                             \n",
    "\telif test_epenthetique3:\n",
    "\t\tepenthetique = True \n",
    "\t\tmorph = test_epenthetique3.group(2) + '|' +  test_epenthetique3.group(3)\n",
    "\t\ttoken_mute = test_epenthetique3.group(1)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)\n",
    "\t\telse :\n",
    "\t\t\tlem = t.lower() \n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)                    \n",
    "                               \n",
    "# si ce n'est pas une mutation et si on a des double crochets [[ ]]              \n",
    "\telif m:     \n",
    "\t\t# cas \"amalgames\" et/ou mots contenant un tiret: \n",
    "\t\t#exemple [[da|à]].[[pronom incorporé|vous]] ou [[den|parent]].[[pluriel interne|s]]-[[kozh|vieil]]  \n",
    "\t\tif '.[[' in tr or ']].' in tr or '-[['in tr or ']]-' in tr: \n",
    "\t\t\tlems, glosss, poss, ponct = [], [], [], [] \n",
    "\t\t\tlemt, glosst, post = [], [], []\n",
    "\t\t\tif '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "\t\t\t\ttr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')      \n",
    "\t\t\t#s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "\t\t\t#ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]] \n",
    "\t\t\tif re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr):               \n",
    "\t\t\t\tsuffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "\t\t\t\tsuffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "\t\t\t\ttr = tr.replace(suffixe, suffixe_transforme)             \n",
    "\t\t\ttr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "\t\t\tfor i in range(len(tr.split())):         \n",
    "\t\t\t\ttest = tr.split()[i]                 \n",
    "\t\t\t\tif relinks2.search(test):                 \n",
    "\t\t\t\t\tele = relinks2.search(test).group(0)                                        \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\telse:    \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "\t\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\t\tposs += [getFirstPos(lem, gloss, currentPage)]\n",
    "\t\t\t\t\texcept:              \n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "\t\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t#on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "\t\t\t\tif tr.split()[i].startswith(\".\"): \n",
    "\t\t\t\t\tlemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\".{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\".{poss[i]}\")  \n",
    "\t\t\t\telif tr.split()[i].startswith(\"-\"): \n",
    "\t\t\t\t\tlemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\"-{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\"-{poss[i]}\")  \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tlemt.append(lems[i].replace('#', '-')) \n",
    "\t\t\t\t\tglosst.append(glosss[i])                    \n",
    "\t\t\t\t\tpost.append(poss[i])                                        \n",
    "\t\t\tlem = ''.join(lemt)\n",
    "\t\t\tgloss = ''.join(glosst)\n",
    "\t\t\tpos = ''.join(post)             \n",
    "\t\t\treturn lem, pos, gloss, morph        \n",
    "        \n",
    "        \n",
    "        \n",
    "\t\t# cas \"normal\" : pas d'amalgame, un token et une gloss --> [[token|gloss]]\n",
    "\t\tif '|' in m.group(1):          \n",
    "\t\t\tif \"numéraux_cardinaux\" in m.group(1): \n",
    "\t\t\t\tlem = t.lower() \n",
    "\t\t\t\tpos = m.group(1).split('|')[0]\n",
    "\t\t\t\tgloss = m.group(1).split('|')[1] \n",
    "\t\t\telse:                 \n",
    "\t\t\t\tlem = m.group(1).split('|')[0]\n",
    "\t\t\t\tgloss = m.group(1).split('|')[1]                  \n",
    "\t\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\"                      \n",
    "\t\t\t\telif lem == 'P.e' or lem == 'P/e':  \n",
    "\t\t\t\t\tlem = \"e\"\n",
    "\t\t\t\t\tpos = \"préposition\"           \n",
    "\t\t\t\telif lem == 'nom_propre':  \n",
    "\t\t\t\t\tpos = lem\n",
    "\t\t\t\t\tlem = gloss                 \n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpos = getFirstPos(lem, gloss, currentPage)                        \n",
    "\t\t\t\tif pos=='?':\n",
    "\t\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "                \n",
    "\t\t# cas sans amalgame mais sans gloss --> [[ne]]\n",
    "\t\telse:           \n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = relinks.search(tr).group(1)\n",
    "\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"\n",
    "\t\t\tif 'DIM' in gloss and t.endswith('ig') or t.endswith('ig%'):\n",
    "\t\t\t\tlem = \"-ig\"                \n",
    "\t\t\t\tpos = \"suffixe\"                \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)      \n",
    "\t# cas d'un mot seul sans crochets --> foot dans la page \"Abardaez,_enderv\"      \n",
    "\telse :\n",
    "\t\tif \"%\" in t and t.replace(\"%\", \"\").startswith(currentPage.lower()): \n",
    "\t\t\tlem = currentPage.lower()        \n",
    "\t\telse: \n",
    "\t\t\tlem = t\n",
    "\t\tgloss = tr\n",
    "\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\tpos = \"particule_verbale\"            \n",
    "\t\telse:\n",
    "\t\t\tpos = getFirstPos(t, gloss, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, gloss, currentPage)\n",
    "\treturn lem, pos, gloss, morph\n",
    "\n",
    "\n",
    "# Nettoyage des tokens + remplacement de certains caractères par d'autres\n",
    "# pour éviter des erreurs quand on splitte\n",
    "def cleanToken(t):\n",
    "\tt = t.replace(\"...\", \"…\")   \n",
    "\tt = t.replace(\"tout.à.l'heure\", \"tout_à_l'heure\")    \n",
    "\tt = t.replace(\"'''\", \"%\")\n",
    "\tt = t.replace(\"''\", \"\")\n",
    "\tt = re.sub(r'</?font.*?>','',t)\n",
    "\tt = re.sub(r'<sup>','_',t)\n",
    "\tt = re.sub(r'</sup>','_',t)\n",
    "\tt = re.sub(r'<u>','',t)\n",
    "\tt = re.sub(r'</u>','',t)\n",
    "\tt = re.sub(r'<sub>.*?</sub>','',t)\n",
    "\tt = re.sub(r'\\(\\[\\[\\*\\]\\].*?\\)','$$$',t)\n",
    "\tt = re.sub(r\"c'h\",'cxxxh',t)\n",
    "\tt = re.sub(r\"C'h\",'Cxxxh',t)\n",
    "\tt = re.sub(r\"`\",'',t) \n",
    "\tt = re.sub(r\"#\",'-',t)   \n",
    "\t# pour spliter sur les apostrophes sans les enlever : mettre @ pour spliter sur @ et donc conserver l'apostrophe\n",
    "\tif not t.strip().startswith(\"'\"):   \n",
    "\t\tt = re.sub(r\"'\",\"'@\",t)\n",
    "\tt = re.sub(r\"bez'@\",\"bez'\",t) \n",
    "\tt = re.sub(r\"Bez'@\",\"B ez'\",t)     \n",
    "\tt = re.sub(r\"d'@ici\",\"d'ici\",t)   \n",
    "\tt = re.sub(\"aujourd'@hui\", \"aujourd'hui\", t)  \n",
    "\tt = re.sub(\"l'@heure\", \"l'heure\", t)\n",
    "\tt = re.sub(\"si.ce.n'@est\", \"si.ce.n'est\", t)   \n",
    "\tt = re.sub(\"aujourd'@hui\", \"aujourd'hui\", t)      \n",
    "\treturn t.strip()\n",
    "\n",
    "# Remettre les tokens qui ont été remplacés\n",
    "def remettretoken(t):\n",
    "\tt = re.sub(r\"%\",\"\",t)\n",
    "\tt = re.sub(r'cxxxh',\"c'h\",t)\n",
    "\tt = re.sub(r'Cxxxh',\"C'h\",t)\n",
    "\tt = re.sub(r\"'@\", \"'\",t)\n",
    "\tt = re.sub(r\"\\$\\$\\$ \", \"\",t)\n",
    "\tt = re.sub(r\"\\[\\[\", \"\",t)   \n",
    "\tt = re.sub(r\"\\]\\]\", \"\",t) \n",
    "\tt = t.replace(\"..\", \".\")     \n",
    "\tt = t.replace(\"e'@idin\", \"e'idin\")          \n",
    "\treturn t\n",
    "\n",
    "# Savoir si une string contient un nombre\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "## analyzePage: FONCTION PRINCIPALE\n",
    "def analyzePage(title): \n",
    "\tglobal total_example, error_count, other_table, converted_tables, problem\n",
    "\tcurrentPage = title\n",
    "\tmpos = rePOS.search(pages[title])\n",
    "\tpagepos = None\n",
    "\tconlls=[]\n",
    "\tgrammatical = True\n",
    "\tcomparison = False  \n",
    "\texample = True\n",
    "\ts= ''\n",
    "\tif mpos:\n",
    "\t\t# met tout en minuscule\n",
    "\t\tpagepos = mpos.group(0).lower()\n",
    "\t#pour chaque tableau (et donc chaque exemple)\n",
    "\tnb_tables = 0    \n",
    "\tfor m in repretty.finditer(pages[title]):\n",
    "\t\ttotal_example += 1        \n",
    "\t\tprettytable = m.group(1)        \n",
    "\t\tif \"font color=white\" in prettytable: \n",
    "\t\t\texample = False\n",
    "\t\t\tnon_examples.write(\"__\"+currentPage+\"\\nAutre type de tableau:\\n\"+ prettytable + \"\\n\\n\")            \n",
    "\t\t\tbreak      \n",
    "\t\tnb_tables += 1        \n",
    "\t\ttokens, trans = [], []\n",
    "\t\ttranslation, source, source2 = '', '', ''\n",
    "\t\tlocation, dialect, phonetic, texttype, l = '','','', '', ''\n",
    "\t\tgrammatical=True         \n",
    "\n",
    "\t\t# si le tableau est \"standard\" avec une ligne de token, une ligne de glose et une ligne source (9 lignes dans le texte wiki)\n",
    "\t\tif (len(m.group(1).split('\\n'))) == 9 :\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table              \n",
    "\t\t\t\tif li[:2]=='|(' :                   \n",
    "\t\t\t\t\tif '|| ||' in li:\n",
    "\t\t\t\t\t\tli = li.replace('|| ||', '||')                       \n",
    "\t\t\t\t\tif '[[*]]' in li.split('||')[0]:\n",
    "\t\t\t\t\t\tgrammatical = False                     \n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]]  \n",
    "\t\t\t\t\tif \"vs.\" in tokens: \n",
    "\t\t\t\t\t\tcomparison = True                        \n",
    "\t\t\t\t\tif tokens[0]==\"[[*]]\" :\n",
    "\t\t\t\t\t\tgrammatical = False   \n",
    "\t\t\t\t\telif '[' in tokens[0] and not \"[…]\" in tokens[0]:                     \n",
    "\t\t\t\t\t\ttokens[0] = tokens[0].replace('[', '').strip()     \n",
    "\t\t\t\t\telif ']' in tokens[-1] and not '[…]' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "\t\t\t\t\tif '' in tokens: \n",
    "\t\t\t\t\t\twhile '' in tokens: \n",
    "\t\t\t\t\t\t\ttokens.remove('')         \n",
    "                                                                           \n",
    "\t\t\t\tif li[:3]=='|||' and grammatical:\n",
    "\t\t\t\t\tif '|| ||' in li:\n",
    "\t\t\t\t\t\tli = li.replace('|| ||', '||')\n",
    "\t\t\t\t\tif not trans: #la première fois = la traduction                      \n",
    "\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]                            \n",
    "\t\t\t\t\t\tif '' in trans: \n",
    "\t\t\t\t\t\t\twhile '' in trans: \n",
    "\t\t\t\t\t\t\t\ttrans.remove('')                                \n",
    "\t\t\t\t\t\tif \"Glose en [[KLT']\" in trans[-1]:\n",
    "\t\t\t\t\t\t\ttrans[-1] = trans[-1].replace(\"Glose en [[KLT']\", '').strip()       \n",
    "\t\t\t\t\t\tif 'Graphie peurunvan' in trans[-1]: \n",
    "\t\t\t\t\t\t\ttrans[-1] = trans[-1].replace('Graphie peurunvan', '').strip()                           \n",
    "\t\t\t\t\t\tfor i in range(len(trans)):                         \n",
    "\t\t\t\t\t\t\tfor ele in nospace:                                \n",
    "\t\t\t\t\t\t\t\tif ele in trans[i]: \n",
    "\t\t\t\t\t\t\t\t\ttrans[i]= trans[i].replace(ele, nospace[ele]) \n",
    "\t\t\t\t\t\t\tif re.search(nospace2, trans[i]):\n",
    "\t\t\t\t\t\t\t\tto_replace = re.search(nospace2, trans[i]).group(1)\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"_\")) \n",
    "\t\t\t\t\t\t\tif re.search(nospace3, trans[i]):\n",
    "\t\t\t\t\t\t\t\tto_replace = re.search(nospace3, trans[i]).group(1)\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"_\"))                                     \n",
    "\t\t\t\t\t\t\tif re.search(interjections, trans[i]):\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(re.search(interjections, trans[i]).group(0), re.search(interjections, trans[i]).group(0).replace(\" \", \"_\"))  \n",
    "\t\t\t\t\t\t\tif re.search(\"(.* !)\", trans[i]):\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(re.search(\"(.* !)\", trans[i]).group(0), re.search(\"(.* !)\", trans[i]).group(0).replace(\" \", \"\") )                                \n",
    "                                \n",
    "\t\t\t\t\telif 'colspan=' in li or '|||||||||||||||' in li:                  \n",
    "\t\t\t\t\t\t# vu que les deux lignes translation et source remplissent cette condition, quand on arrive à source\n",
    "\t\t\t\t\t\t# la string translation est remplie donc on n'écrase pas et on passe à source\n",
    "\t\t\t\t\t\tif not translation:\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\t\tfor i in range(len(trans)): \n",
    "\t\t\t\t\t\t\t\tif any(ele in trans[i] for ele in nospace):\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\" \", \"_\")     \n",
    "\t\t\t\t\t\t\t\tif \"jours de la semaine\" or \"jour de la semaine\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = sansespace(trans[i]) \n",
    "                                \n",
    "\t\t\t\t\t\telif not source: # si déjà trans, alors on a la source\n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','')\n",
    "\t\t\t\t\t\t\tif re.search(\"(\\'\\'.*\\'\\' ?,?).*\", source):                               \n",
    "\t\t\t\t\t\t\t\tother = re.search(\"(\\'\\'.*?\\'\\' ?,?).*\", source).group(1)                                 \n",
    "\t\t\t\t\t\t\t\tsource = source.replace(other, '').replace(\"''\",\"\")                               \n",
    "                            \n",
    "\t\t\t\t\t\t\tif reLanguage.search(source):                              \n",
    "\t\t\t\t\t\t\t\tgrammatical = False     \n",
    "\t\t\t\t\t\t\t\tbreak                                \n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", li):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", li):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif reDialect.search(li): \n",
    "\t\t\t\t\t\t\t\tdialect = reDialect.search(li)[0]                                \n",
    "\t\t\t\t\t\t\tif \":\" in source: \n",
    "\t\t\t\t\t\t\t\tl = re.search(\"(.*):.*\", source).group(1) + ')'                                   \n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tl = source\n",
    "\t\t\t\t\t\t\tif re.search(r\"(.*?)\\[\\[\", li):                            \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                                                                \n",
    "\t\t\t\t\t\t\tif l in pages:                                     \n",
    "\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                                 \n",
    "\t\t\t\t\t\t\t\t\t#on suit la redirection                                        \n",
    "\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)\n",
    "\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\ts = l                                    \n",
    "\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                   \n",
    "\t\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\t\tif reLanguage.search(ele):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")                                                \n",
    "\t\t\t\t\t\t\telse:                                    \n",
    "\t\t\t\t\t\t\t\ts = l\n",
    "\t\t\t\t\t\t\tif not dialect:                                           \n",
    "\t\t\t\t\t\t\t\tif reDialect.search(li):\n",
    "\t\t\t\t\t\t\t\t\tdialect = reDialect.search(li).group(0)\n",
    "\t\t\t\t\t\t\t\telse:                                                   \n",
    "\t\t\t\t\t\t\t\t\tfor region in regions:                                                 \n",
    "\t\t\t\t\t\t\t\t\t\tif region in li:\n",
    "\t\t\t\t\t\t\t\t\t\t\tdialect = regions[region]                                             \n",
    "\n",
    "\t\t\tif not( tokens and trans and translation and source) and grammatical is not False: # if one is missing                \n",
    "\t\t\t\tif not tokens: \n",
    "\t\t\t\t\terrorout.write('\\n'+currentPage+': Tokens line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t\tproblem = True                    \n",
    "\t\t\t\tif not trans: \n",
    "\t\t\t\t\terrorout.write('\\n'+currentPage+': Trans line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t\tproblem = True\n",
    "\t\t\t\tif not translation:\n",
    "\t\t\t\t\terrorout.write('\\n'+currentPage+': Translation line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\tproblem = True\n",
    "\t\t\t\tif not source:\n",
    "\t\t\t\t\terrorout.write('\\n'+currentPage+': Source line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t\tproblem = True\n",
    "\t\t\t\tcontinue\n",
    "                \n",
    "\t\t\tif grammatical is False: \n",
    "\t\t\t\tnon_examples.write(\"__\"+currentPage+\"\\nFound agrammatical sentence:\\n\"+ prettytable + \"\\n\\n\")\n",
    "\t\t\t\tother_table += 1 \n",
    "\t\t\t\tbreak  \n",
    "                \n",
    "\t\t\tif comparison is True: \n",
    "\t\t\t\tnon_examples.write(\"__\"+currentPage+\"\\nTableau de comparaison:\\n\"+ prettytable + \"\\n\\n\")\n",
    "\t\t\t\tother_table += 1 \n",
    "\t\t\t\tbreak                  \n",
    "                \n",
    "\n",
    "\t\t# S'il y a plus de lignes que prévu : par ex une ligne de phonétique ou deux lignes source, ou les deux\n",
    "\t\telse:\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table\n",
    "\t\t\t\t#si ce n'est pas une ligne de phonétique                \n",
    "\n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==False:\n",
    "\t\t\t\t\tif '[[*]]' in li.split('||')[0]:\n",
    "\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tif '|| ||' in li:\n",
    "\t\t\t\t\t\tli = li.replace('|| ||', '||')\n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]] \n",
    "\t\t\t\t\tif \"vs.\" in tokens: \n",
    "\t\t\t\t\t\tcomparison = True \n",
    "\t\t\t\t\tif tokens[0]==\"[[*]]\" :\n",
    "\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\tbreak      \n",
    "\t\t\t\t\tif '' in tokens: \n",
    "\t\t\t\t\t\twhile '' in tokens: \n",
    "\t\t\t\t\t\t\ttokens.remove('')  \n",
    "\t\t\t\t\tif 'Equivalent [[KLT' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace('Equivalent [[KLT', '').strip()\n",
    "\t\t\t\t\tif ']' in tokens[-1] and not '[…]' in tokens[-1] : \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "\t\t\t\t\tif '[' in tokens[0] and not '[…]' in tokens[0]: \n",
    "\t\t\t\t\t\ttokens[0] = tokens[0].replace('[', '').strip()                        \n",
    "\t\t\t\t#si c'est une ligne de phonétique\n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==True:\n",
    "\t\t\t\t\tphonetic = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\tclean_phonetic = ''\n",
    "\t\t\t\t\tif '[' or '/' in phonetic[0]: \n",
    "\t\t\t\t\t\tphonetic[0] = phonetic[0].replace('[', '').replace('/', '').strip()         \n",
    "\t\t\t\t\tif ']' or '/' in phonetic[-1]: \n",
    "\t\t\t\t\t\tphonetic[-1] = phonetic[-1].replace(']', '').replace('/', '').strip()\n",
    "\t\t\t\t\tfor ele in phonetic:        \n",
    "\t\t\t\t\t\tclean_phonetic += ele\n",
    "\t\t\t\t\t\tclean_phonetic += ' '\n",
    "\n",
    "\t\t\t\tif li[:3]=='|||':\n",
    "\t\t\t\t\tif '|| ||' in li:\n",
    "\t\t\t\t\t\tli = li.replace('|| ||', '||')\n",
    "\t\t\t\t\t# si on a rencontré une ligne phonétique et donc qu'on a pas encore de tokens, cette ligne (2e ligne) est la ligne de tokens                   \n",
    "\t\t\t\t\tif not tokens :\n",
    "\t\t\t\t\t\ttokens = [cleanToken(t) for t in li[3:].split('||')] \n",
    "\t\t\t\t\t\tif \"vs.\" in tokens: \n",
    "\t\t\t\t\t\t\tcomparison = True \n",
    "\t\t\t\t\t\tif '' in tokens: \n",
    "\t\t\t\t\t\t\twhile '' in tokens: \n",
    "\t\t\t\t\t\t\t\ttokens.remove('') \n",
    "\t\t\t\t\t\tif 'Equivalent [[KLT' in tokens[-1]: \n",
    "\t\t\t\t\t\t\ttokens[-1] = tokens[-1].replace('Equivalent [[KLT', '').strip()\n",
    "\t\t\t\t\t\tif 'Graphie peurunvan' in tokens[-1]: \n",
    "\t\t\t\t\t\t\ttokens[-1] = tokens[-1].replace('Graphie peurunvan', '').strip()\n",
    "\t\t\t\t\t\tif ']' in tokens[-1]: \n",
    "\t\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "\t\t\t\t\t\tif '[' in tokens[0]: \n",
    "\t\t\t\t\t\t\ttokens[0] = tokens[0].replace('[', '').strip() \n",
    "                        \n",
    "\t\t\t\t\telif not trans: #la première fois = la traduction  \n",
    "\t\t\t\t\t\tif not \"Équivalent standardisé\" in li:                       \n",
    "\t\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]                            \n",
    "\t\t\t\t\t\t\tif '' in trans: \n",
    "\t\t\t\t\t\t\t\twhile '' in trans: \n",
    "\t\t\t\t\t\t\t\t\ttrans.remove('')  \n",
    "\t\t\t\t\t\t\tfor i in range(len(trans)): \n",
    "\t\t\t\t\t\t\t\tif \"aujourd'hui\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\t\ttrans[i]  = trans[i].replace(\"aujourd'hui\", \"aujourdhui\")      \n",
    "\t\t\t\t\t\t\t\tif any(ele in trans[i] for ele in nospace):\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\" \", \"_\")     \n",
    "\t\t\t\t\t\t\t\tif \"jours de la semaine\" or \"jour de la semaine\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = sansespace(trans[i]) \n",
    "\t\t\t\t\t\t\tif re.search(interjections, trans[i]):\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(re.search(interjections, trans[i]).group(0), re.search(interjections, trans[i]).group(0).replace(\" \", \"_\"))\n",
    "\t\t\t\t\t\t\tif re.search(r\".*\\|(.* !)\\]\\]\", trans[i]): \n",
    "\t\t\t\t\t\t\t\tto_replace = re.search(r\".*\\|(.* !)\\]\\]\", trans[i]).group(1)\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"\"))    \n",
    "\t\t\t\t\t\t\tif re.search(r\".*\\|(.* \\?)\\]\\]\", trans[i]): \n",
    "\t\t\t\t\t\t\t\tto_replace2 = re.search(r\".*\\|(.* \\?)\\]\\]\", trans[i]).group(1)\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(to_replace2, to_replace2.replace(\" \", \"\"))  \n",
    "                                    \n",
    "                                    \n",
    "\t\t\t\t\telif 'colspan=' in li:\n",
    "\t\t\t\t\t\tif li.startswith('|||colspan='):\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\telif not source:                        \n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','')   \n",
    "\t\t\t\t\t\t\tif re.search(\"(\\'\\'.*\\'\\' ?,?).*\", source):                              \n",
    "\t\t\t\t\t\t\t\tother = re.search(\"(.*?\\'\\' ?,?).*\", source).group(1)                               \n",
    "\t\t\t\t\t\t\t\tsource = source.replace(other, \"\").replace(\"''\",\"\")  \n",
    "\t\t\t\t\t\t\tif reLanguage.search(li):\n",
    "\t\t\t\t\t\t\t\tgrammatical = False                             \n",
    "\t\t\t\t\t\telif not source2:                            \n",
    "\t\t\t\t\t\t\tsource2 = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')    \n",
    "\t\t\t\t\t\t\tif reLanguage.search(li):\n",
    "\t\t\t\t\t\t\t\tgrammatical = False \n",
    "\t\t\t\t\t\tif source:             \n",
    "\t\t\t\t\t\t\tif source2 : \n",
    "\t\t\t\t\t\t\t\t#source = source2                                \n",
    "\t\t\t\t\t\t\t\tsource, source2 = source2, source\n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", li):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", li):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif reDialect.search(li): \n",
    "\t\t\t\t\t\t\t\tdialect = reDialect.search(li)[0]\n",
    "\t\t\t\t\t\t\tif \":\" in source: \n",
    "\t\t\t\t\t\t\t\tl = re.search(\"(.*):.*\", source).group(1) + ')' \n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tl = source \n",
    "\t\t\t\t\t\t\tif re.search(r\"(.*?)\\[\\[\", li): \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                                          \n",
    "\t\t\t\t\t\t\tif l in pages:                                 \n",
    "\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                                       \n",
    "\t\t\t\t\t\t\t\t\t#on suit la redirection\n",
    "\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)                \n",
    "\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\ts = l                                              \n",
    "\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                 \n",
    "\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\tif reLanguage.search(ele):\n",
    "\t\t\t\t\t\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "\t\t\t\t\t\t\t\tif not dialect:\n",
    "\t\t\t\t\t\t\t\t\tif reDialect.search(li):\n",
    "\t\t\t\t\t\t\t\t\t\tdialect = reDialect.search(li).group(0)\n",
    "\t\t\t\t\t\t\t\t\telse:                                                \n",
    "\t\t\t\t\t\t\t\t\t\tfor region in regions:\n",
    "\t\t\t\t\t\t\t\t\t\t\tif region in li:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = regions[region]\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(li):\n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                                 \n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\ts = l\n",
    "#à améliorer pour trouver le lieu                                \n",
    "# \t\t\t\t\t\t\tif not location:\n",
    "# \t\t\t\t\t\t\t\tif re.search(\"(''.*'')\", li):                                \n",
    "# \t\t\t\t\t\t\t\t\ttest = re.search(\"''(.*)''\", li).group(1) \n",
    "# \t\t\t\t\t\t\t\t\tif not reDialect.search(test) and not test.endswith(\"ais\"):                                     \n",
    "# \t\t\t\t\t\t\t\t\t\tlocation = test \n",
    "                           \n",
    "\t\t\t# print(tokens, trans, translation, source)\n",
    "\t\t\tif not( tokens and trans and translation and source) and grammatical is not False: # if one is missing                \n",
    "\t\t\t\tif not tokens: \n",
    "\t\t\t\t\terrorout.write('\\n__'+currentPage+': Tokens line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\tif not trans: \n",
    "\t\t\t\t\terrorout.write('\\n__'+currentPage+': Trans line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\tif not translation:\n",
    "\t\t\t\t\terrorout.write('\\n__'+currentPage+': Translation line could not be found:'+m.group(1)+'\\n')\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\tif not source:\n",
    "\t\t\t\t\terrorout.write('\\n__'+currentPage+': Source line could not be found:'+m.group(1)+'\\n')  \n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\tcontinue \n",
    "\t\t\tif grammatical is False: \n",
    "\t\t\t\tnon_examples.write(\"__\"+currentPage+\"\\nFound agrammatical sentence:\\n\"+ prettytable + \"\\n\\n\")\n",
    "\t\t\t\tother_table += 1 \n",
    "\t\t\tif comparison is True: \n",
    "\t\t\t\tnon_examples.write(\"__\"+currentPage+\"\\nTableau de comparaison:\\n\"+ prettytable + \"\\n\\n\")\n",
    "\t\t\t\tother_table += 1 \n",
    "\n",
    "\n",
    "\t#pour supprimer les mots agrammaticaux \n",
    "\t#exemple: ( C'hwi / * Ac'hanoc'h ) --> C'hwi  \n",
    "\t\tcl_tokens = []   \n",
    "       \n",
    "\t\tfor i in range(len(tokens)):\n",
    "\t\t\tif tokens[i] == ']':\n",
    "\t\t\t\ttokens[i] = tokens[i].replace(\"]\", \"\")     \n",
    "\t\t\tif tokens[i] == '[':\n",
    "\t\t\t\ttokens[i] = tokens[i].replace(\"[\", \"\")  \n",
    "\t\t\tif '[…]' in tokens[i]:\n",
    "\t\t\t\ttokens[i] = tokens[i].replace('[…]', '')\n",
    "\t\t\tif '(…)' in tokens[i]:\n",
    "\t\t\t\ttokens[i] = tokens[i].replace('(…)', '')   \n",
    "\t\t\t\tif 'Equivalent [[KLT' in tokens[i]: \n",
    "\t\t\t\t\ttokens[i] = tokens[i].replace('Equivalent [[KLT', '').strip()\n",
    "\t\t\tif re.match(r\".*\\((.*\\.)\\)\", title):            \n",
    "\t\t\t\tele = re.match(r\".*\\((.*)\\.\\)\", title).group(1)                \n",
    "\t\t\t\ttokens[i] = tokens[i].replace(ele, \"\")                \n",
    "\t\t\tif '/' in tokens[i] and '*' in tokens[i] and len(tokens)== len(trans):                \n",
    "\t\t\t\telems = tokens[i].split('/')                \n",
    "\t\t\t\tfor m in range(len(elems)):                    \n",
    "\t\t\t\t\tidx = m                    \n",
    "\t\t\t\t\tif '*' in elems[m]:                        \n",
    "\t\t\t\t\t\ttokens[i] = tokens[i].replace(elems[m], '').replace('(', '').replace('{', '').replace('/', '')   \n",
    "\t\t\t\t\t\tcl_tokens.append(tokens[i])                        \n",
    "\t\t\t\t\t\tfor i in range(len(trans)) :              \n",
    "\t\t\t\t\t\t\tif '/' in trans[i]:                       \n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].split('/')[idx].replace('(', '').replace('{', '').replace('/', '')\n",
    "\t\t\telif '/' in tokens[i] and not '*' in tokens[i] and len(tokens) == len(trans):           \n",
    "\t\t\t\tif reAlternatives.search(tokens[i]):  \n",
    "\t\t\t\t\ttry:                     \n",
    "\t\t\t\t\t\ttokens[i] = tokens[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')\n",
    "\t\t\t\t\t\ttrans[i] = trans[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')   \n",
    "\t\t\t\t\texcept: \n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}\\nList index out of range, could not get first alternative:\\ntoken:{tokens[i]}, trans: {trans[i]}\\n\\n\")\n",
    "\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\tproblem = True                                       \n",
    "\t\t\t\t\t\terror_count += 1 \n",
    "\t\tif 'Équivalent standardisé' in tokens: \n",
    "\t\t\ttokens.remove('Équivalent standardisé')        \n",
    "\t\tif re.search(r\"\\(.*/\", translation): \n",
    "\t\t\toptions = re.search(r\"\\(.*\\)\", translation).group(0)\n",
    "\t\t\toption = re.search(r\"\\((.*?)/\", translation).group(1)  \n",
    "\t\t\ttranslation = translation.replace(options, option)     \n",
    "            \n",
    "\t\tif len(' '.join(tokens).split()) == len(' '.join(trans).split()): \n",
    "\t\t\ttokens = ' '.join(tokens).split()               \n",
    "\t\t\ttrans = ' '.join(trans).split()\n",
    "            \n",
    "            \n",
    "\n",
    "\t\tif grammatical is False: \n",
    "\t\t\tbreak \n",
    "\t\ttext = ' '.join(tokens)\n",
    "\t\ttext_ch = remettretoken(text) \n",
    "\n",
    "\n",
    "\t\tif '' in tokens:          \n",
    "\t\t\twhile '' in tokens: \n",
    "\t\t\t\ttokens.remove('')            \n",
    "\t\tif '' in trans: \n",
    "\t\t\twhile '' in trans: \n",
    "\t\t\t\ttrans.remove('') \n",
    "\t\t# construction of the conll:         \n",
    "\t\ttext = ' '.join(tokens)\n",
    "\t\ttext_ch = remettretoken(text)        \n",
    "\t\treferences[source] = references.get(source,[])\n",
    "\t\tif text in references[source]:\n",
    "\t\t\tind = references[source].index(text)+1\n",
    "\t\telse:\n",
    "\t\t\treferences[source]+=[text]\n",
    "\t\t\tind = 1\n",
    "\t\tsentences[text] = sentences.get(text,0)+1\n",
    "\t\tconllis = ['# sent_id = '+source.replace(' ', '') +'__'+str(ind)]\n",
    "\t\tconllis += ['# text = '+text_ch]        \n",
    "\t\tconllis += ['# text_fr = '+ remettretoken(cleanToken(translation)).replace('[…]', '').replace('(…)', '')]\n",
    "\t\tif phonetic:\n",
    "\t\t\tconllis += ['# text_phon = '+remettretoken(clean_phonetic.replace('[…]', '').replace('(…)', ''))]\n",
    "\t\tif dialect:\n",
    "\t\t\tconllis += ['# dialect = ' + dialect]\n",
    "\t\tif location:\n",
    "\t\t\tconllis += ['# location = ' + location]\n",
    "\t\tif s :\n",
    "\t\t\tconllis += ['# source = ' + s] \n",
    "\t\tif source2:\n",
    "\t\t\tconllis += ['# source2 = ' + source2]\n",
    "\t\tif texttype :\n",
    "\t\t\tconllis += ['# texttype = ' + texttype]   \n",
    "\t\t##si on a trouvé du vert alors conllis += ['# phonetic = yes ']\n",
    "\t\t##si un morceau en phonétique alors mettre en trait MISC\n",
    "\n",
    "\t\textrai = 1\n",
    "        \n",
    "\t\tif len(tokens) != len(trans):         \n",
    "\t\t\terrorout.write('\\n___ '+ currentPage + \"\\n\" + \"len tokens:\" \n",
    "\t\t\t+ str(len(tokens)) + \"\\nlen trans:\" + str(len(trans)) + \"\\n\" + \"tokens:\" + str(tokens)+ \"\\n\"+\n",
    "\t\t\t\"trans:\" + str(trans) + \"\\n \" +prettytable + \"\\n\\n\" )\n",
    "\t\t\terror_count += 1 \n",
    "\t\t\tcontinue    \n",
    "\t\tfor i,t in enumerate(tokens):           \n",
    "\t\t\tif not t:\n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': pas de token '+str(i)+' in ' + prettytable + \"\\n\")\n",
    "\t\t\t\tt='???'\n",
    "\t\t\t\terror_count += 1 \n",
    "\t\t\tif \"[[R]]_[[4]] / [[+C]]_\" in trans[i]: \n",
    "\t\t\t\ttrans[i] = trans[i].replace(\" \", \"\").replace(\" / \", \",\")\n",
    "\t\t\t\tprint(f\"here: {trans[i]}\")\n",
    "\t\t\tif len(trans)<=i:                \n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': la longueur de la glose est inférieure à la longueur des tokens '+str(i)+' in ' + prettytable + \"\\n\")\n",
    "\t\t\t\ttr = '???'  \n",
    "\t\t\t\terror_count += 1 \n",
    "\t\t\telse:\n",
    "\t\t\t\ttr = trans[i]              \n",
    "                \n",
    "\t\t\t# délimiteur : @ et espace         \n",
    "\t\t\tif t in one_token:             \n",
    "\t\t\t\ttest1 = [t.replace(\" \", \"_\").replace(\"-\", \"_\")]  \n",
    "\t\t\t\ttest2 = [tr.replace(\" \", \"_\")]\n",
    "\t\t\telif any(ele in t for ele in one_token):\n",
    "\t\t\t\tfor i in range(len(one_token)):                \n",
    "\t\t\t\t\tif re.search(one_token[i], t): \n",
    "\t\t\t\t\t\tif '-' in re.search(one_token[i], t).group(0):                        \n",
    "\t\t\t\t\t\t\ttest1 = re.split(\"@| |-\", t)[:-2]                         \n",
    "\t\t\t\t\t\t\tto_add1 = re.split(\"@| |-\", t)[-2:]\n",
    "\t\t\t\t\t\t\ttest1.append('_'.join(to_add1))             \n",
    "\t\t\t\t\t\t\ttest2 = tr.split()[:-2]\n",
    "\t\t\t\t\t\t\tto_add = tr.split()[-2:]\n",
    "\t\t\t\t\t\t\ttest2.append('_'.join(to_add))\n",
    "\t\t\t\t\t\telif '-' in re.search(one_token[i], t).group(0):                        \n",
    "\t\t\t\t\t\t\ttest1 = re.split(\"@| \", t)[:-2]                      \n",
    "\t\t\t\t\t\t\tto_add1 = re.split(\"-| \", t)[-2:]\n",
    "\t\t\t\t\t\t\ttest1.append('_'.join(to_add1))              \n",
    "\t\t\t\t\t\t\ttest2 = tr.split()[:-2]\n",
    "\t\t\t\t\t\t\tto_add = tr.split()[-2:]\n",
    "\t\t\t\t\t\t\ttest2.append('_'.join(to_add))                        \n",
    "                       \n",
    "\t\t\telse: \n",
    "\t\t\t\ttest1 = re.split(\"@| \", t)\n",
    "\t\t\t\tif '' in test1:  \n",
    "\t\t\t\t\ttest1.remove(\"\")           \n",
    "\t\t\t\ttest2 = re.split(' |@', sansespace(tr))\n",
    "\t\t\t# si on a le même nombre d'éléments pour les tokens et la glose entre chaque || ||          \n",
    "\t\t\tif len(test1) == len(test2):                  \n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]                  \n",
    "\t\t\t\t\ttr1 = test2[j]                           \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tif t1[-1] not in \".,;!?…\" and t1 != \"$$$\":          \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)      \n",
    "\t\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]# et morph ?\n",
    "\t\t\t\t\t\t\tif t1 in ['an', 'al', 'ar']:\n",
    "\t\t\t\t\t\t\t\tpos = 'article défini'\n",
    "\t\t\t\t\t\t\tif t1 in ['un', 'ul', 'ur']:\n",
    "\t\t\t\t\t\t\t\tpos = 'article indéfini'\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]             \n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\tif 'R' in morph:                                \n",
    "\t\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]   \n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\teles[9]+='|'+morph                                \n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]                              \n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                    \n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\t\tif not (lem or pos or gloss or morph) :\n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\t\textrai += 1  \n",
    "                            \n",
    "\t\t\t\t\t\telif t1[-1] in \".,;!?…\" and not t1[-3:] == \"...\" :\n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1[:-1], title, currentPage)\n",
    "\t\t\t\t\t\t\t#si on ne trouve pas de pos, on cherche dans la page elle-même\n",
    "\t\t\t\t\t\t\tif pos=='?' and t1[:-1].lower()==title.lower() and pagepos:\n",
    "\t\t\t\t\t\t\t\tpos = pagepos\n",
    "\t\t\t\t\t\t\tdic[t1[:-1]] = dic.get(t1[:-1], [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[:-1]), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]          \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "\t\t\t\t\t\t\tdic[t1[-1:]] = dic.get(t1[-1:], [(t1[-1:], 'PUNCT','punct')])\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[-1:]), remettretoken(t1[-1:]), 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tprint(f\"error cas 2: ligne du conll invalide (longueur {len(eles)})\")  \n",
    "\t\t\t\t\t\t\tif not (lem or pos or gloss or morph) :\n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)\n",
    "\t\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]                       \n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\ttry:                             \n",
    "\t\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\t\texcept: \n",
    "\t\t\t\t\t\t\t\t\tprint(\"error\") \n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)] \n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                             \n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\t\tif not (lem or pos or gloss or morph) :\n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "              \n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tcontinue                        \n",
    "                        \n",
    "\t\t\t# si on a le nombre d'éléments de tokens est supérieur à celui de la glose                  \n",
    "\t\t\telif len(test1)>len(test2):\n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]                       \n",
    "\t\t\t\t\tif t1 == \"$$$\":\n",
    "\t\t\t\t\t\terrorout.write(f\"__{currentPage}\\nElément agrammatical dans la phrase:\\n{prettytable}test1: {test1}\\ntest2: {test2}\\n\\n\")  \n",
    "\t\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\ttr1 = test2[j]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\ttr1 = \"?\"                     \n",
    "                        \n",
    "                        \n",
    "\t\t\t\t\t# si la ponctuation est tout simplement séparée du token                   \n",
    "\t\t\t\t\tif t1 in \"/-.,;!?…\":\n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [(t1, 'PUNCT','punct')])\n",
    "\t\t\t\t\t\teles = [str(extrai),t1, t1, 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\terrorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                                                    \n",
    "\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\textrai += 1           \n",
    "                    \n",
    "\t\t\t\t\t# les autres tokens sont traités normalement\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)\n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]                       \n",
    "\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\ttry:                             \n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\texcept: \n",
    "\t\t\t\t\t\t\t\tprint(\"error\") \n",
    "\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)] \n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\terrorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                                                       \n",
    "\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\tif not (lem or pos or gloss or morph) :\n",
    "\t\t\t\t\t\t\t\terrorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "\t\t\t\t\t\t\t\terror_count += 1\n",
    "\t\t\t\t\t\textrai += 1\n",
    "\n",
    "\t\t\t# si on a le nombre d'éléments de la glose est supérieur à celui des tokens \n",
    "\t\t\telif len(test1)<len(test2):                   \n",
    "\t\t\t\t# print(f\"PROBLEME ! éléments de la glose supérieur à celui des tokens: {test1, test2}\") \n",
    "\t\t\t\t\terrorout.write('\\n___ '+currentPage+': slash trouvé ' +str(i)+' in '+ \n",
    "prettytable + 'test1: ' + str(test1) + str(len(test1)) + \"\\n\" + 'test2: ' + str(test2) + str(len(test2)) + \"\\n\\n\")\n",
    "\t\t\t\t\terror_count += 1 \n",
    "\t\t\t\t\tproblem = True\n",
    "\n",
    "# pour vérifier qu'aucun token ne manque dans le conll\n",
    "# problème : on splitte seulement sur les espaces mais il faudrait aussi splitter\n",
    "# sur les apostrophes quand ils ne sont pas en début de mot \n",
    "\t\t# test_conll = []\n",
    "\t\t# punct_list = [\".\", \",\", \":\", \"!\", \";\", \"…\", ]\n",
    "\t\t# for i in range(len(conllis)):\n",
    "\t\t# \tif re.search(\"^[0-9]\", conllis[i]):\n",
    "\t\t# \t\ttest_conll.append(conllis[i].split('\\t')[1])\t\n",
    "\t\t# for ele in conllis[1].split()[3:]:\n",
    "\t\t# \tif ele.replace(\".\", \"\").replace(\":\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"…\", \"\") not in test_conll and ele not in punct_list:\n",
    "\t\t# \t\tprint(f\"error: le mot {ele} a disparu\")\n",
    "\t \n",
    "\t\tif any(\"# dialect\" in string for string in conllis):         \n",
    "\t\t\tfor d in dialects:\n",
    "\t\t\t\tif f'# dialect = {d.capitalize()}' in conllis: \n",
    "\t\t\t\t\tdialects[d] += [conllis]  \n",
    "\t\telse: \n",
    "\t\t\tdialects['inconnu'] +=  [conllis]             \n",
    "\tbilan.write(f\"Nombre total: {total_example}, autres exemples: {other_table}, tableaux erreur {error_count}\\n\\n\")\n",
    "\treturn dialects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bb22511-47ca-492d-9e84-a842e73c638f",
   "metadata": {
    "id": "9bb22511-47ca-492d-9e84-a842e73c638f"
   },
   "source": [
    "# trying to run through the whole site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
    "outputId": "d06bcad2-4c05-4761-efaa-ad70357c2289"
   },
   "outputs": [],
   "source": [
    "print(len(pages))\n",
    "dic = {}\n",
    "errorout = open('breton.errors.txt','w', encoding=\"utf-8\")\n",
    "nopos = open('no_pos.txt', 'w', encoding='utf-8')\n",
    "pos_list = open('pos_list.txt','w', encoding='utf-8')\n",
    "bilan = open('bilan.txt','w', encoding='utf-8')\n",
    "doublons = open(\"doublon.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "for title in tqdm.tqdm(pages):\n",
    "\tanalyzePage(title)  \n",
    "for d in dialects:\n",
    "\tfor conll in sorted(dialects[d], key = lambda x: x[1]):\n",
    "\t\tif conll[1].translate(str.maketrans('','',string.punctuation)) not in unique_sentences:         \n",
    "\t\t\topen('bretonconlls/'+re.sub(r'\\W','_',d)+'.conllu','a', encoding=\"utf-8\").write('\\n'.join(conll) + '\\n\\n')  \n",
    "\t\t\tunique_sentences.add(conll[1].translate(str.maketrans('','',string.punctuation)))\n",
    "\t\t\tconverted_tables += 1  \n",
    "\t\telse:\n",
    "\t\t\tdoublon += 1\n",
    "\t\t\tdoublons.write(\"__\\n\" + str('\\n'.join(conll)) + \"\\n\\n\")\n",
    "for key, value in lispos.items():\n",
    "\tpos_list.write(f\"{key} \\t {value}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11bdb8-7193-40e2-81ae-a4a8febae7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "breton_27_07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
