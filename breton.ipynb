{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
   "metadata": {
    "id": "7906d823-690d-4ecb-901c-2ce5ca990ddc",
    "tags": []
   },
   "source": [
    "# exporting the Breton example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, tqdm, re, time\n",
    "#!pip install mwclient\n",
    "#!pip install mwparserfromhell\n",
    "#!pip install mwcomposerfromhell\n",
    "from mwclient import Site\n",
    "# import dominate, pypandoc\n",
    "# from dominate.tags import *\n",
    "# from dominate.util import raw\n",
    "#myparserfromhell : parser pour le code wiki\n",
    "import mwparserfromhell, mwcomposerfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04865f40-0ac3-4c8b-91d5-69c6d23a17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ipykernel\n",
    "#!pip install mwcomposerfromhell\n",
    "#%pip install pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ae8f1-c7a4-4b77-9215-e6b7de521237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('connecting...')\n",
    "#site contenant tous les exemples\n",
    "site = Site('arbres.iker.cnrs.fr/', path='/')\n",
    "#site.login('Kimgerdes', 'azerAZER1')\n",
    "outf = open('problemPages.html','w')\n",
    "\n",
    "# images = str(list(site.allimages()))\n",
    "#print(images)\n",
    "#print(dir(images[0]))\n",
    "#print(images[0].name)\n",
    "#qsdf\n",
    "allpages = list(site.allpages())\n",
    "print('got all', len(allpages), 'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1c7f7-606b-4cff-a02e-2c0f39ffbd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "#dictionnaire contenant toutes les pages aspirées\n",
    "#pages = {}\n",
    "#on peut ici modifier le nombre de pages à charger pour charger petit à petit ex : allpages[:3000] pour les 3000 premières pages\n",
    "#newlist=allpages[:8068]\n",
    "#for p in tqdm.tqdm(allpages):\n",
    "#for p in tqdm.tqdm(newlist):\n",
    "#\t#si les pages ne sont pas de la documentation\n",
    "#\tif not('/documentation' in p.name or '/docname' in p.name or 'Module:' in p.name):\n",
    "#\t\t#on ajoute le nom de la page comme clé, son contenu comme valeur\n",
    "#\t\tpages[p.name] = p.text() \n",
    "#print('extraction done. it took', time.time()-t, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b64fe-779e-446a-a721-460784bf0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#une fois toutes les pages chargées, on le met dans un pickle\n",
    "#with open(\"Dict.txt\", \"wb\") as myFile:\n",
    "#    pickle.dump(pages, myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecd28ae-7f11-4b5a-95d1-980e7613b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on utilise le(s) pickle(s)\n",
    "import pickle\n",
    "with open(\"Dict.txt\", \"rb\") as myFile:\n",
    "    pages = pickle.load(myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76a06d9-1a49-4454-a663-84b922217f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sources.txt\", \"rb\") as file:\n",
    "    sources = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 8068 pages\n"
     ]
    }
   ],
   "source": [
    "# print(', '.join(pages.keys()))\n",
    "#on créé un fichier txt contenant tous les titres de pages\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34c8233-eb54-4ed8-a66b-fbe832350c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 13422 examples\n"
     ]
    }
   ],
   "source": [
    "nbpretty=0\n",
    "#on parcourt les clés et valeurs du dictionnaire pages\n",
    "for name,text in pages.items():\n",
    "\t#et on compte le nombre de tableaux (donc d'exemples)\n",
    "\tnbpretty += text.count('class=\"prettytable\"')\n",
    "print('found',nbpretty,'examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "\n",
      "Ar Borgn (2011)\n",
      "Ar Borgn (2011)\n",
      "Martin (1929)\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "Kerrien (2000)\n",
      "Milin (1922)\n",
      "GW.\n",
      "Gros (1970b)\n",
      "Gros (1970b)\n",
      "Gros (1970b)\n",
      "IB.\n",
      "Trépos (2001)\n",
      "Seite (1998)\n",
      "Seite (1998)\n",
      "Kerrain (2001)\n",
      "Avezard-Roger (2004a)\n",
      "Avezard-Roger (2004a)\n",
      "Seite (1998)\n",
      "Seite (1998)\n",
      "German (2007)\n",
      "Heusaff (1996)\n",
      "# sent_id = Kerrien(2000:12)__1\n",
      "# text = Nikun ne da tre, 'met a_[[1]]_ Vreiz e ve.\n",
      "# text_fr = 'Personne ne pénètre, à moins d'être né en Bretagne.'\n",
      "# dialect = Léonard\n",
      "# source = Kerrien (1926)\n",
      "# texttype = Références de corpus\n",
      "1\tNikun\tnikun\tquantifieur\t_\t_\t_\t_\t_\tGloss=personne\n",
      "2\tne\tne\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=ne|Mutation=trigger|MutationNUM=1\n",
      "3\tda\tdont\tverbe\t_\t_\t_\t_\t_\tGloss=vient\n",
      "4\ttre\tpostpositions\tpostposition\t_\t_\t_\t_\t_\tGloss=postpos.\n",
      "5\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "6\t'\tmet\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=sauf\n",
      "7\tmet\tmet\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=?\n",
      "8\ta_[[1]]_\ta_[[1]]_\t?\t_\t_\t_\t_\t_\tGloss=1\n",
      "9\tVreiz\tBreizh\tnom\t_\t_\t_\t_\t_\tGloss=Bretagne\n",
      "10\te\te\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "11\tve\tCOP\tverbe\t_\t_\t_\t_\t_\tGloss=serait\n",
      "12\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Milin(1922:402)__1\n",
      "# text = Ar plac'h yaouank, ne ouie doare eus a netra, a zo souezet.\n",
      "# text_fr = 'La jeune femme, qui ne savait rien de rien, est étonnée.'\n",
      "# dialect = Léonard\n",
      "# location = Saint Pol de Léon\n",
      "# source = Milin (1922)\n",
      "# texttype = Références de corpus\n",
      "1\tAr\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "2\tplac'h\tplac'h\t?\t_\t_\t_\t_\t_\tGloss=fille\n",
      "3\tyaouank\tyaouank\tadjectif\t_\t_\t_\t_\t_\tGloss=jeune\n",
      "4\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "5\tne\tne\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=ne|Mutation=trigger|MutationNUM=1\n",
      "6\touie\tgouzout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=savait\n",
      "7\tdoare\tdoare\tnom\t_\t_\t_\t_\t_\tGloss=façon\n",
      "8\teus\teus\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "9\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "10\tnetra\tnetra\tquantifieur\t_\t_\t_\t_\t_\tGloss=rien\n",
      "11\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "12\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "13\tzo\tzo\t?\t_\t_\t_\t_\t_\tGloss=est\n",
      "14\tsouezet\tsouezhañ.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=étonn.é\n",
      "15\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Seite(1998:21)__1\n",
      "# text = Te, marteze a hellfe kana anezi.\n",
      "# text_fr = 'Toi, tu pourrais peut-être la chanter.'\n",
      "# dialect = Léonard\n",
      "# location = Cléder\n",
      "# source = Seite (1998)\n",
      "# texttype = Références de corpus\n",
      "1\tTe\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=toi\n",
      "2\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "3\tmarteze\tmarteze\tadverbe\t_\t_\t_\t_\t_\tGloss=peut.être\n",
      "4\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "5\thellfe\tgallout\tverbe\t_\t_\t_\t_\t_\tGloss=pourrait\n",
      "6\tkana\tkanañ\tverbe\t_\t_\t_\t_\t_\tGloss=chanter\n",
      "7\tanezi\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=elle\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Seite(1998:21)__1\n",
      "# text = Greet am-eus anez-o oll.\n",
      "# text_fr = 'Je les ai toutes faites.' (les troménies)\n",
      "# dialect = Léonard\n",
      "# location = Cléder\n",
      "# source = Seite (1998)\n",
      "# texttype = Références de corpus\n",
      "1\tGreet\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=fait\n",
      "2\tam-eus\tam-eus\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\tanez-o\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=eux\n",
      "4\toll\tholl\tadverbe|quantifieur\t_\t_\t_\t_\t_\tGloss=tous\n",
      "5\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Seite(1998:7)__1\n",
      "# text = Méd lavar din anezi da welet.\n",
      "# text_fr = 'Mais dis-la moi pour voir.'\n",
      "# dialect = Léonard\n",
      "# location = Cléder\n",
      "# source = Seite (1998)\n",
      "# texttype = Références de corpus\n",
      "1\tMéd\tmet\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=mais\n",
      "2\tlavar\tlavarout\tverbe\t_\t_\t_\t_\t_\tGloss=dis\n",
      "3\tdin\tda.pronom_incorporé\tpréposition.?\t_\t_\t_\t_\t_\tGloss=à.moi\n",
      "4\tanezi\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=elle\n",
      "5\tda\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=pour|Mutation=trigger|MutationNUM=1\n",
      "6\twelet\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=voir\n",
      "7\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Seite(1998:8)__1\n",
      "# text = Hemañ a oa eur paotr fin anezañ.\n",
      "# text_fr = 'Celui-ci était malin.'\n",
      "# dialect = Léonard\n",
      "# location = Cléder\n",
      "# source = Seite (1998)\n",
      "# texttype = Références de corpus\n",
      "1\tHemañ\tDEM\tpronom\t_\t_\t_\t_\t_\tGloss=celui.ci\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\toa\tCOP\tverbe\t_\t_\t_\t_\t_\tGloss=était\n",
      "4\teur\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "5\tpaotr\tpaotr\tnom\t_\t_\t_\t_\t_\tGloss=gars\n",
      "6\tfin\tfin\t?\t_\t_\t_\t_\t_\tGloss=fin\n",
      "7\tanezañ\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Martin(1929:181)__1\n",
      "# text = Jôzeb, pa zec'h de Lañnijen, c'hui zei da welet nounn.\n",
      "# text_fr = 'Joseph, quand tu passeras par Lanvénégen, tu viendras me voir.'\n",
      "# dialect = Cornouaillais\n",
      "# location = Lanvenegen\n",
      "# source = Martin (1929)\n",
      "# texttype = Références de corpus\n",
      "1\tJôzeb\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Joseph\n",
      "2\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "3\tpa\tpa\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=quand|Mutation=trigger|MutationNUM=1\n",
      "4\tzec'h\tdont\tverbe\t_\t_\t_\t_\t_\tGloss=viendrez\n",
      "5\tde\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=à|Mutation=trigger|MutationNUM=1\n",
      "6\tLañnijen\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Lanvénégen\n",
      "7\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "8\tzei\tdont\tverbe\t_\t_\t_\t_\t_\tGloss=viendra\n",
      "9\tda\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=pour|Mutation=trigger|MutationNUM=1\n",
      "10\twelet\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=voir\n",
      "11\tnounn\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=moi\n",
      "12\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Trépos(2001:§317)__1\n",
      "# text = Kontit deom pep a gontadenn.\n",
      "# text_fr = 'Contez-nous chacun une histoire.'\n",
      "# dialect = Cornouaillais\n",
      "# location = Bigouden\n",
      "# source = Trépos (1968)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tKontit\tkontañ\tverbe\t_\t_\t_\t_\t_\tGloss=contez\n",
      "2\tdeom\tda.pronom_incorporé\tpréposition.?\t_\t_\t_\t_\t_\tGloss=à.nous\n",
      "3\tpep\tpep\tquantifieur\t_\t_\t_\t_\t_\tGloss=chaque\n",
      "4\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "5\tgontadenn\tkontadenn\t?\t_\t_\t_\t_\t_\tGloss=histoire\n",
      "6\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = German(2007:172)__1\n",
      "# text = Ac yna edrych ohonaw fe ar liw yr erchwys.\n",
      "# text_fr = 'Et il regarda la couleur de la meute.'\n",
      "# dialect = Cornouaillais\n",
      "# source = German (2007)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tAc\tAc\t?\t_\t_\t_\t_\t_\tGloss=et\n",
      "2\tyna\tyna\t?\t_\t_\t_\t_\t_\tGloss=là\n",
      "3\tedrych\tedrych\t?\t_\t_\t_\t_\t_\tGloss=regarda\n",
      "4\tohonaw\tohonaw\t?\t_\t_\t_\t_\t_\tGloss=P.lui\n",
      "5\tfe\tfe\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "6\tar\tar\tadverbe\t_\t_\t_\t_\t_\tGloss=sur\n",
      "7\tliw\tliw\t?\t_\t_\t_\t_\t_\tGloss=couleur\n",
      "8\tyr\tyr\t?\t_\t_\t_\t_\t_\tGloss=le\n",
      "9\terchwys\terchwys\t?\t_\t_\t_\t_\t_\tGloss=meute\n",
      "10\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Gros(1970b:§'libistra')__1\n",
      "# text = Eur marmouz bennag e-nevoa libistret ar prenestr a goh-saout.\n",
      "# text_fr = 'Quelque galopin avait enduit la fenêtre de bouse de vache.'\n",
      "# dialect = Trégorrois\n",
      "# source = Gros (1970b)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tEur\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "2\tmarmouz\tmarmouz\tnom\t_\t_\t_\t_\t_\tGloss=singe\n",
      "3\tbennag\tbennak\tquantifieur|adjectif\t_\t_\t_\t_\t_\tGloss=quelconque\n",
      "4\te-nevoa\tkaout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=avait\n",
      "5\tlibistret\tlibistrañ.-et_(Adj.)\t?.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=barbouill.é\n",
      "6\tar\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "7\tprenestr\tprenestr\tnom\t_\t_\t_\t_\t_\tGloss=fenêtre\n",
      "8\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "9\tgoh-saout\tkaoc'h\t?\t_\t_\t_\t_\t_\tGloss=merde\n",
      "10\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Gros(1970b:§'lien')__1\n",
      "# text = Hennez a oa e bord eur batiment a lïen.\n",
      "# text_fr = 'Celui-là était à bord d'un navire à voiles.'\n",
      "# dialect = Trégorrois\n",
      "# source = Gros (1970b)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tHennez\tDEM\tpronom\t_\t_\t_\t_\t_\tGloss=celui.ci\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\toa\tCOP\tverbe\t_\t_\t_\t_\t_\tGloss=était\n",
      "4\te\tP.e\tpréposition\t_\t_\t_\t_\t_\tGloss=en\n",
      "5\tbord\tbord\t?\t_\t_\t_\t_\t_\tGloss=bord\n",
      "6\teur\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "7\tbatiment\tbatimant\t?\t_\t_\t_\t_\t_\tGloss=bâtiment\n",
      "8\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "9\tlïen\tlien\tnom\t_\t_\t_\t_\t_\tGloss=voile\n",
      "10\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Gros(1970b:§'leue')__1\n",
      "# text = Aliesoh a grohen leue a ya da zeha evid a grohen buoh.\n",
      "# text_fr = 'Il meurt plus de jeunes que de vieux.'\n",
      "# dialect = Trégorrois\n",
      "# source = Gros (1970b)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tAliesoh\talies.-oc'h\tadverbe.?\t_\t_\t_\t_\t_\tGloss=souvent.plus\n",
      "2\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "3\tgrohen\tgrohen\t?\t_\t_\t_\t_\t_\tGloss=peau\n",
      "4\tleue\tleue\tnom\t_\t_\t_\t_\t_\tGloss=veau\n",
      "5\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "6\tya\tmont\tverbe\t_\t_\t_\t_\t_\tGloss=va\n",
      "7\tda\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=à|Mutation=trigger|MutationNUM=1\n",
      "8\tzeha\tsec'hiñ\t?\t_\t_\t_\t_\t_\tGloss=sécher\n",
      "9\tevid\tevit\tcomplémenteur|préposition\t_\t_\t_\t_\t_\tGloss=que\n",
      "10\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "11\tgrohen\tgrohen\t?\t_\t_\t_\t_\t_\tGloss=peau\n",
      "12\tbuoh\tbuoc'h\t?\t_\t_\t_\t_\t_\tGloss=vache\n",
      "13\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = ArBorgn(2011:45)__1\n",
      "# text = Monet a rae a bihan fourchad.\n",
      "# text_fr = 'Il allait à petits pas.'\n",
      "# dialect = Vannetais\n",
      "# source = Ar Borgn (2011)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tMonet\tmont\tverbe\t_\t_\t_\t_\t_\tGloss=aller\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\trae\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=faisait\n",
      "4\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=à\n",
      "5\tbihan\tbihan\tadverbe|quantifieur|adjectif\t_\t_\t_\t_\t_\tGloss=petit\n",
      "6\tfourchad\tfourch.-ad,_-iad\tnom.nom|suffixe\t_\t_\t_\t_\t_\tGloss=pas.long\n",
      "7\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = ArBorgn(2011:110)__1\n",
      "# text = C'hwi a gemera ac'hanin e'it un azen pe petra ?\n",
      "# text_fr = 'Tu me prends pour un âne ou quoi ?'\n",
      "# dialect = Vannetais\n",
      "# source = Ar Borgn (2011)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tC'hwi\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=vous\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\tgemera\tkemer\tverbe\t_\t_\t_\t_\t_\tGloss=prend\n",
      "4\tac'hanin\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=moi\n",
      "5\te'\tevit\tcomplémenteur|préposition\t_\t_\t_\t_\t_\tGloss=pour\n",
      "6\tit\tit\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "7\tun\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "8\tazen\tazen\tnom\t_\t_\t_\t_\t_\tGloss=âne\n",
      "9\tpe\tpe\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=ou\n",
      "10\tpetra\tpetra\tpronom|complémenteur\t_\t_\t_\t_\t_\tGloss=quoi\n",
      "11\t?\t?\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Vannetais,IB.(1910:19)__1\n",
      "# text = É nep tu ne vehé kavet kement a sord guskemanteu èl é Breih.\n",
      "# text_fr = 'Nulle part on ne trouverait autant de sortes de costumes qu'en Bretagne.'\n",
      "# dialect = Vannetais\n",
      "# source = IB.\n",
      "# texttype = Références de corpus\n",
      "1\tÉ\tP.e\tpréposition\t_\t_\t_\t_\t_\tGloss=en\n",
      "2\tnep\tnep_X\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=nep\n",
      "3\ttu\ttu\tnom\t_\t_\t_\t_\t_\tGloss=côté\n",
      "4\tne\tne\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=ne|Mutation=trigger|MutationNUM=1\n",
      "5\tvehé\tCOP\tverbe\t_\t_\t_\t_\t_\tGloss=serait\n",
      "6\tkavet\tkavout.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=trouv.é\n",
      "7\tkement\tkement\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=autant\n",
      "8\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "9\tsord\tsort\tnom|quantifieur|préposition|adjectif\t_\t_\t_\t_\t_\tGloss=sorte\n",
      "10\tguskemanteu\tgwiskañ.-amant.-où_(PL.)\tverbe.nom|suffixe.suffixe\t_\t_\t_\t_\t_\tGloss=costum.e.s\n",
      "11\tèl\tevel\tcomplémenteur|préposition\t_\t_\t_\t_\t_\tGloss=comme\n",
      "12\té\tP.e\tpréposition\t_\t_\t_\t_\t_\tGloss=en\n",
      "13\tBreih\tBreizh\tnom\t_\t_\t_\t_\t_\tGloss=Bretagne\n",
      "14\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Standard,Kerrain(2001)__1\n",
      "# text = ( C'hwi / *]] Ac'hanoc'h ) am eus gwelet er marc'had gant ur garrigellad avaloù.\n",
      "# text_fr = 'C'est vous que j'ai vu au marché avec un chariot de pommes.'\n",
      "# dialect = Standard\n",
      "# source = Kerrain (2001)\n",
      "# texttype = Ouvrages pédagogiques\n",
      "1\t(\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=vous\n",
      "2\tC'hwi\tC'hwi\t?\t_\t_\t_\t_\t_\tGloss=/\n",
      "3\t/\t/\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "4\t*]]\t*]]\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "5\tAc'hanoc'h\tAc'hanoc'h\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "6\t)\t)\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "7\tam\tam\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "8\teus\tkaout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=a\n",
      "9\tgwelet\tgwelet.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=v.u\n",
      "10\ter\tP.e.art\tpréposition.quantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=en.le\n",
      "11\tmarc'had\tmarc'had\t?\t_\t_\t_\t_\t_\tGloss=marché\n",
      "12\tgant\tgant\tpréposition\t_\t_\t_\t_\t_\tGloss=avec\n",
      "13\tur\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "14\tgarrigellad\tkarr\tnom\t_\t_\t_\t_\t_\tGloss=char]].[[DIM|iot]].[[-ad|ée|Mutation=target|MutationNUM=1\n",
      "15\tavaloù\taval.-où_(PL.)\tnom.suffixe\t_\t_\t_\t_\t_\tGloss=pomme.s\n",
      "16\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = arBarzhig(1976:36)__1\n",
      "# text = Darn a red a-gleiz, darn all a-zehou...\n",
      "# text_fr = 'Certains courent à gauche, d'autres à droite.'\n",
      "# location = Kaouenneg\n",
      "# source = ar Barzhig (1976)\n",
      "1\tDarn\tdarn\tquantifieur\t_\t_\t_\t_\t_\tGloss=certain\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\tred\tredek\tverbe\t_\t_\t_\t_\t_\tGloss=court\n",
      "4\ta-gleiz\tkleiz\tnom|adjectif\t_\t_\t_\t_\t_\tGloss=gauche|Mutation=target|MutationNUM=1\n",
      "5\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "6\tdarn\tdarn\tquantifieur\t_\t_\t_\t_\t_\tGloss=certain\n",
      "7\tall\tall\tquantifieur|adjectif\t_\t_\t_\t_\t_\tGloss=autre\n",
      "8\ta-zehou\tdehoù\tnom|adjectif\t_\t_\t_\t_\t_\tGloss=droite|Mutation=target|MutationNUM=1\n",
      "11\t...\t...\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Nicolas(2005:48)__1\n",
      "# text = Me 'anava mat ar vaouez an hani 'zo ' chom a-kostez.\n",
      "# text_fr = 'Je connais bien la femme qui habite à côté.'\n",
      "# text_phon = mǝ anawa mat ar vwes nɛ͂n zi ʃom akosti \n",
      "# source = ar Barzhig (1976)\n",
      "1\tMe\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=moi\n",
      "2\t'\tanavezout\tverbe\t_\t_\t_\t_\t_\tGloss=connait\n",
      "3\tanava\tanava\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "4\tmat\tmat\tadverbe|adjectif\t_\t_\t_\t_\t_\tGloss=bien\n",
      "5\tar\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "6\tvaouez\tmaouez\tnom\t_\t_\t_\t_\t_\tGloss=femme|Mutation=target|MutationNUM=1\n",
      "7\tan\tan\t?\t_\t_\t_\t_\t_\tGloss=\n",
      "8\thani\thani\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "9\t'\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "10\tzo\tzo\t?\t_\t_\t_\t_\t_\tGloss=hini\n",
      "11\t'\tzo\t?\t_\t_\t_\t_\t_\tGloss=est\n",
      "12\t\t\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "13\tchom\tchom\tverbe\t_\t_\t_\t_\t_\tGloss=?\n",
      "14\ta-kostez\tparticule_o\tadverbe|préposition\t_\t_\t_\t_\t_\tGloss=à\n",
      "15\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Nicolas(2005:26)__1\n",
      "# text = Betaat a rit a-vuzul m'eh vrasait.\n",
      "# text_fr = 'Vous devenez plus bête en grandissant.'\n",
      "# text_phon =  betad ǝ rǝt avǝzǝl mi frasǝt  \n",
      "# source = Ar Borgn (2011)\n",
      "1\tBetaat\tbetaat\t?\t_\t_\t_\t_\t_\tGloss=bêtir\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\trit\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=faites\n",
      "4\ta-vuzul\ta-vuzul\tpréposition\t_\t_\t_\t_\t_\tGloss=à-mesure\n",
      "5\tm'\tma\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=que\n",
      "6\teh\teh\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "7\tvrasait\tbrasaat\t?\t_\t_\t_\t_\t_\tGloss=grandissez\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:287)__1\n",
      "# text = Me 'wel ac'hanoc'h.\n",
      "# text_fr = 'Je vous vois.'\n",
      "# text_phon =  me wɛl hãX   \n",
      "# source = Martin (1929)\n",
      "1\tMe\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=moi\n",
      "2\t'\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=voit\n",
      "3\twel\twel\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "4\tac'hanoc'h\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=vous\n",
      "5\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:288)__1\n",
      "# text = Gwelout a ran ac'hanoc'h.\n",
      "# text_fr = 'Je vous vois.'\n",
      "# text_phon =  gɥɛl arõ oX   \n",
      "# source = Martin (1929)\n",
      "1\tGwelout\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=voir\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\tran\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=fais\n",
      "4\tac'hanoc'h\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=vous\n",
      "5\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Kernaudour(2017:7,11)__1\n",
      "# text = kent ma sortio deus a Vreizh  Équivalent standardisé\n",
      "# text_fr = 'avant qu'elle ne s'expatrie'\n",
      "# text_phon = quent ma sortiou deus a vreis \n",
      "# source = Milin (1922)\n",
      "1\tkent\tkent\tadverbe\t_\t_\t_\t_\t_\tGloss=avant\n",
      "2\tma\tma\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=que|Mutation=trigger|MutationNUM=4\n",
      "3\tsortio\tsortial\tverbe\t_\t_\t_\t_\t_\tGloss=sortira\n",
      "4\tdeus\tdeus\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "5\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "6\tVreizh\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Bretagne\n",
      "7\t??\t??\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "8\t?\t?\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "9\tÉquivalent\tÉquivalent\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "10\tstandardisé\tstandardisé\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "\n",
      "\n",
      "# sent_id = GW.__1\n",
      "# text = An buhez Sant Gwenole abat kentaf eus a Landevennec.\n",
      "# text_fr = 'La vie de Saint-Gwénolé premier abbé de Landévennec.'\n",
      "# location = Berrien\n",
      "# source = Mystère de Saint Gwénolé(moyen breton)\n",
      "# texttype = Références de corpus\n",
      "1\tAn\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "2\tbuhez\tbuhez\tnom\t_\t_\t_\t_\t_\tGloss=vie\n",
      "3\tSant\tsant\tnom\t_\t_\t_\t_\t_\tGloss=Saint\n",
      "4\tGwenole\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Gwenole\n",
      "5\tabat\tabad\t?\t_\t_\t_\t_\t_\tGloss=abbé\n",
      "6\tkentaf\tkentañ\tadjectif\t_\t_\t_\t_\t_\tGloss=premier\n",
      "7\teus\teus\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "8\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "9\tLandevennec\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Landevennec\n",
      "10\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = arBarzhig(1976:36)__1\n",
      "# text = Darn a red a-gleiz, darn all a-zehou...\n",
      "# text_fr = '[[*]] Certains couraient venant de la gauche, d'autres venant de la droite.'\n",
      "# source = Mystère de Saint Gwénolé(moyen breton)\n",
      "1\tDarn\tdarn\tquantifieur\t_\t_\t_\t_\t_\tGloss=certain\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\tred\tredek\tverbe\t_\t_\t_\t_\t_\tGloss=court\n",
      "4\ta-gleiz\ta-gleiz\t?\t_\t_\t_\t_\t_\tGloss=1\n",
      "5\t,\t,\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "6\tdarn\tdarn\tquantifieur\t_\t_\t_\t_\t_\tGloss=certain\n",
      "7\tall\tall\tquantifieur|adjectif\t_\t_\t_\t_\t_\tGloss=autre\n",
      "8\ta-zehou\ta-zehou\t?\t_\t_\t_\t_\t_\tGloss=1\n",
      "11\t...\t...\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = ArFloc'h(1985:75)__1\n",
      "# text = Ar frer rener eta a rankas bezañ klevet un diaoul a dra diwar va fenn...\n",
      "# text_fr = 'Le frère supérieur a dû entendre une diable de chose sur mon compte... '\n",
      "# location = Bodilis\n",
      "# source = Ar Floch (1985)\n",
      "1\tAr\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=le\n",
      "2\tfrer\tfrer\t?\t_\t_\t_\t_\t_\tGloss=frère\n",
      "3\trener\tren.-er,_-our\tverbe.nom|suffixe\t_\t_\t_\t_\t_\tGloss=direct.eur\n",
      "4\teta\teta\tconjonction\t_\t_\t_\t_\t_\tGloss=donc\n",
      "5\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "6\trankas\trankout\tverbe\t_\t_\t_\t_\t_\tGloss=dut\n",
      "7\tbezañ\tbezañ\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=être\n",
      "8\tklevet\tklevout.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=entend.u\n",
      "9\tun\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "10\tdiaoul\tdiaoul\tnom\t_\t_\t_\t_\t_\tGloss=diable\n",
      "11\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=1\n",
      "12\tdra\ttra\tnom\t_\t_\t_\t_\t_\tGloss=chose\n",
      "13\tdiwar\tdiwar\tpréposition\t_\t_\t_\t_\t_\tGloss=de.sur\n",
      "14\tva\tPOSS\tdéterminant\t_\t_\t_\t_\t_\tGloss=mon|Mutation=trigger|MutationNUM=2\n",
      "15\tfenn\tpenn\tnom\t_\t_\t_\t_\t_\tGloss=tête\n",
      "18\t...\t...\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:121)__1\n",
      "# text = Ur sizhun 'zo dija 'meus ket bet de keve deus eñ.\n",
      "# text_fr = 'Il y a une semaine déjà que je n'ai pas eu de nouvelles de lui.'\n",
      "# text_phon = o zyn zo deʒa mœs ke bit dø '@kɛv_ə_ døs ã \n",
      "# source = Ar Floch (1985)\n",
      "1\tUr\tart\tquantifieur|déterminant\t_\t_\t_\t_\t_\tGloss=un\n",
      "2\tsizhun\tsizhun\tnom\t_\t_\t_\t_\t_\tGloss=semaine\n",
      "3\t'\tzo\t?\t_\t_\t_\t_\t_\tGloss=y.a\n",
      "4\tzo\tdija\tadverbe\t_\t_\t_\t_\t_\tGloss=déjà\n",
      "5\tdija\tdija\tadverbe\t_\t_\t_\t_\t_\tGloss=?\n",
      "6\t'\tkaout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=ai\n",
      "7\tmeus\tmeus\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "8\tket\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "9\tbet\tbet\t?\t_\t_\t_\t_\t_\tGloss=eu\n",
      "10\tde\tde\t?\t_\t_\t_\t_\t_\tGloss=de\n",
      "11\tkeve\tkeve\t?\t_\t_\t_\t_\t_\tGloss=nouvelles\n",
      "12\tdeus\tdeus\tpréposition\t_\t_\t_\t_\t_\tGloss=de\n",
      "13\teñ\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=lui\n",
      "14\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:419)__1\n",
      "# text = anezi e velan\n",
      "# text_fr = 'Je la vois.' (C'est elle que je vois)\n",
      "# source = Avezard-Roger (2004a)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tanezi\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=elle\n",
      "2\te\te\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\tvelan\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=vois\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:419)__1\n",
      "# text = anezo e velɔ̃m bɛmde\n",
      "# text_fr = 'Nous les voyons tous les jours.' (C'est eux que nous...)\n",
      "# source = Avezard-Roger (2004a)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tanezo\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=elle\n",
      "2\te\te\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "3\tvelɔ̃m\tgwelout\tverbe\t_\t_\t_\t_\t_\tGloss=voyons\n",
      "4\tbɛmde\tbemdez\tadverbe\t_\t_\t_\t_\t_\tGloss=chaque.jour\n",
      "\n",
      "\n",
      "# sent_id = Avezard-Roger(2004a:419)__1\n",
      "# text = Karout a ra anezhan e vugale.\n",
      "# text_fr = 'Ses enfants l'aiment.'\n",
      "# text_phon =  kaʁud aʁa anezã he vygale \n",
      "# source = Avezard-Roger (2004a)\n",
      "1\tKarout\tkarout\tverbe\t_\t_\t_\t_\t_\tGloss=aimer\n",
      "2\ta\ta\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R|Mutation=trigger|MutationNUM=1\n",
      "3\tra\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=fait\n",
      "4\tanezhan\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "5\te\tPOSS\tdéterminant\t_\t_\t_\t_\t_\tGloss=son|Mutation=trigger|MutationNUM=1\n",
      "6\tvugale\tbugel.pluriel_interne\tnom.?\t_\t_\t_\t_\t_\tGloss=enfant.s\n",
      "7\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Douarnenez,[HD2010]__1\n",
      "# text = Marijo neus (o frenet / prenet anezho).\n",
      "# text_fr = 'Marijo (les) a achetés.'\n",
      "# source = Seite (1998)\n",
      "1\tMarijo\tnom_propre\t?\t_\t_\t_\t_\t_\tGloss=Marijo\n",
      "2\tneus\tkaout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=a\n",
      "3\t(o\tPOP\tpronom\t_\t_\t_\t_\t_\tGloss=les|Mutation=trigger|MutationNUM=2\n",
      "4\tfrenet\tprenañ.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=achet.é\n",
      "5\t/\t/\t?\t_\t_\t_\t_\t_\tGloss=/\n",
      "6\tprenet\tprenañ.-et_(Adj.).[pronom_incorporé\tverbe.suffixe|verbe.?\t_\t_\t_\t_\t_\tGloss=achet.é.eux\n",
      "7\tanezho).\tanezho).\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "\n",
      "\n",
      "# sent_id = Cheveau&Kersulec(2012-évolutif:Moëlan,'a-bosubl')__1\n",
      "# text = Me zo deuet a-benn da gompren anezhañ a-bosubl n'en deus ket kaozeet ouzhin anezhañ.\n",
      "# text_fr = 'J'ai réussi à le comprendre quand bien même il ne m'a pas parlé.'\n",
      "# text_phon = mə zo ˈdɛd a bɛn da ˈgõpʁən ˈnaɔ̯̃ a ˈpoʃə nø ʃə ˈkoːst ijõ ˌnõ \n",
      "# source = Seite (1998)\n",
      "1\tMe\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=moi\n",
      "2\tzo\tzo\t?\t_\t_\t_\t_\t_\tGloss=est\n",
      "3\tdeuet\tdont.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=ven.u\n",
      "4\ta-benn\ta-benn\tcomplémenteur|préposition\t_\t_\t_\t_\t_\tGloss=à.bout\n",
      "5\tda\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=de|Mutation=trigger|MutationNUM=1\n",
      "6\tgompren\tkompren\tverbe\t_\t_\t_\t_\t_\tGloss=comprendre\n",
      "7\tanezhañ\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "8\ta-bosubl\ta-bosubl\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=même.si\n",
      "9\tn'\tn'\t?\t_\t_\t_\t_\t_\tGloss=ne\n",
      "10\ten\ten\tparticule_verbale\t_\t_\t_\t_\t_\tGloss=R\n",
      "11\tdeus\tkaout\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=a\n",
      "12\tket\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "13\tkaozeet\tkaozeal.-et_(Adj.)\tverbe.suffixe|verbe\t_\t_\t_\t_\t_\tGloss=parl.é\n",
      "14\touzhin\touzh.pronom_incorporé\tpréposition.?\t_\t_\t_\t_\t_\tGloss=à.moi\n",
      "15\tanezhañ\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "16\t.\t.\tPUNCT\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Widmer2017:226)__1\n",
      "# text = hogos dez eu anezy\n",
      "# text_fr = 'Ce sera bientôt le matin.'\n",
      "# source2 =  Breton XVI°, Le Berre (2011) \n",
      "# source = Seite (1998)\n",
      "1\thogos\thogos\tadverbe|adjectif\t_\t_\t_\t_\t_\tGloss=presque\n",
      "2\tdez\tdeiz\tnom\t_\t_\t_\t_\t_\tGloss=jour\n",
      "3\teu\tE\tverbe\t_\t_\t_\t_\t_\tGloss=est\n",
      "4\tanezy\tpronom_incorporé\t?\t_\t_\t_\t_\t_\tGloss=elle\n",
      "\n",
      "\n",
      "# sent_id = Heusaff(1996:30)__1\n",
      "# text = nid oes dim ohono efe 'n canu.\n",
      "# text_fr = 'Il n'est pas en train de chanter.'\n",
      "# location = Morgannwg\n",
      "# source = Heusaff (1996)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tnid\tnid\t?\t_\t_\t_\t_\t_\tGloss=NEG\n",
      "2\toes\toes\tverbe\t_\t_\t_\t_\t_\tGloss=COP\n",
      "3\tdim\tdim\t?\t_\t_\t_\t_\t_\tGloss=NEG\n",
      "4\tohono\tohono\t?\t_\t_\t_\t_\t_\tGloss=P.lui\n",
      "5\tefe\tefe\t?\t_\t_\t_\t_\t_\tGloss=lui\n",
      "6\t'\t'\t?\t_\t_\t_\t_\t_\tGloss=à\n",
      "7\tn\tn\t?\t_\t_\t_\t_\t_\tGloss=chanter\n",
      "8\tcanu.\tcanu.\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "{| class=\"prettytable\"\n",
    "|(1)|| N'eo || ket || dleet || koduiñ || hag || evañ || er || memes || amzer.\n",
    "|-\n",
    "||| [[ne]] [[COP|est]] || [[ket|pas]] || [[dleout|dû]] || conduire || [[&|et]] || [[evañ|boire]] || [[P.e|en]].[[art|le]] || [[memes|même]] || temps\n",
    "|-\n",
    "|||colspan=\"10\" | 'On ne doit pas boire et conduire en même temps /Il ne faut pas boire et conduire' \n",
    "|-\n",
    "|||||||||colspan=\"10\" | ''Lesneven/Kerlouan'', [[Y. M. (04/2016)]]\n",
    "|}\n",
    "\"\"\"\n",
    "\n",
    "dialects = {\"léonard\" : [], \"cornouaillais\" : [], \"trégorrois\": [], \"vannetais\" : [], \"standard\" : [], \"inconnu\" : []}\n",
    "references = {} # keeps track of the references of the sentences: ref:[sentence]\n",
    "sentences = {} # keeps the actual texts\n",
    "dic = {} # token to lemma, pos, gloss\n",
    "errorout = open('breton.errors.txt','w', encoding='utf-8')\n",
    "\n",
    "nopos = open('no_pos.txt','w', encoding='utf-8')\n",
    "\n",
    "currentPage=''\n",
    "\n",
    "unknownPOS = []\n",
    "\n",
    "### EXPRESSIONS REGULIERES\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "rePOS = re.compile(r'verbes|auxiliaires|copules|adverbes|complémenteurs|conjonctions|prépositions|adjectifs|noms|particules verbales|interjections|postpositions|déterminants|quantifieurs|pronoms|noms propres|suffixe', re.IGNORECASE)\n",
    "reDialect = re.compile(r'léonard|cornouaillais|trégorrois|vannetais|standard', re.IGNORECASE)\n",
    "reLanguage = re.compile(r'gallois|moyen gallois|italien|espagnol|hébreu|arabe|français|anglais|allemand', re.IGNORECASE)\n",
    "reType = re.compile(r'ouvrages de recherche|références de corpus|élicitations|ouvrages pédagogiques', re.IGNORECASE)\n",
    "\n",
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "relinks2 = re.compile(r\"\\[\\[.*?\\]\\]\")\n",
    "\n",
    "#print(pages['Konduiñ'])\n",
    "#for key in pages:\n",
    "\t#if key.startswith('A'):\n",
    "\t\t#print(key)\n",
    "\n",
    "# Recherche du pos dans une page titre\n",
    "# la fontion cherche le pos dans toutes les pages selon le lemme donné et retourne le pos associé\n",
    "def getFirstPos(title):\n",
    "\tif not title: return '?'\n",
    "\tmpos='' \n",
    "\tif title.isupper() and title != 'R':\n",
    "\t\tmpos = title   \n",
    "#\tif title == '[[R]]' :   \n",
    "#\t\tmpos='particule_verbale'\n",
    "\tif title == 'P.e':\n",
    "\t\tmpos='préposition'    \n",
    "\telse:        \n",
    "\t#si le titre contient un tiret bas, on le remplace par un espace pour le trouver dans les titres de pages\n",
    "\t\tif '_' in title: \n",
    "\t\t\ttitle = title.replace(\"_\", \" \")\n",
    "#sinon on met une majuscule à la première lettre du titre\n",
    "\t\telse:             \n",
    "\t\t\ttitle = title[0].upper()+title[1:]\n",
    "\t#si le titre est contenu dans le dictionnaire pages\n",
    "\t\tif title in pages:           \n",
    "\t\t\twikicode=''\n",
    "\t\t\t#gérer les redirections (parfois les exemples se trouvent sur d'autres pages)\n",
    "\t\t\tif '#REDIRECTION' in pages[title]:             \n",
    "\t\t\t\t#on suit la redirection\n",
    "\t\t\t\tnewtitle = relinks.search(pages[title]).group(1)                \n",
    "\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\tif '_' in newtitle: \n",
    "\t\t\t\t\tnewtitle = newtitle.replace(\"_\", \" \")            \n",
    "\t\t\t\t#et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\twikicode = pages[newtitle]              \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t# print(8888,newtitle, newtitle in pages)\n",
    "\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnewtitle = newtitle.split(',')[0]\n",
    "\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\terrorout.write('\\n'+currentPage+': strange redirect to page that does not exist: '+newtitle+'\\n')\n",
    "\t\t\telse:\n",
    "\t\t\t\twikicode = pages[title]                \n",
    "                \n",
    "\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "\t\t\t\tcats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)\n",
    "\t\t\t\tlist_cats = set()\n",
    "\t\t\t\tfor cat in cats:\n",
    "\t\t\t\t\tif rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "\t\t\t\t\t\tlist_cats.add(rePOS.search(cat).group(0).rstrip('s'))\n",
    "\t\t\t\t\t\tif len(list_cats) == 1:                                          \n",
    "\t\t\t\t\t\t\tmpos = list(list_cats)[0]\n",
    "\t\t\t\t\t\telif len(list_cats) >0: \n",
    "\t\t\t\t\t\t\tmpos = \"|\".join(list_cats)                            \n",
    "\t\t\tif mpos:\n",
    "\t\t\t\tmpos = mpos\n",
    "\t\t\telse:\n",
    "\t\t\t\tmpos = \"?\"\n",
    "\t\telse:\n",
    "\t\t\tmpos = \"?\"\n",
    "\treturn mpos\n",
    "\n",
    "# Suppression des espaces dans les [[ ]] sinon, le split avec comme délimiteur espace va mal se passer...\n",
    "# case : [[koll|perd]].[[-et (Adj.)|u]]\n",
    "## ATTENTION SI DEUX ESPACES DANS UN MEME [[ ]] n'enlèvera que le premier, à améliorer !!\n",
    "def sansespace(ch):\n",
    "\tfor e in relinks.finditer(ch):\n",
    "\t\tif \" \" in e.group(0):\n",
    "\t\t\tstart, end = e.start(), e.end()\n",
    "\t\t\tnewch = re.sub(r\"\\[\\[(.*?) (.*?)\\]\\]\", \"[[\\\\1_\\\\2]]\", e.group(0))\n",
    "\t\t\tch = ch[:start] + newch + ch[start+len(newch)+1:]\n",
    "\treturn ch\n",
    "#exemple :\n",
    "#print(sansespace(\"[[art|le]] _[[1]]_[[hey j]]_[[maouez | femme]]\"))\n",
    "#resultat : \"[[art|le]] _[[1]]_[[heyj]]_[[maouez| femme]]\")\n",
    "\n",
    "## ATTENTION la mutation +C (consonne épenthétique n'est pas prise encompte), de même que le cas [[R]]_[[+C]],[[4]]_ ou encore le cas _[[1]]_exemple\n",
    "mutation1 = re.compile(r\"_\\[\\[([12345])\\]\\]_\\[\\[(.*)\\]\\]\")\n",
    "mutation2 = re.compile(r\"\\[\\[(.*)\\]\\]_\\[\\[([12345])\\]\\]_\")\n",
    "#test = \"_[[1]]_[[kozh|vieille]]\"\n",
    "#print(mutation1.search(test))\n",
    "\n",
    "groupes_pipe = re.compile(r\"(.*?)\\|(.*)\")\n",
    "# fonction qui prend un token, avec ou sans double crochets [[ ]]\n",
    "# NB : tr = traduction ; t = token\n",
    "def tokentrans2lemposglossmorph(tr, t):\n",
    "\t#print(f't:{t}, tr:{tr}')\n",
    "\t# Rq : morph a par défaut '_'\n",
    "\tlem, pos, gloss, morph = '', '', '', ''\n",
    "\tm = relinks.search(tr)\n",
    "\ttest_mutation1 = mutation1.search(tr)\n",
    "\ttest_mutation2 = mutation2.search(tr)\n",
    "    \n",
    "    # Si c'est une mutation\n",
    "\tif test_mutation1 : \n",
    "\t\t#print(\"yes1\")    \n",
    "\t\tmorph = test_mutation1.group(1) + \" \" + \"target\"\n",
    "\t\ttoken_mute = test_mutation1.group(2)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:          \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:   \n",
    "\t\t\t\tpos = getFirstPos(lem)\n",
    "\t\telse :\n",
    "\t\t\t#print('just a word')\n",
    "\t\t\tlem = t        \n",
    "\t\t\tgloss = token_mute   \n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                          \n",
    "\t\t\t\tpos = getFirstPos(t)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr)\n",
    "\telif test_mutation2 : \n",
    "\t\t#print(\"yes2\")            \n",
    "\t\tmorph = test_mutation2.group(2) + \" \" + \"trigger\"\n",
    "\t\ttoken_mute = test_mutation2.group(1)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem)\n",
    "\t\telse :\n",
    "\t\t\t# print('just a word')\n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\"            \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr)\n",
    "                \n",
    "# si ce n'est pas une mutation et si on a des double crochets [[ ]]              \n",
    "\telif m:\n",
    "\t\t# cas \"amalgames\": exemple [[da|à]].[[pronom incorporé|vous]]\n",
    "\t\t## ATTENTION ne prend pas en compte un amalgame de type coeur.brûle\n",
    "\t\tif ('.[[' or ']].') in tr:     \n",
    "\t\t\tlems, glosss, poss = [], [], []\n",
    "\t\t\tfor mbis in relinks2.finditer(tr):\n",
    "\t\t\t\t# on appelle de façon récursive la fonction sur chaque [[ ]], on les met dans une liste et on les reconcatène avec un \".\"\n",
    "\t\t\t\t#MODIF !!!                \n",
    "\t\t\t\ttry:                \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(mbis.group(0), t)\n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem)]\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\twith open('abc.txt','w', encoding=\"utf-8\") as f :                \n",
    "\t\t\t\t\t\tf.write(f\"PROBLEMEEEEEEEEEEEEE : {tr} et {t}\") \n",
    "\t\t\t\tlem = '.'.join(lems)\n",
    "\t\t\t\tgloss = '.'.join(glosss)\n",
    "\t\t\t\tpos = '.'.join(poss)\n",
    "\t\t\treturn lem, pos, gloss, morph\n",
    "        \n",
    "\t\t# cas \"normal\" : pas d'amalgame, un token et une gloss --> [[token|gloss]]\n",
    "\t\tif '|' in m.group(1):\n",
    "\t\t\tlem = m.group(1).split('|')[0]\n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\"            \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(lem)               \n",
    "\t\t\tgloss = m.group(1).split('|')[1]\n",
    "\t\t\tif pos=='?':\n",
    "\t\t\t\t# ?\n",
    "\t\t\t\tpos = getFirstPos(t)\n",
    "                \n",
    "\t\t# cas sans amalgame mais sans gloss --> [[ne]]\n",
    "\t\telse:            \n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = relinks.search(tr).group(1)\n",
    "\t\t\tif gloss == 'R':  \n",
    "\t\t\t\tpos = \"particule_verbale\"            \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr)      \n",
    "\t# cas d'un mot seul sans crochets --> foot dans la page \"Abardaez,_enderv\"      \n",
    "\telse :\n",
    "\t\t# print('just a word')\n",
    "\t\tlem = t\n",
    "\t\tgloss = tr\n",
    "\t\tif gloss == 'R':  \n",
    "\t\t\tpos = \"particule_verbale\"            \n",
    "\t\telse:\n",
    "\t\t\tpos = getFirstPos(t)\n",
    "\t\tif pos == '?':\n",
    "\t\t\tpos = getFirstPos(tr)\n",
    "\t#print('__lem, pos, gloss:', lem, pos, gloss)\n",
    "\treturn lem, pos, gloss, morph\n",
    "\n",
    "\n",
    "# Nettoyage des tokens\n",
    "def cleanToken(t):\n",
    "\t## ATTENTION ''' ''' veut dire que le pos du token est sur la page même ! peut être à utiliser..   \n",
    "\tt = t.replace(\"'''\", \"\")\n",
    "\tt = t.replace(\"''\", \"\")\n",
    "\tt = re.sub(r'</?font.*?>','',t)\n",
    "\tt = re.sub(r'<sup>','_',t)\n",
    "\tt = re.sub(r'</sup>','_',t)\n",
    "\tt = re.sub(r'<u>','',t)\n",
    "\tt = re.sub(r'</u>','',t)\n",
    "\tt = re.sub(r'<sub>.*?</sub>','',t)\n",
    "\tt = re.sub(r'\\(\\[\\[\\*\\]\\].*?\\)','$$$',t)\n",
    "\tt = re.sub(r\"c'h\",'cxxxh',t)\n",
    "\tt = re.sub(r\"C'h\",'Cxxxh',t)\n",
    "\t## ne pas splitter quand l'apostrophe est en début de mot (Rannig) -> pas réussi avec la regex    \n",
    "\t#if t.startswith(\"'\"):    \n",
    "\t\t#t = re.sub(r\"'([^ ])\",\"%\\\\1\",t)\n",
    "        \n",
    "\t# pour spliter sur les apostrophes sans les enlever : mettre @ pour spliter sur @ et donc conserver l'apostrophe\n",
    "\t##ATTENTION guillemet en début de mot puis token collés mal fait \n",
    "\tt = re.sub(r\"'\",\"'@\",t)\n",
    "\treturn t.strip()\n",
    "\n",
    "#test\n",
    "#print(cleanToken(\"'anava\"))\n",
    "\n",
    "# Remettre les tokens qui ont été remplacés\n",
    "def remettretoken(t):\n",
    "\tt = re.sub(r'cxxxh',\"c'h\",t)\n",
    "\tt = re.sub(r'Cxxxh',\"C'h\",t)\n",
    "\t#t = re.sub(r\"%\",\"'\",t)\n",
    "\tt = re.sub(r\"'@\", \"'\",t)\n",
    "\tt = re.sub(r\"\\$\\$\\$ \", \"\",t)\n",
    "\treturn t\n",
    "\n",
    "# Savoir si une string contient un nombre\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def analyzePage(title): \n",
    "\tcurrentPage = title\n",
    "\tmpos = rePOS.search(pages[title])\n",
    "\tpagepos = None\n",
    "\tconlls=[]\n",
    "\tgrammatical = True\n",
    "\ts= ''\n",
    "\tif mpos:\n",
    "\t\t# mets tout en minuscule\n",
    "\t\tpagepos = mpos.group(0).lower()\n",
    "\t\t# print('pagepos',pagepos)\n",
    "\t\t\t\t\t\n",
    "\t#pour chaque tableau (et donc chaque exemple)        \n",
    "\tfor m in repretty.finditer(pages[title]):\n",
    "\t\t# print('\\n\\n_______________',m.group(1))\n",
    "\t\ttokens, trans = [], []\n",
    "\t\ttranslation, source, source2 = '', '', ''\n",
    "\t\tlocation, dialect, phonetic, texttype, l = '','','', '', ''\n",
    "\t\tgrammatical=True         \n",
    "\t\t# si le tableau est \"standard\" avec une ligne de token, une ligne de glose et une ligne source\n",
    "\t\tif (len(m.group(1).split('\\n'))) == 9 :\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table\n",
    "\t\t\t\tif li[:2]=='|(' :\n",
    "\t\t\t\t\tif '[[*]]' in li.split('||')[0]:\n",
    "\t\t\t\t\t\tgrammatical = False                       \n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\tif '[' in tokens[0]:\n",
    "\t\t\t\t\t\ttokens[0] = tokens[0].replace('[', '').strip()\n",
    "\t\t\t\t\t\t#phonetic[0] = phonetic[0].replace('[', '')         \n",
    "\t\t\t\t\tif ']' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "\t\t\t\t\t\t#phonetic[-1] = phonetic[-1].replace(']', '')\n",
    "\t\t\t\t\t#MODIF !!!                    \n",
    "\t\t\t\t\tif tokens[0]==\"[[*]]\" :\n",
    "\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\tbreak                       \n",
    "\t\t\t\t#modifier cette condition parc\n",
    "\t\t\t\tif li[:3]=='|||' and grammatical:\n",
    "\t\t\t\t\tif not trans: #la première fois = la traduction\n",
    "\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]\n",
    "\t\t\t\t\t\t# print(len(tokens), len(tokens)==len(trans))\n",
    "\t\t\t\t\telif 'colspan=' in li:\n",
    "\t\t\t\t\t\t# vu que les deux lignes translation et source remplissent cette condition, quand on arrive à source\n",
    "\t\t\t\t\t\t# la string translation est remplie donc on n'écrase pas et on passe à source\n",
    "\t\t\t\t\t\tif not translation:\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\telif not source: # si déjà trans, alors on a la source\n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'').replace(' ','')\n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", li):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", li):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif re.search(r\"\\[\\[(.*?)\\]\\]\", li):                 \n",
    "\t\t\t\t\t\t\t\tl = re.search(r\"\\[\\[(.*?)\\]\\]\", li).group(0) \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                   \n",
    "\t\t\t\t\t\t\t\tl = str(l).split('|')[0].replace('[[', '').replace(']]', '').replace(\"'\", \"\")\n",
    "\t\t\t\t\t\t\t\tif l in pages:              \n",
    "\t\t\t\t\t\t\t\t\tprint(l)                                    \n",
    "\t\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                      \n",
    "\t\t\t\t\t\t\t\t\t\t#on suit la redirection                                        \n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)\n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\t\ts = l                                    \n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                 \n",
    "\t\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "\t\t\t\t\t\t\t\t\t\tif not dialect: \n",
    "\t\t\t\t\t\t\t\t\t\t\tif reDialect.search(source):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = reDialect.search(source).group(0)\n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\ts = l\n",
    "\t\t\t\t\t\t\t# print(111,source)\n",
    "\t\t\t# print(tokens, trans, translation, source)\n",
    "\t\t\tif not( tokens and trans and translation and source): # if one is missing\n",
    "\t\t\t\t#if verbose: print('Houston, we got a problem: The number of tokens and the number of transcriptions differ, or translation or source are missing:', title) \n",
    "\t\t\t\terrorout.write('\\n'+currentPage+': The number of tokens and the number of transcriptions differ or translation or source are missing:'+m.group(1)+'\\n')\n",
    "\t\t\t\tcontinue\n",
    "\t\t# S'il y a plus de lignes que prévu : par ex une ligne de phonétique ou deux lignes source, ou les deux\n",
    "\t\telse:\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table\n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==False:\n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\t\t#tokens[0] = tokens[0].replace('[', '')         \n",
    "\t\t\t\t\tif ']' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '')\n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==True:\n",
    "\t\t\t\t\tphonetic = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\tclean_phonetic = ''\n",
    "\t\t\t\t\tif '[' in phonetic[0]: \n",
    "\t\t\t\t\t\tphonetic[0] = phonetic[0].replace('[', '')         \n",
    "\t\t\t\t\tif ']' in phonetic[-1]: \n",
    "\t\t\t\t\t\tphonetic[-1] = phonetic[-1].replace(']', '')\n",
    "\t\t\t\t\tfor ele in phonetic:        \n",
    "\t\t\t\t\t\tclean_phonetic += ele\n",
    "\t\t\t\t\t\tclean_phonetic += ' '\n",
    "\t\t\t\t#modifier cette condition parc\n",
    "\t\t\t\tif li[:3]=='|||':\n",
    "\t\t\t\t\t# si on a rencontré une ligne phonétique et donc qu'on a pas encore de tokens, cette ligne (2e ligne) est la ligne de tokens                   \n",
    "\t\t\t\t\tif not tokens :\n",
    "\t\t\t\t\t\ttokens = [cleanToken(t) for t in li[3:].split('||')]                \n",
    "\t\t\t\t\t\t#print(tokens)\n",
    "\t\t\t\t\telif not trans: #la première fois = la traduction\n",
    "\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]\n",
    "\t\t\t\t\t\t# print(len(tokens), len(tokens)==len(trans))\n",
    "\t\t\t\t\telif 'colspan=' in li:\n",
    "\t\t\t\t\t\tif li.startswith('|||colspan='):\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\telif not source:                          \n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')\n",
    "\t\t\t\t\t\telif not source2:\n",
    "\t\t\t\t\t\t\tsource2 = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')                         \n",
    "\t\t\t\t\t\tif source and source2:\n",
    "\t\t\t\t\t\t\tsource, source2 = source2, source\n",
    "\t\t\t\t\t\t\t### ATTENTION ICI CONTINUER, REMETTRE COMME DANS PLUS HAUT : Dialecte etc.\n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", source):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", source):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", source)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif re.search(r\"\\[\\[(.*?)\\]\\]\", source):                 \n",
    "\t\t\t\t\t\t\t\tl = re.search(r\"\\[\\[(.*?)\\]\\]\", source).group(0) \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", source).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                   \n",
    "\t\t\t\t\t\t\t\tl = str(l).split('|')[0].replace('[[', '').replace(']]', '').replace(\"'\", \"\")\n",
    "                            \n",
    "\t\t\t\t\t\t\t\tif l in pages:                                 \n",
    "\t\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                                       \n",
    "\t\t\t\t\t\t\t\t\t\t#on suit la redirection\n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)                \n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\t\ts = l                                              \n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                 \n",
    "\t\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "\t\t\t\t\t\t\t\t\t\tif not dialect: \n",
    "\t\t\t\t\t\t\t\t\t\t\tif reDialect.search(source):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = reDialect.search(source).group(0)\n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\ts = l\n",
    "\n",
    "                            \n",
    "                            \n",
    "\t\t\t# print(tokens, trans, translation, source)\n",
    "\t\t\tif not( tokens and trans and translation and source): # if one is missing\n",
    "\t\t\t\t#if verbose: print('Houston, we got a problem: The number of tokens and the number of transcriptions differ, or translation or source are missing:', title) \n",
    "\t\t\t\terrorout.write('\\n'+currentPage+': The number of tokens and the number of transcriptions differ or translation or source are missing:'+m.group(1)+'\\n')\n",
    "\t\t\t\tcontinue            \n",
    " \n",
    "\n",
    "\n",
    "\t\t# construction of the conll:         \n",
    "\t\ttext = ' '.join(tokens)\n",
    "\t\ttext_ch = remettretoken(text)        \n",
    "\t\t# print(text, text_ch)\n",
    "\t\treferences[source] = references.get(source,[])\n",
    "\t\tif text in references[source]:\n",
    "\t\t\tind = references[source].index(text)+1\n",
    "\t\telse:\n",
    "\t\t\treferences[source]+=[text]\n",
    "\t\t\tind = 1\n",
    "\t\tsentences[text] = sentences.get(text,0)+1\n",
    "\t\tconllis = ['# sent_id = '+source.replace(' ', '') +'__'+str(ind)]\n",
    "\t\tconllis += ['# text = '+text_ch]\n",
    "\t\t#MODIF !!! \n",
    "\t\tconllis += ['# text_fr = '+ remettretoken(cleanToken(translation))]\n",
    "\t\tif phonetic:\n",
    "\t\t\tconllis += ['# text_phon = '+clean_phonetic]\n",
    "\t\tif dialect:\n",
    "\t\t\tconllis += ['# dialect = ' + dialect]\n",
    "\t\tif location:\n",
    "\t\t\tconllis += ['# location = ' + location]\n",
    "\t\tif source2:\n",
    "\t\t\tconllis += ['# source2 = ' + source2]\n",
    "\t\tif s :\n",
    "\t\t\tconllis += ['# source = ' + s] \n",
    "\t\tif texttype :\n",
    "\t\t\tconllis += ['# texttype = ' + texttype]   \n",
    "\t\t##si on a trouvé du vert alors conllis += ['# phonetic = yes ']\n",
    "\t\t##si un morceau en phonétique alors mettre en trait MISC\n",
    "\n",
    "\t\textrai = 1\n",
    "\t\t# si un token en trop dans les tokens, alors enlever le dernier \n",
    "\t\t# case : ||| N'eo || ket || dereat || teañ || e || dad. || ''Équivalent standardisé''\n",
    "\t\tif len(tokens) == len(trans)+1:\n",
    "\t\t\ttokens = tokens[:-1] \n",
    "            \n",
    "\t\tfor i,t in enumerate(tokens):\n",
    "\t\t\tif not t:\n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': strange token nr '+str(i)+' in '+m.group(1))\n",
    "\t\t\t\tt='???'\n",
    "\t\t\tif len(trans)<=i:\n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': strange token nr '+str(i)+' in '+m.group(1))\n",
    "\t\t\t\ttr='???'\n",
    "\t\t\telse:\n",
    "\t\t\t\ttr = trans[i]\n",
    "                \n",
    "\t\t\t# délimiteur : @ et espace \n",
    "\t\t\t# ATTENTION : tiret en délimiteur non traité pour l'instant !! (faire la même chose que la fonction sansespace, il faut splitter)\n",
    "\t\t\t# sur les tirets EN DEHORS des doubles crochets, non ceux qui sont à l'intérieur          \n",
    "\t\t\ttest1 = re.split(\"@| \", t)\n",
    "\t\t\ttest2 = sansespace(tr).split(\" \")\n",
    "\t\t\t#print(test1)\n",
    "\t\t\t# si on a le même nombre d'éléments pour les tokens et la glose entre chaque || ||\n",
    "\t\t\tif len(test1) == len(test2):\n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]\n",
    "\t\t\t\t\ttr1 = test2[j]  \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tif t1[-1] not in \".,;!?\" and t1 != \"$$$\": \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1)\n",
    "\t\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]# et morph ?\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+gloss]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "                        \n",
    "\t\t\t\t\t# si dernier caractère est une ponctuation attention \"...\" non traité \n",
    "\t\t\t\t\t\tif t1[-1] in \".,;!?\":\n",
    "\t\t\t\t\t\t\tif t1[-3:] == \"...\" :                            \n",
    "\t\t\t\t\t\t\t\tpunct = 3                   \n",
    "\t\t\t\t\t\t\tif t1[-1] in \".,;!?\" and not t1[-3:] == \"...\" :\n",
    "\t\t\t\t\t\t\t\tpunct = 1\n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1[:-punct])\n",
    "\t\t\t\t\t\t\t#si on ne trouve pas de pos, on cherche dans la page elle-même\n",
    "\t\t\t\t\t\t\tif pos=='?' and t1[:-punct].lower()==title.lower() and pagepos:\n",
    "\t\t\t\t\t\t\t\t# couldn't find a pos, maybe it's the word of the page itself?\n",
    "\t\t\t\t\t\t\t\tpos = pagepos\n",
    "\t\t\t\t\t\t\tdic[t1[:-punct]] = dic.get(t1[:-punct], [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[:-punct]), remettretoken(lem), pos]+5*['_']+['Gloss='+gloss]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += punct\n",
    "\t\t\t\t\t\t\tdic[t1[-punct:]] = dic.get(t1[-punct:], [(t1[-punct:], 'PUNCT','punct')])\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[-punct:]), remettretoken(t1[-punct:]), 'PUNCT']+4*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += punct                       \n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tcontinue                        \n",
    "                        \n",
    "\t\t\t# si on a le nombre d'éléments de tokens est supérieur à celui de la glose                  \n",
    "\t\t\telif len(test1)>len(test2):\n",
    "\t\t\t\t#print(f'PROBLEME : {test1} longueur {len(test1)}, {test2} longueur {len(test2)}')\n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]\n",
    "\t\t\t\t\tif t1 == \"$$$\":\n",
    "\t\t\t\t\t\tprint(\"AAAAAA\")                        \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\ttr1 = test2[j]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\ttr1 = \"?\"\n",
    "                        \n",
    "\t\t\t\t\t# si la ponctuation est tout simplement séparée du token                   \n",
    "\t\t\t\t\tif t1 in \"/-.,;!?\" or t1 == \"...\":\n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [(t1, 'PUNCT','punct')])\n",
    "\t\t\t\t\t\teles = [str(extrai),t1, t1, 'PUNCT']+4*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\textrai += 1\n",
    "\t\t\t\t\t# les autres tokens sont traités normalement\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1)\n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+gloss]\n",
    "\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\textrai += 1\n",
    "\n",
    "\t\t\t# si on a le nombre d'éléments de la glose est supérieur à celui des tokens \n",
    "\t\t\telif len(test1)<len(test2):\n",
    "\t\t\t\tprint(\"PROBLEME ! éléments de la glose esupérieur à celui des tokens\")                \n",
    "\t\t\t\t#errorout.write('\\n___ '+currentPage+': strange token nr ' +str(i)+' in '+m.group(1) + 'test1' + str(len(test1)) + 'test2' + str(len(test2)))             \n",
    "\n",
    "\t\tif any(\"# dialect\" in string for string in conllis):         \n",
    "\t\t\tfor d in dialects:\n",
    "\t\t\t\tif f'# dialect = {d.capitalize()}' in conllis: \n",
    "\t\t\t\t\tdialects[d] += [conllis]  \n",
    "\t\telse: \n",
    "\t\t\tdialects['inconnu'] += [['\\n'.join(conllis)]]             \n",
    "            \n",
    "#\t\tconlls += ['\\n'.join(conllis)]\n",
    "\treturn dialects\n",
    "\n",
    "print('____________\\n')\n",
    "\n",
    "#for conll in analyzePage('Konduiñ'):\n",
    "analyzePage('A')\n",
    "for d in dialects:\n",
    "\tfor conll in dialects[d]:   \n",
    "\t\tprint('\\n'.join(conll) + '\\n\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11e098-4b26-46f1-8e26-43ec010492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b86d6f-2c73-46c4-8a22-824d7e9848dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for conll in analyzePage('Memes'):\n",
    "\tprint(conll,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6f25-c079-4dc0-9cee-e0654bf29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071bfc38-e641-4032-94d6-9510ceffcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDic():\n",
    "\tout = open('breton.dic.tsv','w', encoding=\"utf-8\")\n",
    "\tfor t, lpg in sorted(dic.items()):\n",
    "\t\tlpg = sorted(set(lpg))\n",
    "\t\t\n",
    "\t\tlines = ['\\t'.join([t,l,p,g]) for l,p,g in lpg]\n",
    "\t\tout.write('\\n'.join(lines)+'\\n')\n",
    "writeDic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb22511-47ca-492d-9e84-a842e73c638f",
   "metadata": {
    "id": "16b61a38"
   },
   "source": [
    "# trying to run through the whole site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                              | 26/8068 [00:00<00:32, 249.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                              | 51/8068 [00:00<00:39, 205.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                              | 88/8068 [00:00<00:29, 267.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                            | 152/8068 [00:00<00:27, 285.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                            | 182/8068 [00:00<00:28, 273.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                            | 210/8068 [00:00<00:31, 248.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                           | 264/8068 [00:00<00:23, 326.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                          | 355/8068 [00:01<00:15, 485.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                          | 406/8068 [00:01<00:24, 309.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                         | 478/8068 [00:01<00:39, 190.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                         | 505/8068 [00:02<00:51, 148.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "dic = {}\n",
    "errorout = open('breton.errors.txt','w', encoding=\"utf-8\")\n",
    "nopos = open('no_pos.txt', 'w', encoding='utf-8')\n",
    "for title in tqdm.tqdm(pages):\n",
    "\t# print('____________',title)\n",
    "\tanalyzePage(title)  \n",
    "for d in dialects:\n",
    "\tfor conll in dialects[d]:   \n",
    "\t\topen('bretonconlls/'+re.sub(r'\\W','_',d)+'.conllu','a', encoding=\"utf-8\").write('\\n'.join(conll) + '\\n\\n')    \n",
    "#\tif conlls:\n",
    "#\t\topen('bretonconlls/'+re.sub(r'\\W','_',title)+'.conllu','w', encoding=\"utf-8\").write('\\n\\n'.join(conlls))\n",
    "#writeDic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cbc9b-7747-42cf-90b0-834bfb7a4f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
