{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
   "metadata": {
    "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
    "tags": []
   },
   "source": [
    "# exporting the Breton example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6",
   "metadata": {
    "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6"
   },
   "outputs": [],
   "source": [
    "import datetime, tqdm, re, time, string\n",
    "# !pip install mwclient\n",
    "# !pip install mwparserfromhell\n",
    "# !pip install mwcomposerfromhell\n",
    "from mwclient import Site\n",
    "#myparserfromhell : parser pour le code wiki\n",
    "import mwparserfromhell, mwcomposerfromhell\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f90d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "got all 9140 pages\n"
     ]
    }
   ],
   "source": [
    "print('connecting...')\n",
    "#site contenant tous les exemples\n",
    "site = Site('arbres.iker.cnrs.fr/', path='/')\n",
    "outf = open('problemPages.html','w')\n",
    "allpages = list(site.allpages())\n",
    "print('got all', len(allpages), 'pages')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
   "metadata": {
    "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
    "tags": []
   },
   "source": [
    "#### Charger toutes les pages du site: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a6e0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9140/9140 [28:52<00:00,  5.28it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction done. it took 1732.5992450714111 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "#dictionnaire contenant toutes les pages aspirées\n",
    "pages = {}\n",
    "#on peut ici modifier le nombre de pages à charger pour charger petit à petit ex : allpages[:3000] pour les 3000 premières pages\n",
    "# newlist=allpages\n",
    "for p in tqdm.tqdm(allpages):\n",
    "# for p in tqdm.tqdm(newlist):\n",
    "    #si les pages ne sont pas de la documentation\n",
    "    if not('/documentation' in p.name or '/docname' in p.name or 'Module:' in p.name):\n",
    "        #on ajoute le nom de la page comme clé, son contenu comme valeur\n",
    "        pages[p.name] = p.text() \n",
    "print('extraction done. it took', time.time()-t, 'seconds.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
   "metadata": {
    "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
    "tags": []
   },
   "source": [
    "#### Compter tous les exemples trouvés dans les pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d0685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour enregistrer les pages chargées dans un fichier Pickle: \n",
    "# une fois toutes les pages chargées, on le met dans un pickle\n",
    "with open(\"Pages.pickle\", \"wb\") as myFile:\n",
    "   pickle.dump(pages, myFile)\n",
    "with open(\"Pages.txt\", \"w\") as myFile:\n",
    "    for title,text in pages.items():\n",
    "        myFile.write(title+'\\n'+text+'\\n------------------------\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
    "outputId": "119a6b13-11d3-41cc-e22e-302399c2a06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 9140 pages\n"
     ]
    }
   ],
   "source": [
    "# print(', '.join(pages.keys()))\n",
    "#on créé un fichier txt contenant tous les titres de pages\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7221879-cd08-49de-990c-73a09bf3550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9da780d9",
   "metadata": {},
   "source": [
    "## wiki2lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39d52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki2lines(wikitext, title):\n",
    "    \n",
    "    phonetic, text, gloss, standardise, posgloss, translation, info = None, None, None, None, None, None, None\n",
    "    rest = ''\n",
    "    for i,line in enumerate(wikitext.split('\\n')):\n",
    "        if line.strip().startswith('|-') or 'prettytable' in line: \n",
    "            continue\n",
    "        if line[:2]=='|(' and bool(re.search(r'</?font.*?>',line))==True and not phonetic:\n",
    "            phonetic = line[1:]\n",
    "        elif re.match(r'^\\|', line) and not text:\n",
    "            text = line[1:]\n",
    "        elif 'Équivalent standardisé' in line and not standardise:\n",
    "            standardise = line[1:]\n",
    "        elif text and '[[' in line and not posgloss and not 'colspan' in line:\n",
    "            posgloss = line[1:]\n",
    "        elif text and re.match(r'^\\|', line) and not posgloss and not gloss:\n",
    "            gloss = line[1:]\n",
    "        elif 'colspan' in line and not translation:\n",
    "            split_result = re.split(r'colspan=\"\\d+\".*?\\|\\s*', line)\n",
    "            if len(split_result) > 1:\n",
    "                translation = split_result[1].strip()\n",
    "        elif 'colspan' in line and not info:\n",
    "            split_result = re.split(r'colspan=\"\\d+\".*?\\|\\s*', line)\n",
    "            if len(split_result) > 1:\n",
    "                info = split_result[1].strip()\n",
    "        elif len(line) > 2:\n",
    "            rest+=line\n",
    "    \n",
    "    obj = {'title':title,'phonetic':phonetic,'text':text, 'gloss':gloss, 'standardise':standardise,'posgloss':posgloss,'translation':translation, 'info':info}\n",
    "    obj = info2catsource(obj)\n",
    "\n",
    "    if rest:\n",
    "        return {**obj, 'rest':rest}\n",
    "\n",
    "    return obj\n",
    "\n",
    "def prettyprint(item):\n",
    "    for key, value in item.items():\n",
    "            print('____',key)\n",
    "            print(str(value) )\n",
    "    print( '__________________\\n')\n",
    "\n",
    "def info2catsource(obj):\n",
    "    line_info = obj['info']\n",
    "    if not line_info:\n",
    "        return obj\n",
    "        \n",
    "    categories, sources, source1, source2 = None, None, None, None\n",
    "    if line_info.startswith(\"''\"):\n",
    "        line_split = line_info.split(\"'', \")\n",
    "        categories = line_split[0][2:]\n",
    "        if len(line_split) > 1:\n",
    "            sources = line_split[1]\n",
    "    elif line_info.startswith(\"[[\"):\n",
    "        sources = line_info\n",
    "    else:\n",
    "        print('Too many items: ', line_info)\n",
    "    # return object with categories and sources\n",
    "    return {**obj, 'categories':categories, 'sources':sources} # **obj = spread obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e4a0f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'test',\n",
       " 'phonetic': \"(2)||<font color=green> ër  mouéz'''i''' ||<font color=green> a ||<font color=green> gae ||<font color=green>dë ||<font color=green> chèrr ||<font color=green> bélurèd ||<font color=green> d'ër saorn ||<font color=green> drant ma gae ||<font color=green> ar baotrèd ||<font color=green> d'ër mor\",\n",
       " 'text': \"|| Ar maouezi || a || yae || da || serriñ || pelured || d'ar sadorn || tra ma yae || ar baotred || d'ar mor.\",\n",
       " 'standardise': \"|| [[art|le]] [[maouez|femme]].s || [[R]]<sup>[[1]]</sup> || [[mont|allait]] || [[da|pour]]<sup>[[1]]</sup> || [[serriñ|ramasser]] || [[pelurenn|palourde]].[[-ed (PL.)|s]] || [[da|pour]]<sup>[[1]]</sup> [[art|le]] [[sadorn|samedi]] || [[durant|pendant]] [[ma|que]]<sup>[[4]]</sup> [[mont|allait]] || [[art|le]] [[paotr|homme]].[[-ed (PL.)|s]] || [[da|pour]]<sup>[[1]]</sup> [[art|le]] [[mor|mer]] |||||| ''Équivalent standardisé''\",\n",
       " 'posgloss': None,\n",
       " 'translation': \"'Les femmes allaient ramasser des palourdes le samedi pendant que les hommes allaient en mer.'\",\n",
       " 'info': \"''Vannetais (Séné)'', [[Le Ruyet (2012b)]]\",\n",
       " 'categories': \"''Vannetais (Séné)''\",\n",
       " 'sources': '[[Le Ruyet (2012b)]]'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "obj = wiki2lines(\"\"\"{{| class=\"prettytable\"\n",
    "|(2)||<font color=green> ër  mouéz'''i''' ||<font color=green> a ||<font color=green> gae ||<font color=green>dë ||<font color=green> chèrr ||<font color=green> bélurèd ||<font color=green> d'ër saorn ||<font color=green> drant ma gae ||<font color=green> ar baotrèd ||<font color=green> d'ër mor\n",
    "|-\n",
    "||| Ar maouezi || a || yae || da || serriñ || pelured || d'ar sadorn || tra ma yae || ar baotred || d'ar mor.\n",
    "|-\n",
    "||| [[art|le]] [[maouez|femme]].s || [[R]]<sup>[[1]]</sup> || [[mont|allait]] || [[da|pour]]<sup>[[1]]</sup> || [[serriñ|ramasser]] || [[pelurenn|palourde]].[[-ed (PL.)|s]] || [[da|pour]]<sup>[[1]]</sup> [[art|le]] [[sadorn|samedi]] || [[durant|pendant]] [[ma|que]]<sup>[[4]]</sup> [[mont|allait]] || [[art|le]] [[paotr|homme]].[[-ed (PL.)|s]] || [[da|pour]]<sup>[[1]]</sup> [[art|le]] [[mor|mer]] |||||| ''Équivalent standardisé''\n",
    "|- \n",
    "||| colspan=\"15\" | 'Les femmes allaient ramasser des palourdes le samedi pendant que les hommes allaient en mer.' \n",
    "|- \n",
    "||||||||| colspan=\"15\" | ''Vannetais (Séné)'', [[Le Ruyet (2012b)]]  \n",
    "|}\"\"\", 'test')\n",
    "prettyprint(obj)\n",
    "info2catsource(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f274024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3094/9140 [00:00<00:00, 15501.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996).\n",
      "Too many items:  Le Roux (1915)\n",
      "Too many items:  Le Roux (1915)\n",
      "Too many items:  Klerg\n",
      "Too many items:  Le Roux\n",
      "Too many items:  '[[*]] Certains couraient venant de la gauche, d'autres venant de la droite.'\n",
      "Too many items:  Kervella, 253\n",
      "Too many items:  Guillam ar Borgn, ''Le Scorff''\n",
      "Too many items:  Interview Annie Ebrel, ''Le Poher Hebdo'' [04/2009]\n",
      "Too many items:  'La récréation est finie ?' (une heure après la fin de la récréation)\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996)\n",
      "Too many items:  Y. Drezen\n",
      "Too many items:  Taldir\n",
      "Too many items:  > Dépêchons-nous.\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  Hingant (1868:[[Hingant (1868:§139-146)|§139-146]])\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996)\n",
      "Too many items:  Chanson traditionnelle, ''[http://pagesperso-orange.fr/per.kentel/seizenn3.htm an teir seizenn]'', Kanomp Uhel, Coop Breizh.\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  traduction de ''Frère Jacques'' par R. Hemon\n",
      "Too many items:  Le Roux\n",
      "Too many items:  Chalm (2008:§Q6)\n",
      "Too many items:  'Moi, je prétends que l'instituteur a raison.' |||| ''Français standard''\n",
      "Too many items:  … et non pas [[*]] 'L'homme qui est ici.'\n",
      "Too many items:  … et non pas [[*]] 'L'homme travaille.'\n",
      "Too many items:  cité dans [[Le Gléau (2000b)|Le Gléau (2000b]]:461)\n",
      "Too many items:  Naig Rozmor, introduction de ''Ar Vastardez'', par Lagadec\n",
      "Too many items:  '(En effet) Elle fait au moins 60m de long et 30 de large.'\n",
      "Too many items:  Fleuriot (2001:20)\n",
      "Too many items:  F. Broudig, ''Kenavo d'am labour'', [http://www.langue-bretonne.com/Pajennou/kenavoLabour.html texte].\n",
      "Too many items:  Fañch Elies Abeozen (1991:45)\n",
      "Too many items:  Hemon, ''An tri boulomig kalon aour'':27\n",
      "Too many items:  'Mais si ! il est là !'\n",
      "Too many items:  M.Jouitteau [05.2009]\n",
      "Too many items:  'Est-ce que les jeunes autrefois devaient frotter sans conclure ?'\n",
      "Too many items:  'Il s'en faut de peu qu'il soit prêt !'\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996)\n",
      "Too many items:  (à propos de partenaire amoureux)\n",
      "Too many items:  Y. Drezen\n",
      "Too many items:  Kadoudal 1961, ''[[Bleun-Brug]]'' 129, p.24\n",
      "Too many items:  cité dans [[Le Gléau (1973)|Le Gléau (1973]]:42)\n",
      "Too many items:  Titre de Jakez (1997), ''[[Hor Yezh]]'' 210: 33-35.\n",
      "Too many items:  'titre de chapitre', [[Herrieu (1974)]]\n",
      "Too many items:  Le Roux (1915)\n",
      "Too many items:  'Alors, il part (de suite) ou il vient ?'\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996).\n",
      "Too many items:  Taldir\n",
      "Too many items:  Y. Gourmelon, rapporté dans [[Favereau (1997)|Favereau (1997]]:§238)\n",
      "Too many items:  Roc'halan\n",
      "Too many items:  = '2 yeux, 6 oreilles' (et non pas '4 yeux, 12 oreilles')\n",
      "Too many items:  (et non pas 'Je mesure deux fois la paie')\n",
      "Too many items:  (et non pas 'de retourner deux fois à la maison/ de faire un aller/retour')\n",
      "Too many items:  cité dans [[Le Gléau (1973)|Le Gléau (1973]]:42)\n",
      "Too many items:  cité dans [[Le Gléau (1973)|Le Gléau (1973]]:42)\n",
      "Too many items:  cité dans Van Olmen (2018)\n",
      "Too many items:  '… il leur appartenait d'y rester.'\n",
      "Too many items:  ...et elles vivaient là et faisaient des sabots pendant la journée.\n",
      "Too many items:  adapté de [[Favereau (1997)|Favereau (1997]]:§200)\n",
      "Too many items:  cité dans [[Menard (1995)|Menard (1995]]:150)\n",
      "Too many items:  cité dans [[Menard (1995)|Menard (1995]]:148)\n",
      "Too many items:  cité dans [[Kervella (1947)|Kervella (1947]]:§744)\n",
      "Too many items:  traduction de Hanotte, X. 2000. ''Derrière la colline'', Belfond.\n",
      "Too many items:  cité dans [[Menard (1995)|Menard (1995]]:157)\n",
      "Too many items:  cité dans [[Le Gléau (2000b)|Le Gléau (2000b]]:461)\n",
      "Too many items:  J. Riou, cité par [[Kervella (1995)|Kervella (1995]]:§704)\n",
      "Too many items:  Taldir,\n",
      "Too many items:  Mar-'Yvon Menguy — Porz-Gwenn, 1895\n",
      "Too many items:  adapté de [[Favereau (1997)|Favereau (1997]]:§200)\n",
      "Too many items:  'B: Oui, il n'a pas une très bonne mine (Il ne marche pas très bien).'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 4736/9140 [00:00<00:00, 15637.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many items:  'être illettré'\n",
      "Too many items:  Davalan\n",
      "Too many items:  Comes (1981:25)\n",
      "Too many items:  Comes (1981:27)\n",
      "Too many items:  Comes (1981:33)\n",
      "Too many items:  Comes (1981:48)\n",
      "Too many items:  Jonathan 2, p.11\n",
      "Too many items:  Jonathan 2, p.41\n",
      "Too many items:  Jonathan 4, p.38\n",
      "Too many items:  Jonathan 7, p.29\n",
      "Too many items:  Jonathan 2, p.25\n",
      "Too many items:  Jonathan 4, p.28\n",
      "Too many items:  traducteur Derib (1982:32)\n",
      "Too many items:  Duval (1982), citée dans [[Timm (1995)|Timm (1995]]:4)\n",
      "Too many items:  'Je ne sais pas.'\n",
      "Too many items:  titre d'histoire, [http://www.portaildupatrimoineoral.org/index.php/record/view/224665 Laudren 1991]\n",
      "Too many items:  'Moi, je prétends que l'instituteur a raison.' |||| ''Français standard''\n",
      "Too many items:  [http://www.alliamm.com ''Al Liamm''] (347:123)\n",
      "Too many items:  Hingant (1868:[[Hingant (1868:§139-146)|§139-146]])\n",
      "Too many items:  Le Roux (1915)\n",
      "Too many items:  'Est-ce que les jeunes autrefois devaient frotter sans conclure ?'\n",
      "Too many items:  Klerg\n",
      "Too many items:  [https://alliamm.bzh/ ''Al Liamm''] (346:115)\n",
      "Too many items:  et pas … [[*]] ''e oan o kas kaout'', \"car ce serait comme si tu allais la chercher\", ''Scaër'', [[H. Gaudart (03/2017)]]\n",
      "Too many items:  (Se dit d'un jour gris, pluvieux ou brumeux, où le ciel semble écraser la terre)\n",
      "Too many items:  (un travail mal fait, où manque l'essentiel)\n",
      "Too many items:  'Moi, j'ai envie de dire que la vérité est avec l'instituteur.' |||||| ''Treger French'', [[Gros (1984)|Gros (1984]]:176)\n",
      "Too many items:  citant Trépos et Kervella (citation non identifiée)\n",
      "Too many items:  et de sa main gantée de noir, désigne une place au frère Arturo.'\n",
      "Too many items:  [http://www.alliamm.com ''Al Liamm''] (347:123)\n",
      "Too many items:  'Peu m'importe ce qu'on fera.'\n",
      "Too many items:  Y. Gourmelon, rapporté dans [[Favereau (1997)|Favereau (1997]]:§238)\n",
      "Too many items:  'Yann souffre d'un problème respiratoire.'\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996).\n",
      "Too many items:  Kervella (1995[[Kervella (1995:§234.III.)|:§234.III.)]]\n",
      "Too many items:  Taldir\n",
      "Too many items:  l.671 des ''[[NG.|Noueloù Gwened]]'', [[Hemon (1956)|Hemon (1956]]:xlii)\n",
      "Too many items:  Taldir\n",
      "Too many items:  cité dans [[Trépos (2001)|Trépos (2001]]:§363)\n",
      "Too many items:  Y. Drezen\n",
      "Too many items:  'Ils viendront si maman leur donne de l'argent.'\n",
      "Too many items:  (Se dit d'un jour gris, pluvieux ou brumeux, où le ciel semble écraser la terre)\n",
      "Too many items:  [https://alliamm.bzh/ ''Al Liamm''] (346:115)\n",
      "Too many items:  Le Bozec (1933:46)\n",
      "Too many items:  adapté de [[Favereau (1997)|Favereau (1997]]:§200)\n",
      "Too many items:  Hingant\n",
      "Too many items:  'Vous allez vous rendre malades si vous restez dehors.'\n",
      "Too many items:  'Ne faites que ce dont vous êtes capables.'\n",
      "Too many items:  'Vous ne lui parlez pas.'\n",
      "Too many items:  Job Jaffré, ''Le Scorff'', cité par [[Ar Borgn (2011)|Ar Borgn (2011]]:53)\n",
      "Too many items:  Jouitteau (à documenter)\n",
      "Too many items:  (la locutrice insiste que c'est la bonne traduction), ''Léonard (Lesneven/Kerlouan)'', [[Y. M. (04/2016)]]\n",
      "Too many items:  'Je viens d'en sortir.'\n",
      "Too many items:  'Quand j'habitais à Vannes… '\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (02/2012)\n",
      "Too many items:  Taldir\n",
      "Too many items:  'le chapeau de Iann'\n",
      "Too many items:  cité dans [[Le Gléau (1973)|Le Gléau (1973]]:17)\n",
      "Too many items:  'la vieille souris, cette vieille souris'\n",
      "Too many items:  Le Gonidec (1838[[Le Gonidec (1838 :186-7)|:186-7)]]\n",
      "Too many items:  traduction de ''Frère Jacques'' par R. Hemon\n",
      "Too many items:  'Ar bidoc'hig', [[Jestin (2006)]]\n",
      "Too many items:  [https://alliamm.bzh/ ''Al Liamm''] (346:115)\n",
      "Too many items:  [http://www.alliamm.com ''Al Liamm'' (346:115)]\n",
      "Too many items:  Hewitt 1988 ([[Hewitt (1988a)|a]] ou [[Hewitt (1988b)|b]])\n",
      "Too many items:  Anne-Marie C. (Clohars), c.p. [2009]\n",
      "Too many items:  (triste, incrédule, décontenancé)\n",
      "Too many items:  Roc'halan\n",
      "Too many items:  cité dans [[Trépos (2001)|Trépos (2001]]:§363)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 7642/9140 [00:00<00:00, 12321.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many items:  Merser (2009:'toull')\n",
      "Too many items:  Kervella (1995:[[Kervella (1995:§234.III.)|§234.III.)]]\n",
      "Too many items:  traduction de ''Frère Jacques'' par R. Hemon\n",
      "Too many items:  (triste, incrédule, décontenancé)\n",
      "Too many items:  I. Krok\n",
      "Too many items:  Communiqué de presse de ''Brezhoweb'' [13/10/11], par C. ar Mero\n",
      "Too many items:  ([[Beyer (2009)|Beyer 2009]]:15)\n",
      "Too many items:  '... to ''any''-one who would like it.' || ''Standard''\n",
      "Too many items:  M. Jouitteau (à tester en élicitation)\n",
      "Too many items:  M. Jouitteau (à tester en élicitation)\n",
      "Too many items:  (en s'adressant au bois de chauffage)\n",
      "Too many items:  ' ''B'': Oui, il n'a pas une très bonne mine (Il ne marche pas très bien).'\n",
      "Too many items:  qui sont tous très jeunes, et pas encore capables de l'aider.'\n",
      "Too many items:  Kenvreuriez ar brezoneg [http://skritur.googlepages.com/sk-avielsantyann (1982:VI:39)]\n",
      "Too many items:  du Père vient à moi. Certes, ''personne'' n'a jamais vu le Père… '\n",
      "Too many items:  adapté de [[Favereau (1997)|Favereau (1997]]:§200)\n",
      "Too many items:  cité dans [[Le Roux (1957)]]\n",
      "Too many items:  (réflexion d'un client las d'attendre une tavernière bavarde...)\n",
      "Too many items:  (qui ne réagissent pas)\n",
      "Too many items:  adaptation de [[Menard & Kadored (2001)|Menard  Kadored (2001]]:'ma<sup>2</sup>)\n",
      "Too many items:  > Beaucoup d'eau a passé sous les ponts.\n",
      "Too many items:  lui permette de se mettre au lit.'\n",
      "Too many items:  [http://www.alliamm.com ''Al Liamm''] (346:127)\n",
      "Too many items:  cité dans [[Ar Borgn (2011)|Ar Borgn (2011]]:83)\n",
      "Too many items:  cité dans [[Schapansky (1996)|Schapansky (1996]]:100)\n",
      "Too many items:  (\"c'est l'apprenti qui est décoré\")\n",
      "Too many items:  (en s'adressant au bois de chauffage)\n",
      "Too many items:  cité dans [[Willis (2011)|Willis (2011]]:14)\n",
      "Too many items:  Titre de chapitre, [[Herrieu (1974)]]\n",
      "Too many items:  Jézégou, ''Prezegennou diwar-benn ar Binijenn hag ar Briedelez'' , p.49\n",
      "Too many items:  'Il disait qu'il était trop vieux.'\n",
      "Too many items:  ('Chacun a son avis puisque chacun a une tête'.)\n",
      "Too many items:  Marion ''Vannetais 1838'', [[IVD.]]\n",
      "Too many items:  Sévéno ''Vannetais 1917'', [[ENVD.]]\n",
      "Too many items:  cité dans [[Trépos (2001)|Trépos (2001]]:§363)\n",
      "Too many items:  cité dans [[Trépos (2001)|Trépos (2001]]:§363)\n",
      "Too many items:  l.671 des ''[[NG.|Noueloù Gwened]]'', [[Hemon (1956)|Hemon (1956]]:xlii)\n",
      "Too many items:  Kenskrid [[Kadored (1912)]], Ar Yeodet-Bocher\n",
      "Too many items:  Troude (1842]]:'ouïr')\n",
      "Too many items:  'Raoul, titre d'article', ''[[Hor Yezh]]'' (1996).\n",
      "Too many items:  I. Krok\n",
      "Too many items:  cité dans [[Menard (1995)|Menard (1995]]:150)\n",
      "Too many items:  'Ar bidoc'hig', [[Jestin (2006)]]\n",
      "Too many items:  Y. Gourmelon, rapporté dans [[Favereau (1997)|Favereau (1997]]:§238)\n",
      "Too many items:  Y. Gourmelon, rapporté dans [[Favereau (1997)|Favereau (1997]]:§238)\n",
      "Too many items:  carte [http://sbahuaud.free.fr/ALBB/Kartenn-111.jpg 111] de l'[[ALBB]]\n",
      "Too many items:  (et non pas [[*]] 'Ce ne serait pas la peine… ')\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (02/2012)\n",
      "Too many items:  (Il meurt plus de jeunes que de vieux)\n",
      "Too many items:  Youenn Troal (''Imbourc'h'', [https://emglev.files.wordpress.com/2013/01/68.pdf hiver 2010])\n",
      "Too many items:  Klerg\n",
      "Too many items:  on reste tard au lit.'\n",
      "Too many items:  (façon de dire 'J'y allais quand j'avais le temps.')\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', Bouzeg c.p. (01/2009)\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (01/2009)\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (01/2009)\n",
      "Too many items:  Klerg\n",
      "Too many items:  Comes (1981:27)\n",
      "Too many items:  Jean-Baptiste Baronian, introduction Comes (1981:5)\n",
      "Too many items:  Comes (1981:25)\n",
      "Too many items:  (= … ''n'en dez ket an nen naon war-lerc'h'')\n",
      "Too many items:  Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (02/2012)\n",
      "Too many items:  Duval (1982), citée dans [[Timm (1995)|Timm (1995]]:4)\n",
      "Too many items:  (se dit à qq. de trop voyant)\n",
      "Too many items:  'Je ne connais ni elle, ni sa sœur.' || ''Français standard''\n",
      "Too many items:  Luzel, cité par [[Favereau (1997)|Favereau (1997]]:§579)\n",
      "Too many items:  cité dans [[Trépos (2001)|Trépos (2001]]:§363)\n",
      "Too many items:  et non pas '[[*]] Yann a vu l'homme qui était en train de courir'\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  [http://diocese-quimper.fr/bibliotheque/files/original/dde339b2f90c775787c2ac0282eb57f8.pdf ''Bleun Brug'' 53]\n",
      "Too many items:  Chanson ''Iwan Gamus'', [[Prigent (1993)]], [[Bertrand (2009)]]\n",
      "Too many items:  Riou (1941:7)\n",
      "Too many items:  cité dans [[Yekel (2016)|Yekel (2016]]:'[http://brezhonegbrovear.bzh/yezhadur.php?pajenn=goulenn_nach goulenn nac'h]')\n",
      "Too many items:  cité dans [[Yekel (2016)|Yekel (2016]]:'[http://brezhonegbrovear.bzh/yezhadur.php?pajenn=goulenn_nach goulenn nac'h]')\n",
      "Too many items:  cité dans [[Yekel (2016)|Yekel (2016]]:'[http://brezhonegbrovear.bzh/yezhadur.php?pajenn=goulenn_nach goulenn nac'h]')\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  I. Krok\n",
      "Too many items:  Radio Breizh, 'keleier ar vro' Karluer [http://www.radiobreizh.bzh/gallo/episode.php?epid=1706 08/02/2012]\n",
      "Too many items:  \"rare\", [[Menard & Kadored (2001)]]\n",
      "Too many items:  Taldir\n",
      "Too many items:  chanson ''Marv Pontkallek'', Tri Yann\n",
      "Too many items:  Taldir\n",
      "Too many items:  cité dans [[Timm (1995)|Timm (1995]]:8)\n",
      "Too many items:  Ar C'halan, 176\n",
      "Too many items:  Ar C'halan, 176\n",
      "Too many items:  Kervella, 345\n",
      "Too many items:  cité dans [[Rezac (2009)]]\n",
      "Too many items:  Naig Rozmor, introduction de ''Ar Vastardez'', par Lagadec\n",
      "Too many items:  breton utilisé en maternelle, [http://maternelleblognotesplouha.blogspot.fr/2012/03/petit-lexique-sonore.html école de Plouha 2017]\n",
      "Too many items:  Helias, ''Brasa pareour er bed'', [http://embann.an.hirwaz.online.fr/document/Heliaz%20Brasa%20pareour%20ar%20bed.pdf pdf]\n",
      "Too many items:  Taldir\n",
      "Too many items:  Sévéno ''Vannetais 1917'', [[ENVD.]]\n",
      "Too many items:  litt. 'Je ne me suis pas réchauffé à eux depuis.'\n",
      "Too many items:  cité dans [[Jouitteau (2005/2010)|Jouitteau (2005]]:189)\n",
      "Too many items:  Kervella, 253\n",
      "Too many items:  Herri, 167 cité dans [[Timm (1995)|Timm (1995]]:4)\n",
      "Too many items:  'Faites ce que vous voudrez.'\n",
      "Too many items:  'Vous ne lui parlez pas.'\n",
      "Too many items:  Nouveau ''Pater'' selon les évêques de France\n",
      "Too many items:  Comes (1981:25)\n",
      "Too many items: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9140/9140 [00:00<00:00, 13378.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mona Bouzeg, ''Cornouaillais (Riec)'', c.p. (02/2012)\n",
      "Too many items:  [équivalent standard: ''N'eus den ebet amañ''- ]\n",
      "Too many items:  (la locutrice insiste que c'est la bonne tradution)\n",
      "Too many items:  Anna Colin [01/1984] ''Cornouaillais (Plouhinec)'', [http://fresques.ina.fr/ouest-en-memoire/fiche-media/Region00850/d-an-nav-vloaz-d-an-aod-ha-da-pevarzek-vloaz-d-an-uzin-des-neuf-ans-a-la-greve-et-en-usine-a-quatorze-ans.html Ouest en mémoire]\n",
      "Too many items:  chanson traditionnelle, ''[http://pagesperso-orange.fr/per.kentel/seizenn3.htm an teir seizenn]'', Kanomp Uhel, Coop Breizh.\n",
      "Nous avons trouvés 14356 exemples(y compris doublons)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tqdm\n",
    "\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "\n",
    "alllineobjects = []\n",
    "error_exemple = []\n",
    "exampleFile = open('examples.txt', 'w')\n",
    "allc=0\n",
    "for title in tqdm.tqdm(pages):\n",
    "    exampleFile.write(title + '\\n_____________________________\\n')\n",
    "    wikitext = pages[title]\n",
    "    counter = 0\n",
    "\n",
    "    for m in repretty.finditer(wikitext):\n",
    "        exampleFile.write(m.group(0) + '\\n________________\\n')\n",
    "        result = wiki2lines(m.group(0), title)\n",
    "        alllineobjects.append(result)\n",
    "        if result.get('rest'):\n",
    "            error_exemple.append(result)\n",
    "        counter += 1\n",
    "\n",
    "    exampleFile.write('\\n' + str(counter) + ' exemples trouvés dans la page ' + title + '\\n================================\\n\\n\\n')\n",
    "    # Ajouter le nombre d'exemples trouvés à la variable totale\n",
    "    allc+=counter\n",
    "# Fermer le fichier\n",
    "exampleFile.close()\n",
    "# Afficher le nombre total d'exemples\n",
    "print(\"Nous avons trouvés\", allc,\"exemples(y compris doublons)\")\n",
    "\n",
    "# Écrire les dictionnaires dans un fichier texte\n",
    "with open('all_line_objects.txt', 'w') as file:\n",
    "    for item in alllineobjects:\n",
    "        for key, value in item.items():\n",
    "            file.write(key + '\\n')\n",
    "            file.write(str(value) + '\\n')\n",
    "        file.write( '__________________\\n')\n",
    "\n",
    "with open('error_exemple.txt', 'w') as file_error:\n",
    "    for item in error_exemple:\n",
    "        for key, value in item.items():\n",
    "            file_error.write(key + '\\n')\n",
    "            file_error.write(str(value) + '\\n')\n",
    "        file_error.write( '__________________\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35cca27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9140/9140 [00:00<00:00, 12236.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons trouvés 14355 exemples(y compris doublons)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE) #expression régulière pour trouver les exemples de table, re.MULTILINE pour que le point puisse matcher les retours à la ligne\n",
    "\n",
    "exampleFile = open('examples.txt','w')\n",
    "allc=0\n",
    "# Parcourir toutes les pages et chercher des exemples de tables avec la classe \"prettytable\"\n",
    "for title in tqdm.tqdm(pages):\n",
    "    exampleFile.write(title+'\\n_____________________________\\n')\n",
    "    wikitext = pages[title] #wikitext est le contenu de la page\n",
    "    counter = 0\n",
    " # Utiliser une expression régulière pour trouver les exemples de table\n",
    "    for m in repretty.finditer(wikitext): #m est chaque exemple trouvé\n",
    "        exampleFile.write(m.group(0)+'\\n________________\\n') #m.group(0) pour matcher la totalité de l'expression régulière\n",
    "        counter += 1\n",
    "    \n",
    "    # Écrire le nombre d'exemples trouvés dans la page actuelle\n",
    "    exampleFile.write('\\n'+str(counter)+' exemples trouvés dans la page '+title+'\\n================================\\n\\n\\n')\n",
    "    # Ajouter le nombre d'exemples trouvés à la variable totale\n",
    "    allc+=counter\n",
    "\n",
    "# Fermer le fichier\n",
    "exampleFile.close()\n",
    "\n",
    "# Afficher le nombre total d'exemples\n",
    "print(\"Nous avons trouvés\", allc,\"exemples(y compris doublons)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b",
   "metadata": {
    "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exemple de tableau standard (4 lignes):\n",
    "\n",
    "{| class=\"prettytable\"\n",
    "|(1)|| N'eo || ket || dleet || koduiñ || hag || evañ || er || memes || amzer.\n",
    "|-\n",
    "||| [[ne]] [[COP|est]] || [[ket|pas]] || [[dleout|dû]] || conduire || [[&|et]] || [[evañ|boire]] || [[P.e|en]].[[art|le]] || [[memes|même]] || temps\n",
    "|-\n",
    "|||colspan=\"10\" | 'On ne doit pas boire et conduire en même temps /Il ne faut pas boire et conduire' \n",
    "|-\n",
    "|||||||||colspan=\"10\" | ''Lesneven/Kerlouan'', [[Y. M. (04/2016)]]\n",
    "|}\n",
    "\"\"\"\n",
    "#dictionnaire des dialectes à récupérer\n",
    "dialects = {\"léonard\" : [], \"cornouaillais\" : [], \"trégorrois\": [], \"vannetais\" : [], \"standard\" : [], \"inconnu\" : [],\n",
    "            \"breton central\" : []}\n",
    "references = {} # keeps track of the references of the sentences: ref:[sentence]\n",
    "sentences = {} # keeps the actual texts\n",
    "dic = {} # token to lemma, pos, gloss\n",
    "lispos = {}\n",
    "#set pour éviter les doublons\n",
    "unique_sentences = set()\n",
    "currentPage=''\n",
    "\n",
    "\n",
    "## OUVERTURE DES FICHIERS\n",
    "#fichier d'erreur\n",
    "errorout = open('breton.errors.txt','w', encoding='utf-8')\n",
    "bilan = open('bilan.txt','w', encoding='utf-8')\n",
    "#fichier contenant les tableaux qu'on ne veut pas traiter\n",
    "non_examples = open('non_examples.txt','w', encoding='utf-8')\n",
    "\n",
    "\n",
    "### EXPRESSIONS REGULIERES\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "rePOS = re.compile(r'verbes|auxiliaires|copules|adverbes|complémenteurs|conjonctions|prépositions|adjectifs|noms|particules verbales|interjections|postpositions|déterminants|quantifieurs|pronoms|noms propres|suffixe|interrogatifs|préfixes|modaux|pluriels|indéfinis|particules de discours|finales|exclamatifs', re.IGNORECASE)\n",
    "reDialect = re.compile(r'léonard|cornouaillais|trégorrois|vannetais|breton central|standard', re.IGNORECASE)\n",
    "reLanguage = re.compile(r\"basque|français de Basse-Bretagne|franco-breton|gaulois|gallo|tohono o'odham|chalcatongo mixtec|tchèque|gallois|roumain|italien|espagnol|hébreu|arabe|français|anglais|allemand|moyen breton|vieux breton|breton pré-moderne\", re.IGNORECASE)\n",
    "reType = re.compile(r'ouvrages de recherche|références de corpus|élicitations|ouvrages pédagogiques|dictionnaires|grammaires', re.IGNORECASE)\n",
    "reNoms = re.compile(r'Yann|Marijo|Anna|Angela|Alejandro|Odile|Anastazi|Nolwenn|Solange|Bastien|Soazig|Marie|Marsel|Lukaz|Lenaig|Kristof|Julie|Jenovefa|Simone|Mona|Mari', re.IGNORECASE)\n",
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "relinks2 = re.compile(r\"\\[\\[.*\\]\\]\") \n",
    "reAlternatives = re.compile(r'\\([^\\W\\d_].* */ *[^\\W\\d_].*\\)', re.U)\n",
    "reWord = re.compile(r'\\[\\[[a-z]+')\n",
    "nospace2 = re.compile(r\".*(-.*\\(.*)\")\n",
    "nospace3 = re.compile(r\".*(.*-,.*-).*\")\n",
    "\n",
    "\n",
    "#éléments de la glose dont on veut supprimer les espaces (pour éviter de splitter sur les espaces)\n",
    "nospace = {\"numéraux cardinaux\": \"numéraux_cardinaux\",\"nom propre\": \"nom_propre\" , \"pronom incorporé\": \"pronom_incorporé\" ,\n",
    "\"nom de titre\": \"nom_de_titre\", 'pronom réfléchi': \"pronom_réfléchi\", \n",
    "\"nom nu\": \"nom_nu\", \"pluriel interne\": \"pluriel_interne\", \"Les pronoms d'@incise contrastifs\": \"Les_pronoms_d'@incise_contrastifs\", \n",
    "\"particule o\": \"particule_o\", \"auxiliaire ober\": \"auxiliaire_ober\" , \"IMP on\": \"IMP_on\",\n",
    "\"bezañ préverbal\": \"bezañ_préverbal\", \"Nep X\":  \"Nep_X\", \"Objet postverbal d'@un infinitif\": \"Objet_postverbal_d'un_infinitif\", \n",
    "\"Objet postverbal avec le verbe '@avoir\": \"Objet_postverbal_avec_le_verbe_'avoir\", \n",
    "\"Indéfini de choix libre par reduplication\": \"Indéfini_de_choix_libre_par_reduplication\", \n",
    "\"châtaigne électrique\": \"châtaigne_électrique\", \"numéral ordinal\": \"numéral_ordinal\",\n",
    "\"avant de\": \"avant_de\", \"Pronom réfléchi\": \"Pronom_réfléchi\", \"Gwazh, gwashañ\": \"Gwazh,_gwashañ\",\n",
    "\"jours de la semaine\" : \"jours_de_la_semaine\", \"jour de la semaine\": \"jour_de_la_semaine\" } \n",
    "\n",
    "#catégories renseignées entre parenthèses dans la glose. Ex: -et(Adj.)\n",
    "categories = {\"V\": \"verbe\", \"Adj\": \"adjectif\", \"PL\": \"suffixe\", \"N\": \"nom\", \"Adv\": \"adverbe\"}\n",
    "# dictionnaire de correspondance pour trouver les dialectes dans les lignes source: Léon --> Léonard \n",
    "regions = {\"Léon\": \"Léonard\", \"Cornouailles\": \"Cornouaillais\", \"Vannes\": \"Vannetais\", \"Trégor\": \"Trégorrois\" }\n",
    "# regex pour supprimer les espaces dans les interjections dans la glose\n",
    "interjections = re.compile(\"Eh bien quoi|Ma parole|eh bien\", re.IGNORECASE)\n",
    "# Rannig + verbe avoir à ne pas séparer \n",
    "one_token = [\"am-oa\",\"he-devoa\", \"en em\", \"on-oa\", \"em-eus\", \"am-eus\", \"e-nije\", \"e-nevoa\", \"he-doa\", \n",
    "             \"en ur\", \"'m eus\", \"em-bije\", \"en-deus\", \"o-deus\", \"em-bije\", \"em-oa\", \"am-bez\", \"am-bo\", \"o-devoa\", \"o-devez\", \"o-deveze\",\n",
    "                        \"o-doa\"]\n",
    "\n",
    "                        \n",
    "#compteurs: nombre de tableaux trouvés, nombre de tableaux dans le fichier d'erreur, nombre de tableaux à ne pas traiter, \n",
    "#nombre de tableaux traités et nombre de doublons\n",
    "total_example = 0\n",
    "error_count = 0\n",
    "other_table = 0\n",
    "converted_tables = 0\n",
    "doublon = 0\n",
    "\n",
    "\n",
    "\n",
    "# getFirstPos: RECHERCHE DE POS DANS UNE PAGE TITRE \n",
    "# la fonction cherche le pos dans toutes les pages selon le lemme donné et retourne le pos associé\n",
    "# si aucun pos n'est trouvé, on met un point d'interrogation\n",
    "def getFirstPos(title, gloss, currentPage):\n",
    "    mpos=''  \n",
    "    if not title: \n",
    "        return '?' \n",
    "    elif title.isupper() and title != 'R':\n",
    "        mpos = title  \n",
    "    # si on cherche la Page \"a\", on aura une catégorie erronée (préposition au lieu de particule verbale) \n",
    "    elif title == 'a' or title == 'e' and 'R' in gloss :\n",
    "        mpos='particule_verbale'\n",
    "    elif title == 'P.e' or title == 'P/e':     \n",
    "        mpos='préposition'\n",
    "    # on vérifie si la catégorie est présente dans la glose\n",
    "    elif re.match(r\".*\\((.*\\.)\\)\", title) and re.match(r\".*\\((.*\\.)\\)\", title).group(1) in categories:                    \n",
    "        test_pos = re.match(r\".*\\((.*\\.)\\)\", title).group(1)                   \n",
    "        if test_pos.replace(\".\", \"\") in categories:                       \n",
    "            mpos = categories[test_pos.replace(\".\", \"\")]                 \n",
    "    else:        \n",
    "    #si le titre contient un tiret bas, on le remplace par un espace pour le trouver dans les titres de pages\n",
    "        if '_' in title: \n",
    "            title = title.replace(\"_\", \" \")\n",
    "        if '#' in title: \n",
    "            title = title.replace(\"#\", \"-\")            \n",
    "#sinon on met une majuscule à la première lettre du titre\n",
    "        else:             \n",
    "            title = title[0].upper()+title[1:]\n",
    "    #si le titre est contenu dans le dictionnaire pages\n",
    "        if title in pages:           \n",
    "            wikicode=''\n",
    "            # on traite les redirections vers d'autres pages\n",
    "            if '#REDIRECTION' in pages[title]:             \n",
    "                #on suit la redirection\n",
    "                newtitle = relinks.search(pages[title]).group(1)                \n",
    "                newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                if '_' in newtitle: \n",
    "                    newtitle = newtitle.replace(\"_\", \" \")            \n",
    "                #et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "                if newtitle in pages:\n",
    "                    wikicode = pages[newtitle]              \n",
    "                else: \n",
    "                    newtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "                    if newtitle in pages:\n",
    "                        wikicode = pages[newtitle]\n",
    "                    else:\n",
    "                        newtitle = newtitle.split(',')[0]\n",
    "                        newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                        if newtitle in pages:\n",
    "                            wikicode = pages[newtitle]\n",
    "                        else:\n",
    "                            errorout.write('__currentPage:\\nstrange redirect to page that does not exist: '+newtitle+'\\n')\n",
    "                            error_count += 1                            \n",
    "            else:\n",
    "                wikicode = pages[title]   \n",
    "            # on cherche les catégories renseignées sur la page\n",
    "            if re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "                cats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)               \n",
    "                list_cats = set()\n",
    "                for cat in cats:   \n",
    "                    # on cherche les POS dans les catégories          \n",
    "                    if rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "                        list_cats.add(rePOS.search(cat).group(0).rstrip('s'))                    \n",
    "                        if len(list_cats) == 1:                                          \n",
    "                            mpos = list(list_cats)[0]                         \n",
    "                        elif len(list_cats) >0: \n",
    "                            mpos = \"/\".join(list_cats)    \n",
    "                    else: \n",
    "                        mpos=\"?\"                              \n",
    "            if mpos:              \n",
    "                mpos = mpos\n",
    "            else:\n",
    "                mpos = \"?\"\n",
    "        else:          \n",
    "            if reNoms.search(title):             \n",
    "                mpos = \"nom_propre\"\n",
    "            # lorsqu'on se trouve sur la page du mot, le mot est entourés de trois guillemets (remplacés par %)\n",
    "            # on cherche les POS sur la page où on se trouve\n",
    "            elif \"%\" in title:             \n",
    "                wikicode = pages[currentPage]                               \n",
    "                if re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "                    cats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)\n",
    "                    list_cats = set()\n",
    "                    for cat in cats:\n",
    "                        if cat not in lispos: \n",
    "                            lispos[cat] = 0\n",
    "                        else: \n",
    "                            lispos[cat] += 1                            \n",
    "                        if rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "                            list_cats.add(rePOS.search(cat).group(0).rstrip('s'))\n",
    "                            if len(list_cats) == 1:                                          \n",
    "                                mpos = list(list_cats)[0]\n",
    "                            elif len(list_cats) >0: \n",
    "                                mpos = \"/\".join(list_cats)                               \n",
    "            else:               \n",
    "                mpos = \"?\"              \n",
    "    return mpos\n",
    "\n",
    "\n",
    "# Suppression des espaces dans les [[ ]] sinon, le split avec espace comme délimiteur espace va mal se passer...\n",
    "# case : [[koll|perd]].[[-et (Adj.)|u]]\n",
    "def sansespace(ch):\n",
    "    for e in relinks.finditer(ch):\n",
    "        if \" \" in e.group(0):\n",
    "            start, end = e.start(), e.end()\n",
    "            newch = re.sub(r\"\\[\\[(.*?) (.*?)\\]\\]\", \"[[\\\\1_\\\\2]]\", e.group(0))\n",
    "            newch = newch.replace(' ', '_') \n",
    "            ch = ch[:start] + newch + ch[start+len(newch)+1:]\n",
    "    return ch\n",
    "#exemple :\n",
    "#print(sansespace(\"[[art|le]] _[[1]]_[[hey j]]_[[maouez | femme]]\"))\n",
    "#resultat : \"[[art|le]] _[[1]]_[[heyj]]_[[maouez| femme]]\")\n",
    "#print(sansespace(\"[[R]] [[+C]] [[ez eus|est]]\"))\n",
    "\n",
    "\n",
    "## REGEX POUR LES MUTATIONS ET CONSONNES EPENTHETIQUES\n",
    "mutation1 = re.compile(r\"_\\[\\[([12345])\\]\\]_\\[\\[(.*)\\]\\]\")\n",
    "mutation2 = re.compile(r\"\\[?\\[?(.*)\\]?\\]?_\\[\\[([12345])\\]\\]_\")\n",
    "\n",
    "#ex: \"[[ne]]_[[+C]]_\"\n",
    "epenthetique1 = re.compile(r\"\\[\\[(.*)\\]\\]_,?/?(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[ma|que]]_[[4]],+C_\n",
    "epenthetique2 = re.compile(r\"\\[\\[(.*)\\]\\]_\\[\\[([12345])\\]\\],?/?(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[R]]_[[+C]],[[4]]_  \n",
    "epenthetique3 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?),\\[\\[([12345])\\]\\]_\")\n",
    "#ex : [[R]]_[[+C]]_[[ez eus|est]]\n",
    "#epenthetique4 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?)_[\\[(.*)\\]\\]\") \n",
    "mutation = False\n",
    "epenthetique = False\n",
    "\n",
    "groupes_pipe = re.compile(r\"(.*?)\\|(.*)\")\n",
    "\n",
    "\n",
    "\n",
    "# tokentrans2lemposglossmorph: FONCTION QUI PREND UN TOKEN ET RETOURNE LEMME, POS, GLOSE\n",
    "# NB : tr = traduction ; t = token\n",
    "\n",
    "def tokentrans2lemposglossmorph(tr, t, title, currentPage):\n",
    "    global error_count\n",
    "    # print(f't:{t}, tr:{tr}')         \n",
    "    lem, pos, gloss, morph = '', '', '', ''\n",
    "    m = relinks.search(tr)     \n",
    "    test_mutation1 = mutation1.search(tr)\n",
    "    test_mutation2 = mutation2.search(tr)\n",
    "    test_epenthetique1 = epenthetique1.search(tr)  \n",
    "    test_epenthetique2 = epenthetique2.search(tr)  \n",
    "    test_epenthetique3 = epenthetique3.search(tr)\n",
    "    \n",
    "\n",
    "    if test_mutation1 :     \n",
    "        mutation = True        \n",
    "        if ('.[[' or ']].' or '-[[' or ']]-' ) in tr:      \n",
    "            lems, glosss, poss, ponct = [], [], [], [] \n",
    "            lemt, glosst, post = [], [], []\n",
    "            if '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "                tr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')                 \n",
    "            #s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "            #ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]]             \n",
    "            if re.search(r\"\\[ ?- ?[A-Za-z]\", tr):               \n",
    "                suffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "                suffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "                tr = tr.replace(suffixe, suffixe_transforme)             \n",
    "            tr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "            for i in range(len(tr.split())):         \n",
    "                test = tr.split()[i]           \n",
    "                if relinks2.search(test):                   \n",
    "                    ele = relinks2.search(test).group(0)                                        \n",
    "                    lem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "                    lems += [lem]\n",
    "                    glosss += [gloss]\n",
    "                    poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                else:                                  \n",
    "                    try:                    \n",
    "                        lem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "                        lems += [lem]\n",
    "                        glosss += [gloss]\n",
    "                        poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                    except:\n",
    "                        errorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "                        error_count += 1\n",
    "                #on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "                if tr.split()[i].startswith(\".\"):\n",
    "                    lemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\".{glosss[i]}\")  \n",
    "                    post.append(f\".{poss[i]}\")  \n",
    "                elif tr.split()[i].startswith(\"-\"): \n",
    "                    lemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\"-{glosss[i]}\")  \n",
    "                    post.append(f\"-{poss[i]}\")  \n",
    "                else: \n",
    "                    lemt.append(lems[i].replace('#', '-')) \n",
    "                    glosst.append(glosss[i])                    \n",
    "                    post.append(poss[i]) \n",
    "        \n",
    "        else:         \n",
    "            morph = test_mutation1.group(1) + \" \" + \"target\"\n",
    "            token_mute = test_mutation1.group(2)       \n",
    "            token_mute_pipe = groupes_pipe.search(token_mute)\n",
    "            if token_mute_pipe: \n",
    "                lem = token_mute_pipe.group(1)\n",
    "                gloss = token_mute_pipe.group(2)\n",
    "                if gloss == 'R':  \n",
    "                    pos = \"particule_verbale\" \n",
    "                else:   \n",
    "                    pos = getFirstPos(lem, gloss, currentPage)\n",
    "            else :\n",
    "                lem = t.lower()        \n",
    "                gloss = token_mute   \n",
    "                if gloss == 'R'  or '[[R]' in gloss:  \n",
    "                    pos = \"particule_verbale\" \n",
    "                else:                          \n",
    "                    pos = getFirstPos(t, gloss, currentPage)\n",
    "                if pos == '?':\n",
    "                    pos = getFirstPos(tr, gloss, currentPage)\n",
    "                    \n",
    "                    \n",
    "    elif test_mutation2 :     \n",
    "        mutation = True        \n",
    "        if '.[[' or ']].' or '-[[' or ']]-' in tr:            \n",
    "            lems, glosss, poss, ponct = [], [], [], [] \n",
    "            lemt, glosst, post = [], [], []\n",
    "            if '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "                tr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')    \n",
    "            #s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "            #ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]]             \n",
    "            if re.search(r\"\\[ ?- ?[A-Za-z]\", tr):               \n",
    "                suffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "                suffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "                tr = tr.replace(suffixe, suffixe_transforme)             \n",
    "            tr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "            for i in range(len(tr.split())):                \n",
    "                test = tr.split()[i]  \n",
    "                if relinks2.search(test):                 \n",
    "                    ele = relinks2.search(test).group(0)                                        \n",
    "                    try: \n",
    "                        lem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "                        lems += [lem]\n",
    "                        glosss += [gloss]\n",
    "                        poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                    except:                                   \n",
    "                            errorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "                            error_count += 1\n",
    "                else:                                  \n",
    "                    try:                    \n",
    "                        lem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "                        lems += [lem]\n",
    "                        glosss += [gloss]\n",
    "                        poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                    except:\n",
    "                        errorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\") \n",
    "                        error_count += 1\n",
    "                #on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "                if tr.split()[i].startswith(\".\"): \n",
    "                    lemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\".{glosss[i]}\")  \n",
    "                    post.append(f\".{poss[i]}\")  \n",
    "                elif tr.split()[i].startswith(\"-\"): \n",
    "                    lemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\"-{glosss[i]}\")  \n",
    "                    post.append(f\"-{poss[i]}\")  \n",
    "                else: \n",
    "                    try:                     \n",
    "                        lemt.append(lems[i].replace('#', '-')) \n",
    "                        glosst.append(glosss[i])                    \n",
    "                        post.append(poss[i])\n",
    "                    except:                        \n",
    "                        errorout.write(f\"__{currentPage}:\\nErreur dans les morphèmes du mot suivant : {tr} et {t}\")\n",
    "                        error_count += 1\n",
    "        \n",
    "        else:              \n",
    "            morph = test_mutation2.group(2) + \" \" + \"trigger\"\n",
    "            token_mute = test_mutation2.group(1)       \n",
    "            token_mute_pipe = groupes_pipe.search(token_mute)          \n",
    "            if token_mute_pipe:      \n",
    "                lem = token_mute_pipe.group(1)\n",
    "                gloss = token_mute_pipe.group(2)\n",
    "                if gloss == 'R'  or '[[R]' in gloss:  \n",
    "                    pos = \"particule_verbale\" \n",
    "                else:                \n",
    "                    pos = getFirstPos(lem, gloss, currentPage)\n",
    "            else :\n",
    "                lem = t.lower() \n",
    "                gloss = token_mute\n",
    "                pos = getFirstPos(t, gloss, currentPage)\n",
    "                if pos == '?':\n",
    "                    pos = getFirstPos(tr, gloss,  currentPage)\n",
    "\n",
    "\n",
    "#S'il y a une consonne épenthétique (+C)             \n",
    "    elif test_epenthetique1 : \n",
    "        epenthetique = True    \n",
    "        morph = test_epenthetique1.group(2) \n",
    "        token_mute = test_epenthetique1.group(1)       \n",
    "        token_mute_pipe = groupes_pipe.search(token_mute)\n",
    "        if token_mute_pipe:      \n",
    "            lem = token_mute_pipe.group(1)\n",
    "            gloss = token_mute_pipe.group(2)\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\" \n",
    "            else:                \n",
    "                pos = getFirstPos(lem, gloss, currentPage)\n",
    "        else :\n",
    "            lem = t.lower() \n",
    "            gloss = token_mute\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\"                  \n",
    "            else:\n",
    "                pos = getFirstPos(t, gloss, currentPage)\n",
    "            if pos == '?':\n",
    "                pos = getFirstPos(tr, gloss, currentPage)\n",
    "\n",
    "    elif test_epenthetique2 :\n",
    "        epenthetique = True    \n",
    "        morph = test_epenthetique2.group(2) + '|' +  test_epenthetique2.group(3) \n",
    "        token_mute = test_epenthetique2.group(1)    \n",
    "        token_mute_pipe = groupes_pipe.search(token_mute)\n",
    "        if token_mute_pipe:      \n",
    "            lem = token_mute_pipe.group(1)\n",
    "            gloss = token_mute_pipe.group(2)\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\" \n",
    "            else:                \n",
    "                pos = getFirstPos(lem, gloss, currentPage)\n",
    "        else :\n",
    "            lem = t.lower() \n",
    "            gloss = token_mute\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\"                  \n",
    "            else:\n",
    "                pos = getFirstPos(t, gloss, currentPage)\n",
    "            if pos == '?':\n",
    "                pos = getFirstPos(tr, gloss, currentPage)\n",
    "                             \n",
    "    elif test_epenthetique3:\n",
    "        epenthetique = True \n",
    "        morph = test_epenthetique3.group(2) + '|' +  test_epenthetique3.group(3)\n",
    "        token_mute = test_epenthetique3.group(1)       \n",
    "        token_mute_pipe = groupes_pipe.search(token_mute)\n",
    "        if token_mute_pipe:      \n",
    "            lem = token_mute_pipe.group(1)\n",
    "            gloss = token_mute_pipe.group(2)\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\" \n",
    "            else:                \n",
    "                pos = getFirstPos(lem, gloss, currentPage)\n",
    "        else :\n",
    "            lem = t.lower() \n",
    "            gloss = token_mute\n",
    "            if gloss == 'R' or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\"                  \n",
    "            else:\n",
    "                pos = getFirstPos(t, gloss, currentPage)\n",
    "            if pos == '?':\n",
    "                pos = getFirstPos(tr, gloss, currentPage)                    \n",
    "                               \n",
    "# si ce n'est pas une mutation et si on a des double crochets [[ ]]              \n",
    "    elif m:     \n",
    "        # cas \"amalgames\" et/ou mots contenant un tiret: \n",
    "        #exemple [[da|à]].[[pronom incorporé|vous]] ou [[den|parent]].[[pluriel interne|s]]-[[kozh|vieil]]  \n",
    "        if '.[[' in tr or ']].' in tr or '-[['in tr or ']]-' in tr: \n",
    "            lems, glosss, poss, ponct = [], [], [], [] \n",
    "            lemt, glosst, post = [], [], []\n",
    "            if '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "                tr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)').replace('(Adv.)', '(Adv)').replace('(M.)', '(M)').replace('(C.)', '(C)')      \n",
    "            #s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "            #ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]] \n",
    "            if re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr):               \n",
    "                suffixe = re.search(r\"\\[ ?(-) ?[A-Za-z].*\\|\", tr).group(0)\n",
    "                suffixe_transforme = suffixe.replace(\"-\", \"#\")        \n",
    "                tr = tr.replace(suffixe, suffixe_transforme)             \n",
    "            tr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "            for i in range(len(tr.split())):         \n",
    "                test = tr.split()[i]                 \n",
    "                if relinks2.search(test):                 \n",
    "                    ele = relinks2.search(test).group(0)                                        \n",
    "                    lem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "                    lems += [lem]\n",
    "                    glosss += [gloss]\n",
    "                    poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                else:    \n",
    "                    try:                    \n",
    "                        lem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "                        lems += [lem]\n",
    "                        glosss += [gloss]\n",
    "                        poss += [getFirstPos(lem, gloss, currentPage)]\n",
    "                    except:              \n",
    "                        errorout.write(f\"__{currentPage}:\\nImpossible de trouver lem, pos, gloss, morph pour le mot suivant : {tr} et {t}\")\n",
    "                        error_count += 1 \n",
    "                #on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "                if tr.split()[i].startswith(\".\"): \n",
    "                    lemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\".{glosss[i]}\")  \n",
    "                    post.append(f\".{poss[i]}\")  \n",
    "                elif tr.split()[i].startswith(\"-\"): \n",
    "                    lemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "                    glosst.append(f\"-{glosss[i]}\")  \n",
    "                    post.append(f\"-{poss[i]}\")  \n",
    "                else: \n",
    "                    lemt.append(lems[i].replace('#', '-')) \n",
    "                    glosst.append(glosss[i])                    \n",
    "                    post.append(poss[i])                                        \n",
    "            lem = ''.join(lemt)\n",
    "            gloss = ''.join(glosst)\n",
    "            pos = ''.join(post)             \n",
    "            return lem, pos, gloss, morph        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # cas \"normal\" : pas d'amalgame, un token et une gloss --> [[token|gloss]]\n",
    "        if '|' in m.group(1):          \n",
    "            if \"numéraux_cardinaux\" in m.group(1): \n",
    "                lem = t.lower() \n",
    "                pos = m.group(1).split('|')[0]\n",
    "                gloss = m.group(1).split('|')[1] \n",
    "            else:                 \n",
    "                lem = m.group(1).split('|')[0]\n",
    "                gloss = m.group(1).split('|')[1]                  \n",
    "                if gloss == 'R' or '[[R]' in gloss:  \n",
    "                    pos = \"particule_verbale\"                      \n",
    "                elif lem == 'P.e' or lem == 'P/e':  \n",
    "                    lem = \"e\"\n",
    "                    pos = \"préposition\"           \n",
    "                elif lem == 'nom_propre':  \n",
    "                    pos = lem\n",
    "                    lem = gloss                 \n",
    "                else:\n",
    "                    pos = getFirstPos(lem, gloss, currentPage)                        \n",
    "                if pos=='?':\n",
    "                    pos = getFirstPos(t, gloss, currentPage)\n",
    "                \n",
    "        # cas sans amalgame mais sans gloss --> [[ne]]\n",
    "        else:           \n",
    "            lem = t\n",
    "            gloss = relinks.search(tr).group(1)\n",
    "            if gloss == 'R'  or '[[R]' in gloss:  \n",
    "                pos = \"particule_verbale\"\n",
    "            if 'DIM' in gloss and t.endswith('ig') or t.endswith('ig%'):\n",
    "                lem = \"-ig\"                \n",
    "                pos = \"suffixe\"                \n",
    "            else:\n",
    "                pos = getFirstPos(t, gloss, currentPage)\n",
    "            if pos == '?':\n",
    "                pos = getFirstPos(tr, gloss, currentPage)      \n",
    "    # cas d'un mot seul sans crochets --> foot dans la page \"Abardaez,_enderv\"      \n",
    "    else :\n",
    "        if \"%\" in t and t.replace(\"%\", \"\").startswith(currentPage.lower()): \n",
    "            lem = currentPage.lower()        \n",
    "        else: \n",
    "            lem = t\n",
    "        gloss = tr\n",
    "        if gloss == 'R'  or '[[R]' in gloss:  \n",
    "            pos = \"particule_verbale\"            \n",
    "        else:\n",
    "            pos = getFirstPos(t, gloss, currentPage)\n",
    "            if pos == '?':\n",
    "                pos = getFirstPos(tr, gloss, currentPage)\n",
    "    return lem, pos, gloss, morph\n",
    "\n",
    "\n",
    "# Nettoyage des tokens + remplacement de certains caractères par d'autres\n",
    "# pour éviter des erreurs quand on splitte\n",
    "def cleanToken(t):\n",
    "    t = t.replace(\"...\", \"…\")   \n",
    "    t = t.replace(\"tout.à.l'heure\", \"tout_à_l'heure\")    \n",
    "    t = t.replace(\"'''\", \"%\")\n",
    "    t = t.replace(\"''\", \"\")\n",
    "    t = re.sub(r'</?font.*?>','',t)\n",
    "    t = re.sub(r'<sup>','_',t)\n",
    "    t = re.sub(r'</sup>','_',t)\n",
    "    t = re.sub(r'<u>','',t)\n",
    "    t = re.sub(r'</u>','',t)\n",
    "    t = re.sub(r'<sub>.*?</sub>','',t)\n",
    "    t = re.sub(r'\\(\\[\\[\\*\\]\\].*?\\)','$$$',t)\n",
    "    t = re.sub(r\"c'h\",'cxxxh',t)\n",
    "    t = re.sub(r\"C'h\",'Cxxxh',t)\n",
    "    t = re.sub(r\"`\",'',t) \n",
    "    t = re.sub(r\"#\",'-',t)   \n",
    "    # pour spliter sur les apostrophes sans les enlever : mettre @ pour spliter sur @ et donc conserver l'apostrophe\n",
    "    if not t.strip().startswith(\"'\"):   \n",
    "        t = re.sub(r\"'\",\"'@\",t)\n",
    "    t = re.sub(r\"bez'@\",\"bez'\",t) \n",
    "    t = re.sub(r\"Bez'@\",\"B ez'\",t)     \n",
    "    t = re.sub(r\"d'@ici\",\"d'ici\",t)   \n",
    "    t = re.sub(\"aujourd'@hui\", \"aujourd'hui\", t)  \n",
    "    t = re.sub(\"l'@heure\", \"l'heure\", t)\n",
    "    t = re.sub(\"si.ce.n'@est\", \"si.ce.n'est\", t)   \n",
    "    t = re.sub(\"aujourd'@hui\", \"aujourd'hui\", t)      \n",
    "    return t.strip()\n",
    "\n",
    "# Remettre les tokens qui ont été remplacés\n",
    "def remettretoken(t):\n",
    "    t = re.sub(r\"%\",\"\",t)\n",
    "    t = re.sub(r'cxxxh',\"c'h\",t)\n",
    "    t = re.sub(r'Cxxxh',\"C'h\",t)\n",
    "    t = re.sub(r\"'@\", \"'\",t)\n",
    "    t = re.sub(r\"\\$\\$\\$ \", \"\",t)\n",
    "    t = re.sub(r\"\\[\\[\", \"\",t)   \n",
    "    t = re.sub(r\"\\]\\]\", \"\",t) \n",
    "    t = t.replace(\"..\", \".\")     \n",
    "    t = t.replace(\"e'@idin\", \"e'idin\")          \n",
    "    return t\n",
    "\n",
    "# Savoir si une string contient un nombre\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "## analyzePage: FONCTION PRINCIPALE\n",
    "def analyzePage(title): \n",
    "    global total_example, error_count, other_table, converted_tables, problem\n",
    "    currentPage = title\n",
    "    mpos = rePOS.search(pages[title])\n",
    "    pagepos = None\n",
    "    conlls=[]\n",
    "    grammatical = True\n",
    "    comparison = False  \n",
    "    example = True\n",
    "    s= ''\n",
    "    if mpos:\n",
    "        # met tout en minuscule\n",
    "        pagepos = mpos.group(0).lower()\n",
    "    #pour chaque tableau (et donc chaque exemple)\n",
    "    nb_tables = 0    \n",
    "    for m in repretty.finditer(pages[title]):\n",
    "        total_example += 1        \n",
    "        prettytable = m.group(1)        \n",
    "        if \"font color=white\" in prettytable: \n",
    "            example = False\n",
    "            non_examples.write(\"__\"+currentPage+\"\\nAutre type de tableau:\\n\"+ prettytable + \"\\n\\n\")            \n",
    "            break      \n",
    "        nb_tables += 1        \n",
    "        tokens, trans = [], []\n",
    "        translation, source, source2 = '', '', ''\n",
    "        location, dialect, phonetic, texttype, l = '','','', '', ''\n",
    "        grammatical=True         \n",
    "\n",
    "        # si le tableau est \"standard\" avec une ligne de token, une ligne de glose et une ligne source (9 lignes dans le texte wiki)\n",
    "        if (len(m.group(1).split('\\n'))) == 9 :\n",
    "            for li in m.group(1).split('\\n'): # for every line of the table              \n",
    "                if li[:2]=='|(' :                   \n",
    "                    if '|| ||' in li:\n",
    "                        li = li.replace('|| ||', '||')                       \n",
    "                    if '[[*]]' in li.split('||')[0]:\n",
    "                        grammatical = False                     \n",
    "                    tokens = [cleanToken(t) for t in li.split('||')[1:]]  \n",
    "                    if \"vs.\" in tokens: \n",
    "                        comparison = True                        \n",
    "                    if tokens[0]==\"[[*]]\" :\n",
    "                        grammatical = False   \n",
    "                    elif '[' in tokens[0] and not \"[…]\" in tokens[0]:                     \n",
    "                        tokens[0] = tokens[0].replace('[', '').strip()     \n",
    "                    elif ']' in tokens[-1] and not '[…]' in tokens[-1]: \n",
    "                        tokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "                    if '' in tokens: \n",
    "                        while '' in tokens: \n",
    "                            tokens.remove('')         \n",
    "                                                                           \n",
    "                if li[:3]=='|||' and grammatical:\n",
    "                    if '|| ||' in li:\n",
    "                        li = li.replace('|| ||', '||')\n",
    "                    if not trans: #la première fois = la traduction                      \n",
    "                        trans = [cleanToken(tr) for tr in li[3:].split('||')]                            \n",
    "                        if '' in trans: \n",
    "                            while '' in trans: \n",
    "                                trans.remove('')                                \n",
    "                        if \"Glose en [[KLT']\" in trans[-1]:\n",
    "                            trans[-1] = trans[-1].replace(\"Glose en [[KLT']\", '').strip()       \n",
    "                        if 'Graphie peurunvan' in trans[-1]: \n",
    "                            trans[-1] = trans[-1].replace('Graphie peurunvan', '').strip()                           \n",
    "                        for i in range(len(trans)):                         \n",
    "                            for ele in nospace:                                \n",
    "                                if ele in trans[i]: \n",
    "                                    trans[i]= trans[i].replace(ele, nospace[ele]) \n",
    "                            if re.search(nospace2, trans[i]):\n",
    "                                to_replace = re.search(nospace2, trans[i]).group(1)\n",
    "                                trans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"_\")) \n",
    "                            if re.search(nospace3, trans[i]):\n",
    "                                to_replace = re.search(nospace3, trans[i]).group(1)\n",
    "                                trans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"_\"))                                     \n",
    "                            if re.search(interjections, trans[i]):\n",
    "                                trans[i] = trans[i].replace(re.search(interjections, trans[i]).group(0), re.search(interjections, trans[i]).group(0).replace(\" \", \"_\"))  \n",
    "                            if re.search(\"(.* !)\", trans[i]):\n",
    "                                trans[i] = trans[i].replace(re.search(\"(.* !)\", trans[i]).group(0), re.search(\"(.* !)\", trans[i]).group(0).replace(\" \", \"\") )                                \n",
    "                                \n",
    "                    elif 'colspan=' in li or '|||||||||||||||' in li:                  \n",
    "                        # vu que les deux lignes translation et source remplissent cette condition, quand on arrive à source\n",
    "                        # la string translation est remplie donc on n'écrase pas et on passe à source\n",
    "                        if not translation:\n",
    "                            translation = li.split('|')[-1]\n",
    "                            for i in range(len(trans)): \n",
    "                                if any(ele in trans[i] for ele in nospace):\n",
    "                                    trans[i] = trans[i].replace(\" \", \"_\")     \n",
    "                                if \"jours de la semaine\" or \"jour de la semaine\" in trans[i]:\n",
    "                                    trans[i] = sansespace(trans[i]) \n",
    "                                \n",
    "                        elif not source: # si déjà trans, alors on a la source\n",
    "                            source = li.split('|')[-1].replace('[[','').replace(']]','')\n",
    "                            if re.search(\"(\\'\\'.*\\'\\' ?,?).*\", source):                               \n",
    "                                other = re.search(\"(\\'\\'.*?\\'\\' ?,?).*\", source).group(1)                                 \n",
    "                                source = source.replace(other, '').replace(\"''\",\"\")                               \n",
    "                            \n",
    "                            if reLanguage.search(source):                              \n",
    "                                grammatical = False     \n",
    "                                break                                \n",
    "                            if re.findall(\"''.*''\", li):                 \n",
    "                                if re.search(r\"\\)''\", li):           \n",
    "                                    location = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "                                    if has_numbers(location):\n",
    "                                        location = ''\n",
    "                            if reDialect.search(li): \n",
    "                                dialect = reDialect.search(li)[0]                                \n",
    "                            if \":\" in source: \n",
    "                                l = re.search(\"(.*):.*\", source).group(1) + ')'                                   \n",
    "                            else: \n",
    "                                l = source\n",
    "                            if re.search(r\"(.*?)\\[\\[\", li):                            \n",
    "                                lang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "                                if reLanguage.search(lang): \n",
    "                                    grammatical = False                                                                                \n",
    "                            if l in pages:                                     \n",
    "                                if '#REDIRECTION' in pages[l]:                                 \n",
    "                                    #on suit la redirection                                        \n",
    "                                    newtitle = relinks.search(pages[l]).group(1)\n",
    "                                    newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                                    s = newtitle\n",
    "                                    if newtitle in pages:\n",
    "                                        bib_source = pages[newtitle]\n",
    "                                        if \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "                                            s += '(moyen breton)'                                           \n",
    "                                else:\n",
    "                                    s = l                                    \n",
    "                                    bib_source = pages[l]\n",
    "                                    if re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "                                        bibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                   \n",
    "                                        for ele in bibl:\n",
    "                                            if reLanguage.search(ele):\n",
    "                                                grammatical = False\n",
    "                                                break\n",
    "                                            tdialect = reDialect.search(ele)\n",
    "                                            if tdialect:\n",
    "                                                dialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "                                            ttype = reType.search(ele)\n",
    "                                            if ttype:\n",
    "                                                texttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")                                                \n",
    "                            else:                                    \n",
    "                                s = l\n",
    "                            if not dialect:                                           \n",
    "                                if reDialect.search(li):\n",
    "                                    dialect = reDialect.search(li).group(0)\n",
    "                                else:                                                   \n",
    "                                    for region in regions:                                                 \n",
    "                                        if region in li:\n",
    "                                            dialect = regions[region]                                             \n",
    "\n",
    "            if not( tokens and trans and translation and source) and grammatical is not False: # if one is missing                \n",
    "                if not tokens: \n",
    "                    errorout.write('\\n'+currentPage+': Tokens line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                    problem = True                    \n",
    "                if not trans: \n",
    "                    errorout.write('\\n'+currentPage+': Trans line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                    problem = True\n",
    "                if not translation:\n",
    "                    errorout.write('\\n'+currentPage+': Translation line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1\n",
    "                    problem = True\n",
    "                if not source:\n",
    "                    errorout.write('\\n'+currentPage+': Source line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                    problem = True\n",
    "                continue\n",
    "                \n",
    "            if grammatical is False: \n",
    "                non_examples.write(\"__\"+currentPage+\"\\nFound agrammatical sentence:\\n\"+ prettytable + \"\\n\\n\")\n",
    "                other_table += 1 \n",
    "                break  \n",
    "                \n",
    "            if comparison is True: \n",
    "                non_examples.write(\"__\"+currentPage+\"\\nTableau de comparaison:\\n\"+ prettytable + \"\\n\\n\")\n",
    "                other_table += 1 \n",
    "                break                  \n",
    "                \n",
    "\n",
    "        # S'il y a plus de lignes que prévu : par ex une ligne de phonétique ou deux lignes source, ou les deux\n",
    "        else:\n",
    "            for li in m.group(1).split('\\n'): # for every line of the table\n",
    "                #si ce n'est pas une ligne de phonétique                \n",
    "\n",
    "                if li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==False:\n",
    "                    if '[[*]]' in li.split('||')[0]:\n",
    "                        grammatical = False\n",
    "                        break\n",
    "                    if '|| ||' in li:\n",
    "                        li = li.replace('|| ||', '||')\n",
    "                    tokens = [cleanToken(t) for t in li.split('||')[1:]] \n",
    "                    if \"vs.\" in tokens: \n",
    "                        comparison = True \n",
    "                    if tokens[0]==\"[[*]]\" :\n",
    "                        grammatical = False\n",
    "                        break      \n",
    "                    if '' in tokens: \n",
    "                        while '' in tokens: \n",
    "                            tokens.remove('')  \n",
    "                    if 'Equivalent [[KLT' in tokens[-1]: \n",
    "                        tokens[-1] = tokens[-1].replace('Equivalent [[KLT', '').strip()\n",
    "                    if ']' in tokens[-1] and not '[…]' in tokens[-1] : \n",
    "                        tokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "                    if '[' in tokens[0] and not '[…]' in tokens[0]: \n",
    "                        tokens[0] = tokens[0].replace('[', '').strip()                        \n",
    "                #si c'est une ligne de phonétique\n",
    "                if li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==True:\n",
    "                    phonetic = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "                    clean_phonetic = ''\n",
    "                    if '[' or '/' in phonetic[0]: \n",
    "                        phonetic[0] = phonetic[0].replace('[', '').replace('/', '').strip()         \n",
    "                    if ']' or '/' in phonetic[-1]: \n",
    "                        phonetic[-1] = phonetic[-1].replace(']', '').replace('/', '').strip()\n",
    "                    for ele in phonetic:        \n",
    "                        clean_phonetic += ele\n",
    "                        clean_phonetic += ' '\n",
    "\n",
    "                if li[:3]=='|||':\n",
    "                    if '|| ||' in li:\n",
    "                        li = li.replace('|| ||', '||')\n",
    "                    # si on a rencontré une ligne phonétique et donc qu'on a pas encore de tokens, cette ligne (2e ligne) est la ligne de tokens                   \n",
    "                    if not tokens :\n",
    "                        tokens = [cleanToken(t) for t in li[3:].split('||')] \n",
    "                        if \"vs.\" in tokens: \n",
    "                            comparison = True \n",
    "                        if '' in tokens: \n",
    "                            while '' in tokens: \n",
    "                                tokens.remove('') \n",
    "                        if 'Equivalent [[KLT' in tokens[-1]: \n",
    "                            tokens[-1] = tokens[-1].replace('Equivalent [[KLT', '').strip()\n",
    "                        if 'Graphie peurunvan' in tokens[-1]: \n",
    "                            tokens[-1] = tokens[-1].replace('Graphie peurunvan', '').strip()\n",
    "                        if ']' in tokens[-1]: \n",
    "                            tokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "                        if '[' in tokens[0]: \n",
    "                            tokens[0] = tokens[0].replace('[', '').strip() \n",
    "                        \n",
    "                    elif not trans: #la première fois = la traduction  \n",
    "                        if not \"Équivalent standardisé\" in li:                       \n",
    "                            trans = [cleanToken(tr) for tr in li[3:].split('||')]                            \n",
    "                            if '' in trans: \n",
    "                                while '' in trans: \n",
    "                                    trans.remove('')  \n",
    "                            for i in range(len(trans)): \n",
    "                                if \"aujourd'hui\" in trans[i]:\n",
    "                                    trans[i]  = trans[i].replace(\"aujourd'hui\", \"aujourdhui\")      \n",
    "                                if any(ele in trans[i] for ele in nospace):\n",
    "                                    trans[i] = trans[i].replace(\" \", \"_\")     \n",
    "                                if \"jours de la semaine\" or \"jour de la semaine\" in trans[i]:\n",
    "                                    trans[i] = sansespace(trans[i]) \n",
    "                                if re.search(interjections, trans[i]):\n",
    "                                    trans[i] = trans[i].replace(re.search(interjections, trans[i]).group(0), re.search(interjections, trans[i]).group(0).replace(\" \", \"_\"))\n",
    "                                if re.search(r\".*\\|(.* !)\\]\\]\", trans[i]): \n",
    "                                    to_replace = re.search(r\".*\\|(.* !)\\]\\]\", trans[i]).group(1)\n",
    "                                    trans[i] = trans[i].replace(to_replace, to_replace.replace(\" \", \"\"))    \n",
    "                                if re.search(r\".*\\|(.* \\?)\\]\\]\", trans[i]): \n",
    "                                    to_replace2 = re.search(r\".*\\|(.* \\?)\\]\\]\", trans[i]).group(1)\n",
    "                                    trans[i] = trans[i].replace(to_replace2, to_replace2.replace(\" \", \"\"))  \n",
    "                                    \n",
    "                                    \n",
    "                    elif 'colspan=' in li:\n",
    "                        if li.startswith('|||colspan='):\n",
    "                            translation = li.split('|')[-1]\n",
    "                        elif not source:                        \n",
    "                            source = li.split('|')[-1].replace('[[','').replace(']]','')   \n",
    "                            if re.search(\"(\\'\\'.*\\'\\' ?,?).*\", source):                              \n",
    "                                other = re.search(\"(.*?\\'\\' ?,?).*\", source).group(1)                               \n",
    "                                source = source.replace(other, \"\").replace(\"''\",\"\")  \n",
    "                            if reLanguage.search(li):\n",
    "                                grammatical = False                             \n",
    "                        elif not source2:                            \n",
    "                            source2 = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')    \n",
    "                            if reLanguage.search(li):\n",
    "                                grammatical = False \n",
    "                        if source:             \n",
    "                            if source2 : \n",
    "                                #source = source2                                \n",
    "                                source, source2 = source2, source\n",
    "                            if re.findall(\"''.*''\", li):                 \n",
    "                                if re.search(r\"\\)''\", li):           \n",
    "                                    location = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "                                    if has_numbers(location):\n",
    "                                        location = ''\n",
    "                            if reDialect.search(li): \n",
    "                                dialect = reDialect.search(li)[0]\n",
    "                            if \":\" in source: \n",
    "                                l = re.search(\"(.*):.*\", source).group(1) + ')' \n",
    "                            else: \n",
    "                                l = source \n",
    "                            if re.search(r\"(.*?)\\[\\[\", li): \n",
    "                                lang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "                                if reLanguage.search(lang): \n",
    "                                    grammatical = False                                                          \n",
    "                            if l in pages:                                 \n",
    "                                if '#REDIRECTION' in pages[l]:                                       \n",
    "                                    #on suit la redirection\n",
    "                                    newtitle = relinks.search(pages[l]).group(1)                \n",
    "                                    newtitle = newtitle[0].upper()+newtitle[1:]\n",
    "                                    s = newtitle\n",
    "                                    if newtitle in pages:\n",
    "                                        bib_source = pages[newtitle]\n",
    "                                        if \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "                                            s += '(moyen breton)'                                           \n",
    "                                else: \n",
    "                                    s = l                                              \n",
    "                                    bib_source = pages[l]\n",
    "                                if re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "                                    bibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                 \n",
    "                                    for ele in bibl:\n",
    "                                        if reLanguage.search(ele):\n",
    "                                            grammatical = False\n",
    "                                            break\n",
    "                                        tdialect = reDialect.search(ele)\n",
    "                                        if tdialect:\n",
    "                                            dialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "                                        ttype = reType.search(ele)\n",
    "                                        if ttype:\n",
    "                                            texttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "                                if not dialect:\n",
    "                                    if reDialect.search(li):\n",
    "                                        dialect = reDialect.search(li).group(0)\n",
    "                                    else:                                                \n",
    "                                        for region in regions:\n",
    "                                            if region in li:\n",
    "                                                dialect = regions[region]\n",
    "                                if reLanguage.search(li):\n",
    "                                    grammatical = False                                                 \n",
    "                            else: \n",
    "                                s = l\n",
    "#à améliorer pour trouver le lieu                                \n",
    "#                             if not location:\n",
    "#                                 if re.search(\"(''.*'')\", li):                                \n",
    "#                                     test = re.search(\"''(.*)''\", li).group(1) \n",
    "#                                     if not reDialect.search(test) and not test.endswith(\"ais\"):                                     \n",
    "#                                         location = test \n",
    "                           \n",
    "            # print(tokens, trans, translation, source)\n",
    "            if not( tokens and trans and translation and source) and grammatical is not False: # if one is missing                \n",
    "                if not tokens: \n",
    "                    errorout.write('\\n__'+currentPage+': Tokens line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                if not trans: \n",
    "                    errorout.write('\\n__'+currentPage+': Trans line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                if not translation:\n",
    "                    errorout.write('\\n__'+currentPage+': Translation line could not be found:'+m.group(1)+'\\n')\n",
    "                    error_count += 1 \n",
    "                if not source:\n",
    "                    errorout.write('\\n__'+currentPage+': Source line could not be found:'+m.group(1)+'\\n')  \n",
    "                    error_count += 1 \n",
    "                continue \n",
    "            if grammatical is False: \n",
    "                non_examples.write(\"__\"+currentPage+\"\\nFound agrammatical sentence:\\n\"+ prettytable + \"\\n\\n\")\n",
    "                other_table += 1 \n",
    "            if comparison is True: \n",
    "                non_examples.write(\"__\"+currentPage+\"\\nTableau de comparaison:\\n\"+ prettytable + \"\\n\\n\")\n",
    "                other_table += 1 \n",
    "\n",
    "\n",
    "    #pour supprimer les mots agrammaticaux \n",
    "    #exemple: ( C'hwi / * Ac'hanoc'h ) --> C'hwi  \n",
    "        cl_tokens = []   \n",
    "       \n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == ']':\n",
    "                tokens[i] = tokens[i].replace(\"]\", \"\")     \n",
    "            if tokens[i] == '[':\n",
    "                tokens[i] = tokens[i].replace(\"[\", \"\")  \n",
    "            if '[…]' in tokens[i]:\n",
    "                tokens[i] = tokens[i].replace('[…]', '')\n",
    "            if '(…)' in tokens[i]:\n",
    "                tokens[i] = tokens[i].replace('(…)', '')   \n",
    "                if 'Equivalent [[KLT' in tokens[i]: \n",
    "                    tokens[i] = tokens[i].replace('Equivalent [[KLT', '').strip()\n",
    "            if re.match(r\".*\\((.*\\.)\\)\", title):            \n",
    "                ele = re.match(r\".*\\((.*)\\.\\)\", title).group(1)                \n",
    "                tokens[i] = tokens[i].replace(ele, \"\")                \n",
    "            if '/' in tokens[i] and '*' in tokens[i] and len(tokens)== len(trans):                \n",
    "                elems = tokens[i].split('/')                \n",
    "                for m in range(len(elems)):                    \n",
    "                    idx = m                    \n",
    "                    if '*' in elems[m]:                        \n",
    "                        tokens[i] = tokens[i].replace(elems[m], '').replace('(', '').replace('{', '').replace('/', '')   \n",
    "                        cl_tokens.append(tokens[i])                        \n",
    "                        for i in range(len(trans)) :              \n",
    "                            if '/' in trans[i]:                       \n",
    "                                trans[i] = trans[i].split('/')[idx].replace('(', '').replace('{', '').replace('/', '')\n",
    "            elif '/' in tokens[i] and not '*' in tokens[i] and len(tokens) == len(trans):           \n",
    "                if reAlternatives.search(tokens[i]):  \n",
    "                    try:                     \n",
    "                        tokens[i] = tokens[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')\n",
    "                        trans[i] = trans[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')   \n",
    "                    except: \n",
    "                        errorout.write(f\"__{currentPage}\\nList index out of range, could not get first alternative:\\ntoken:{tokens[i]}, trans: {trans[i]}\\n\\n\")\n",
    "                        error_count += 1\n",
    "                        problem = True                                       \n",
    "                        error_count += 1 \n",
    "        if 'Équivalent standardisé' in tokens: \n",
    "            tokens.remove('Équivalent standardisé')        \n",
    "        if re.search(r\"\\(.*/\", translation): \n",
    "            options = re.search(r\"\\(.*\\)\", translation).group(0)\n",
    "            option = re.search(r\"\\((.*?)/\", translation).group(1)  \n",
    "            translation = translation.replace(options, option)     \n",
    "            \n",
    "        if len(' '.join(tokens).split()) == len(' '.join(trans).split()): \n",
    "            tokens = ' '.join(tokens).split()               \n",
    "            trans = ' '.join(trans).split()\n",
    "            \n",
    "            \n",
    "\n",
    "        if grammatical is False: \n",
    "            break \n",
    "        text = ' '.join(tokens)\n",
    "        text_ch = remettretoken(text) \n",
    "\n",
    "\n",
    "        if '' in tokens:          \n",
    "            while '' in tokens: \n",
    "                tokens.remove('')            \n",
    "        if '' in trans: \n",
    "            while '' in trans: \n",
    "                trans.remove('') \n",
    "        # construction of the conll:         \n",
    "        text = ' '.join(tokens)\n",
    "        text_ch = remettretoken(text)        \n",
    "        references[source] = references.get(source,[])\n",
    "        if text in references[source]:\n",
    "            ind = references[source].index(text)+1\n",
    "        else:\n",
    "            references[source]+=[text]\n",
    "            ind = 1\n",
    "        sentences[text] = sentences.get(text,0)+1\n",
    "        conllis = ['# sent_id = '+source.replace(' ', '') +'__'+str(ind)]\n",
    "        conllis += ['# text = '+text_ch]        \n",
    "        conllis += ['# text_fr = '+ remettretoken(cleanToken(translation)).replace('[…]', '').replace('(…)', '')]\n",
    "        if phonetic:\n",
    "            conllis += ['# text_phon = '+remettretoken(clean_phonetic.replace('[…]', '').replace('(…)', ''))]\n",
    "        if dialect:\n",
    "            conllis += ['# dialect = ' + dialect]\n",
    "        if location:\n",
    "            conllis += ['# location = ' + location]\n",
    "        if s :\n",
    "            conllis += ['# source = ' + s] \n",
    "        if source2:\n",
    "            conllis += ['# source2 = ' + source2]\n",
    "        if texttype :\n",
    "            conllis += ['# texttype = ' + texttype]   \n",
    "        ##si on a trouvé du vert alors conllis += ['# phonetic = yes ']\n",
    "        ##si un morceau en phonétique alors mettre en trait MISC\n",
    "\n",
    "        extrai = 1\n",
    "        \n",
    "        if len(tokens) != len(trans):         \n",
    "            errorout.write('\\n___ '+ currentPage + \"\\n\" + \"len tokens:\" \n",
    "            + str(len(tokens)) + \"\\nlen trans:\" + str(len(trans)) + \"\\n\" + \"tokens:\" + str(tokens)+ \"\\n\"+\n",
    "            \"trans:\" + str(trans) + \"\\n \" +prettytable + \"\\n\\n\" )\n",
    "            error_count += 1 \n",
    "            continue    \n",
    "        for i,t in enumerate(tokens):           \n",
    "            if not t:\n",
    "                errorout.write('\\n___ '+currentPage+': pas de token '+str(i)+' in ' + prettytable + \"\\n\")\n",
    "                t='???'\n",
    "                error_count += 1 \n",
    "            if \"[[R]]_[[4]] / [[+C]]_\" in trans[i]: \n",
    "                trans[i] = trans[i].replace(\" \", \"\").replace(\" / \", \",\")\n",
    "                print(f\"here: {trans[i]}\")\n",
    "            if len(trans)<=i:                \n",
    "                errorout.write('\\n___ '+currentPage+': la longueur de la glose est inférieure à la longueur des tokens '+str(i)+' in ' + prettytable + \"\\n\")\n",
    "                tr = '???'  \n",
    "                error_count += 1 \n",
    "            else:\n",
    "                tr = trans[i]              \n",
    "                \n",
    "            # délimiteur : @ et espace         \n",
    "            if t in one_token:             \n",
    "                test1 = [t.replace(\" \", \"_\").replace(\"-\", \"_\")]  \n",
    "                test2 = [tr.replace(\" \", \"_\")]\n",
    "            elif any(ele in t for ele in one_token):\n",
    "                for i in range(len(one_token)):                \n",
    "                    if re.search(one_token[i], t): \n",
    "                        if '-' in re.search(one_token[i], t).group(0):                        \n",
    "                            test1 = re.split(\"@| |-\", t)[:-2]                         \n",
    "                            to_add1 = re.split(\"@| |-\", t)[-2:]\n",
    "                            test1.append('_'.join(to_add1))             \n",
    "                            test2 = tr.split()[:-2]\n",
    "                            to_add = tr.split()[-2:]\n",
    "                            test2.append('_'.join(to_add))\n",
    "                        elif '-' in re.search(one_token[i], t).group(0):                        \n",
    "                            test1 = re.split(\"@| \", t)[:-2]                      \n",
    "                            to_add1 = re.split(\"-| \", t)[-2:]\n",
    "                            test1.append('_'.join(to_add1))              \n",
    "                            test2 = tr.split()[:-2]\n",
    "                            to_add = tr.split()[-2:]\n",
    "                            test2.append('_'.join(to_add))                        \n",
    "                       \n",
    "            else: \n",
    "                test1 = re.split(\"@| \", t)\n",
    "                if '' in test1:  \n",
    "                    test1.remove(\"\")           \n",
    "                test2 = re.split(' |@', sansespace(tr))\n",
    "            # si on a le même nombre d'éléments pour les tokens et la glose entre chaque || ||          \n",
    "            if len(test1) == len(test2):                  \n",
    "                for j,e in enumerate(test1):\n",
    "                    t1 = test1[j]                  \n",
    "                    tr1 = test2[j]                           \n",
    "                    try:                    \n",
    "                        if t1[-1] not in \".,;!?…\" and t1 != \"$$$\":          \n",
    "                            lem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)      \n",
    "                            dic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]# et morph ?\n",
    "                            eles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]             \n",
    "                            if morph != '':\n",
    "                                if 'R' in morph:                                \n",
    "                                    eles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]   \n",
    "                                else: \n",
    "                                    eles[9]+='|'+morph                                \n",
    "                            if len(eles) == 10:                              \n",
    "                                conllis += ['\\t'.join(eles)]                              \n",
    "                            else: \n",
    "                                errorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                    \n",
    "                                error_count += 1\n",
    "                            if not (lem or pos or gloss or morph) :\n",
    "                                errorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "                                error_count += 1\n",
    "                            extrai += 1  \n",
    "                            \n",
    "                        elif t1[-1] in \".,;!?…\" and not t1[-3:] == \"...\" :\n",
    "                            lem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1[:-1], title, currentPage)\n",
    "                            #si on ne trouve pas de pos, on cherche dans la page elle-même\n",
    "                            if pos=='?' and t1[:-1].lower()==title.lower() and pagepos:\n",
    "                                pos = pagepos\n",
    "                            dic[t1[:-1]] = dic.get(t1[:-1], [])+[(lem,pos,gloss)]\n",
    "                            eles = [str(extrai),remettretoken(t1[:-1]), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]\n",
    "                            if morph != '':\n",
    "                                eles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]          \n",
    "                            conllis += ['\\t'.join(eles)]\n",
    "                            extrai += 1\n",
    "                            dic[t1[-1:]] = dic.get(t1[-1:], [(t1[-1:], 'PUNCT','punct')])\n",
    "                            eles = [str(extrai),remettretoken(t1[-1:]), remettretoken(t1[-1:]), 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "                            if len(eles) == 10:                              \n",
    "                                conllis += ['\\t'.join(eles)]\n",
    "                            else: \n",
    "                                print(f\"error cas 2: ligne du conll invalide (longueur {len(eles)})\")  \n",
    "                            if not (lem or pos or gloss or morph) :\n",
    "                                errorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "                                error_count += 1\n",
    "                            extrai += 1\n",
    "\n",
    "                        else: \n",
    "                            lem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)\n",
    "                            dic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "                            eles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]                       \n",
    "                            if morph != '':\n",
    "                                try:                             \n",
    "                                    eles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "                                except: \n",
    "                                    print(\"error\") \n",
    "                            if len(eles) == 10:                              \n",
    "                                conllis += ['\\t'.join(eles)] \n",
    "                            else: \n",
    "                                errorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                             \n",
    "                                error_count += 1\n",
    "                            if not (lem or pos or gloss or morph) :\n",
    "                                errorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "                                error_count += 1\n",
    "                            extrai += 1\n",
    "              \n",
    "                    except:\n",
    "                        continue                        \n",
    "                        \n",
    "            # si on a le nombre d'éléments de tokens est supérieur à celui de la glose                  \n",
    "            elif len(test1)>len(test2):\n",
    "                for j,e in enumerate(test1):\n",
    "                    t1 = test1[j]                       \n",
    "                    if t1 == \"$$$\":\n",
    "                        errorout.write(f\"__{currentPage}\\nElément agrammatical dans la phrase:\\n{prettytable}test1: {test1}\\ntest2: {test2}\\n\\n\")  \n",
    "                        error_count += 1 \n",
    "                    try:                    \n",
    "                        tr1 = test2[j]\n",
    "                    except:\n",
    "                        tr1 = \"?\"                     \n",
    "                        \n",
    "                        \n",
    "                    # si la ponctuation est tout simplement séparée du token                   \n",
    "                    if t1 in \"/-.,;!?…\":\n",
    "                        dic[t1] = dic.get(t1, [(t1, 'PUNCT','punct')])\n",
    "                        eles = [str(extrai),t1, t1, 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "                        if len(eles) == 10:                              \n",
    "                            conllis += ['\\t'.join(eles)]\n",
    "                        else: \n",
    "                            errorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                                                    \n",
    "                            error_count += 1\n",
    "                        extrai += 1           \n",
    "                    \n",
    "                    # les autres tokens sont traités normalement\n",
    "                    else:\n",
    "                        lem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)\n",
    "                        dic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "                        eles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]                       \n",
    "                        if morph != '':\n",
    "                            try:                             \n",
    "                                eles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "                            except: \n",
    "                                print(\"error\") \n",
    "                        if len(eles) == 10:                              \n",
    "                            conllis += ['\\t'.join(eles)] \n",
    "                        else: \n",
    "                            errorout.write(f\"\\n___{currentPage}\\n Erreur dans la : \\n {prettytable}\\n.Ligne du conll invalide (longueur {len(eles)})\\n\\n\")                                                                       \n",
    "                            error_count += 1\n",
    "                        if not (lem or pos or gloss or morph) :\n",
    "                                errorout.write(f\"\\n__{currentPage}\\nImpossible de trouver lem, pos, gloss, morph\\n{prettytable}\\n\\n\")\n",
    "                                error_count += 1\n",
    "                        extrai += 1\n",
    "\n",
    "            # si on a le nombre d'éléments de la glose est supérieur à celui des tokens \n",
    "            elif len(test1)<len(test2):                   \n",
    "                # print(f\"PROBLEME ! éléments de la glose supérieur à celui des tokens: {test1, test2}\") \n",
    "                    errorout.write('\\n___ '+currentPage+': slash trouvé ' +str(i)+' in '+ \n",
    "prettytable + 'test1: ' + str(test1) + str(len(test1)) + \"\\n\" + 'test2: ' + str(test2) + str(len(test2)) + \"\\n\\n\")\n",
    "                    error_count += 1 \n",
    "                    problem = True\n",
    "\n",
    "# pour vérifier qu'aucun token ne manque dans le conll\n",
    "# problème : on splitte seulement sur les espaces mais il faudrait aussi splitter\n",
    "# sur les apostrophes quand ils ne sont pas en début de mot \n",
    "        # test_conll = []\n",
    "        # punct_list = [\".\", \",\", \":\", \"!\", \";\", \"…\", ]\n",
    "        # for i in range(len(conllis)):\n",
    "        #     if re.search(\"^[0-9]\", conllis[i]):\n",
    "        #         test_conll.append(conllis[i].split('\\t')[1])    \n",
    "        # for ele in conllis[1].split()[3:]:\n",
    "        #     if ele.replace(\".\", \"\").replace(\":\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"…\", \"\") not in test_conll and ele not in punct_list:\n",
    "        #         print(f\"error: le mot {ele} a disparu\")\n",
    "     \n",
    "        if any(\"# dialect\" in string for string in conllis):         \n",
    "            for d in dialects:\n",
    "                if f'# dialect = {d.capitalize()}' in conllis: \n",
    "                    dialects[d] += [conllis]  \n",
    "        else: \n",
    "            dialects['inconnu'] +=  [conllis]             \n",
    "    bilan.write(f\"Nombre total: {total_example}, autres exemples: {other_table}, tableaux erreur {error_count}\\n\\n\")\n",
    "    return dialects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bb22511-47ca-492d-9e84-a842e73c638f",
   "metadata": {
    "id": "9bb22511-47ca-492d-9e84-a842e73c638f"
   },
   "source": [
    "# trying to run through the whole site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
    "outputId": "d06bcad2-4c05-4761-efaa-ad70357c2289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 380/9106 [00:00<00:12, 724.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 455/9106 [00:00<00:19, 451.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 624/9106 [00:01<00:18, 460.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1113/9106 [00:01<00:09, 831.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1342/9106 [00:02<00:15, 512.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1534/9106 [00:02<00:12, 583.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1867/9106 [00:03<00:10, 705.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2077/9106 [00:03<00:11, 590.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2255/9106 [00:03<00:11, 580.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3091/9106 [00:05<00:09, 665.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3856/9106 [00:06<00:06, 765.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4553/9106 [00:07<00:05, 842.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 4678/9106 [00:07<00:04, 946.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4858/9106 [00:08<00:07, 579.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 5535/9106 [00:09<00:05, 708.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 5617/9106 [00:09<00:08, 422.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 5755/9106 [00:10<00:09, 346.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 5873/9106 [00:10<00:07, 442.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6142/9106 [00:10<00:05, 509.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 6603/9106 [00:11<00:04, 589.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 6668/9106 [00:12<00:06, 357.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8009/9106 [00:14<00:01, 612.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 8257/9106 [00:14<00:01, 523.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9030/9106 [00:15<00:00, 681.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9106/9106 [00:16<00:00, 564.91it/s]\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "dic = {}\n",
    "errorout = open('breton.errors.txt','w', encoding=\"utf-8\")\n",
    "nopos = open('no_pos.txt', 'w', encoding='utf-8')\n",
    "pos_list = open('pos_list.txt','w', encoding='utf-8')\n",
    "bilan = open('bilan.txt','w', encoding='utf-8')\n",
    "doublons = open(\"doublon.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "for title in tqdm.tqdm(pages):\n",
    "    analyzePage(title)  \n",
    "for d in dialects:\n",
    "    for conll in sorted(dialects[d], key = lambda x: x[1]):\n",
    "        if conll[1].translate(str.maketrans('','',string.punctuation)) not in unique_sentences: \n",
    "            open('bretonconlls/'+re.sub(r'\\W','_',d)+'.conllu','a', encoding=\"utf-8\").write('\\n'.join(conll) + '\\n\\n')\n",
    "            unique_sentences.add(conll[1].translate(str.maketrans('','',string.punctuation)))\n",
    "            converted_tables += 1  \n",
    "        else:\n",
    "            doublon += 1\n",
    "            doublons.write(\"__\\n\" + str('\\n'.join(conll)) + \"\\n\\n\")         \n",
    "for key, value in lispos.items():\n",
    "    pos_list.write(f\"{key} \\t {value}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11bdb8-7193-40e2-81ae-a4a8febae7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "breton_27_07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
